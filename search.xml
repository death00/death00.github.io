<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[力扣337——打家劫舍 III]]></title>
    <url>%2F2020%2F01%2F25%2F%E5%8A%9B%E6%89%A3337%E2%80%94%E2%80%94%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D%20III%2F</url>
    <content type="text"><![CDATA[这一篇也是基于”打家劫舍”的扩展，需要针对特殊情况特殊考虑，当然其本质还是动态规划，优化时需要考虑数据结构。 原题在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 示例 1:12345678910输入: [3,2,3,null,3,null,1] 3 / \ 2 3 \ \ 3 1输出: 7 解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7. 示例 2:12345678910输入: [3,4,5,1,3,null,1] 3 / \ 4 5 / \ \ 1 3 1输出: 9解释: 小偷一晚能够盗取的最高金额 = 4 + 5 = 9. 原题url：https://leetcode-cn.com/problems/house-robber-iii/ 解题先给出树节点的结构：123456public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125; 简单思路这道题简单来说，就是如果存在父节点、子节点、孙子节点三层的话，要么偷父节点 + 孙子节点，要么只偷子节点。 顺着这个思路，我们只要找出每个节点所能偷到的最大值，自然也就能找出从 root 节点开始偷的最大值了。 接下来我们看看代码：12345678910111213141516171819202122232425262728class Solution &#123; Map&lt;TreeNode, Integer&gt; cache = new HashMap&lt;&gt;(); public int rob(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; // 是否已经计算过 if (cache.containsKey(root)) &#123; return cache.get(root); &#125; // 策略1：抢当前节点和孙子节点 int sum1 = root.val + // 左子节点的子节点们 (root.left == null ? 0 : (rob(root.left.left) + rob(root.left.right))) + // 右子节点的子节点们 (root.right == null ? 0 : (rob(root.right.left) + rob(root.right.right))); // 策略2：只抢子节点 int sum2 = rob(root.left) + rob(root.right); // 找出更大的值 int sum = Math.max(sum1, sum2); // 并记录 cache.put(root, sum); return sum; &#125;&#125; 提交OK，执行用时：5 ms，只战胜了52.00%的 java 提交记录，因此还是有值得优化的地方。 优化上面的解法，如果说有什么值得优化的地方，就是在于我们在动态规划时，不仅考虑了子节点，甚至也考虑到了孙子节点，因此当 子节点 变成 父节点 之后，孙子节点 也变成了 子节点。 也就是说，一开始的孙子节点被计算了两遍。虽然我们借用了一个 map 来记录了中间结果，但我们需要注意，这种情况依旧会被计算，只是代价被转移到了针对 map 的操作，这也是需要消耗时间的。 那么现在的优化，就转变成针对中间状态的记录上了。 其实我们针对每个节点的状态，只需要记录两种情况：抢或者不抢。而且这个状态只会被父节点用到，并不需要永久保留。因此我们考虑用一个长度为 2 的数组进行记录，这样就会快捷很多。 接下来我们看看代码：12345678910111213141516171819202122232425class Solution &#123; public int rob(TreeNode root) &#123; // index为0，代表不抢当前节点的最大值 // index为1，代表抢当前节点，不抢子节点的最大值 int[] res = dp(root); return Math.max(res[0], res[1]); &#125; public int[] dp(TreeNode cur) &#123; if(cur == null) &#123; return new int[]&#123;0,0&#125;; &#125; int[] left = dp(cur.left); int[] right = dp(cur.right); // 抢当前节点，子节点都不抢 int rob = cur.val + left[0] +right[0]; // 不抢当前节点，获取左右子节点各自的最大值 int notRob = Math.max(left[0], left[1]) + Math.max(right[0], right[1]); // 返回结果 return new int[]&#123;notRob, rob&#125;; &#125;&#125; 提交OK，时间消耗只有1 ms，确实快了很多。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要还是利用动态规划，只是需要大家进行思路转化，优化时需要考虑的更多是对数据结构的理解。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣213——打家劫舍 II]]></title>
    <url>%2F2020%2F01%2F24%2F%E5%8A%9B%E6%89%A3213%E2%80%94%E2%80%94%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D%20II%2F</url>
    <content type="text"><![CDATA[这一篇是上一篇的扩展，需要针对特殊情况特殊考虑，当然其本质还是动态规划。 原题你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都围成一圈，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 1:123输入: [2,3,2]输出: 3解释: 你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 示例 2:1234输入: [1,2,3,1]输出: 4解释: 你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 原题url：https://leetcode-cn.com/problems/house-robber-ii/ 解题这道题的变化是，同样是一个数组，但是首尾相连了，也就是成了一个环，那么原本递推的方式也就行不通了，因为任何一个节点其实地位都相等了，也就找不到最初的状态，无法进行递推了。 但我们可以将现在的问题转化成我们已经解决的问题，仔细想想。所谓的首尾相连，针对状态进行划分，可以有三种情况： 首尾节点都不选择 只选择首节点，不选择尾结点 只选择尾结点，不选择首节点 因为我们最终是要求出最大值，那么只需要考虑后面两种情况，而这样的话，又可以转化成了原本的线性数组了。 接下来让我们看看代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123; public int rob(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; if (nums.length == 1) &#123; return nums[0]; &#125; // 因为收尾相连，无法按照最初的动态规划来做，因为没有一个可以开始的点。 // 那么就将未知问题转化为已知问题，针对首尾两个节点，可以有三种情况： // 1、首尾节点都不选择 // 2、只选择首节点，不选择尾结点 // 3、只选择尾结点，不选择首节点 // 因为是要取最大值，且是非负整数数据，所以只考虑后两种情况 return Math.max( // 只选择首节点，不选择尾结点 calMax(nums, 0, nums.length - 2), // 只选择尾结点，不选择首节点 calMax(nums, 1, nums.length - 1) ); &#125; public int calMax(int[] nums, int start, int end) &#123; // 存储当前位置，下一个位置，和再下一个位置的结果 int current = 0; int next_1 = 0; int next_2 = 0; // 动态规划，利用中间结果，寻找最大值 for (int i = end; i &gt;= start; i--) &#123; current = Math.max( // 当前不偷 next_1, // 当前偷 nums[i] + next_2 ); next_2 = next_1; next_1 = current; &#125; return current; &#125;&#125; 提交OK。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要还是利用动态规划，只是需要大家进行思路转化，将未知转化为 已知，从而解决问题。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣198——打家劫舍]]></title>
    <url>%2F2020%2F01%2F20%2F%E5%8A%9B%E6%89%A3198%E2%80%94%E2%80%94%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D%2F</url>
    <content type="text"><![CDATA[这次准备连讲三道题，这道题就是最基础的，利用动态规划可以解决。 原题你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 :1234输入: [1,2,3,1]输出: 4解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 原题url：https://leetcode-cn.com/problems/house-robber/ 解题动态规划-自顶向下动态规划大家应该很容易就想到，如果偷了当前的金钱，下一家就不能偷，如果不偷当前的金钱，可以考虑下一家。比如： 小偷到了第3家，他有两个选择：不偷第3家之后去第4家、偷完第3家之后去第5家。这时他需要比较的是： 从第4家开始能偷到的最多金钱 第3家的金钱加上从第5家开始能偷到的最多金钱上面两者谁更多，就选择怎么做。 那主要目的就是要求出从当前到结尾可以偷到的最大值，为了不重复计算，可以利用一个数组记录中间结果。 接下来看看代码：123456789101112131415161718192021222324252627282930313233class Solution &#123; public int rob(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; // 存储中间结果 int[] result = new int[nums.length]; Arrays.fill(result, -1); // 动态规划，利用中间结果，寻找最大值 dp(0, nums, result); return result[0]; &#125; public int dp(int start, int[] nums, int[] result) &#123; if (start &gt;= nums.length) &#123; return 0; &#125; if (result[start] != -1) &#123; return result[start]; &#125; result[start] = Math.max( // 选择偷当前的家 nums[start] + dp(start + 2, nums, result), // 选择不偷当前的家 dp(start + 1, nums, result) ); return result[start]; &#125;&#125; 提交OK。 动态规划-自底向上上面的写法其实是从头向尾考虑，写法上是递归。那么如果想不用递归呢？毕竟递归也是有缺陷的，如果次数过多，总调用栈就会很长。那我们来改造一下。 如果我们是从尾向头考虑呢？也就是从最后一家开始，选择偷或者不偷，最终到第一家。思想上还是很好理解的，和上面差不多，让我们看看代码：1234567891011121314151617181920class Solution &#123; public int rob(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; // 存储中间结果 int[] result = new int[nums.length + 2]; // 动态规划，利用中间结果，寻找最大值 for (int i = nums.length - 1; i &gt;= 0; i--) &#123; result[i] = Math.max( // 当前不偷 result[i + 1], // 当前偷 nums[i] + result[i + 2] ); &#125; return result[0]; &#125;&#125; 提交也是OK的。 空间上的优化我们仔细观察一下上面的写法，其实每次你利用到的中间状态只有两个，下一个位置和再下一个位置。那么此时我们也可以用三个变量来存储就够了，两个存储之后的值，还有一个存储当前的值。 让我们来看看代码：123456789101112131415161718192021222324class Solution &#123; public int rob(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; // 存储当前位置，下一个位置，和再下一个位置的结果 int current = 0; int next_1 = 0; int next_2 = 0; // 动态规划，利用中间结果，寻找最大值 for (int i = nums.length - 1; i &gt;= 0; i--) &#123; current = Math.max( // 当前不偷 next_1, // 当前偷 nums[i] + next_2 ); next_2 = next_1; next_1 = current; &#125; return current; &#125;&#125; 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要利用动态规划就可以解决，可以改写递归，优化空间复杂度。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣322——零钱兑换]]></title>
    <url>%2F2020%2F01%2F16%2F%E5%8A%9B%E6%89%A3322%E2%80%94%E2%80%94%E9%9B%B6%E9%92%B1%E5%85%91%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[这道题主要涉及动态规划，利用这个，就能很好解决这个问题。 原题给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 示例 1:123输入: coins = [1, 2, 5], amount = 11输出: 3 解释: 11 = 5 + 5 + 1 示例 2:12输入: coins = [2], amount = 3输出: -1 说明: 你可以认为每种硬币的数量是无限的。 原题url：https://leetcode-cn.com/problems/coin-change/ 解题求出所有可能我们可以从小到大，求出由当前硬币，组成所有金额的最小数，这样最终就是最大金额所能组成的最小硬币数量。 这种方法核心思想就是记录所有中间状态的结果，如果在实际使用中，你的传入参数amount是不断变化的，那么用这种方法会比较方法，因为之前的结果可以被重复利用，这样也是一种优势。 现在我们来看看代码：12345678910111213141516171819202122232425262728293031323334class Solution &#123; public int coinChange(int[] coins, int amount) &#123; // 排序，升序 Arrays.sort(coins); // 用来记录中间结果，各种金额所需要的硬币数量 int[] result = new int[amount + 1]; result[0] = 0; // 遍历所有可能的金额 for (int currentAmount = 1; currentAmount &lt;= amount; currentAmount++) &#123; int minNum = Integer.MAX_VALUE; // 遍历所有硬币 for (int j = 0; j &lt; coins.length; j++) &#123; // 当前金额已经比硬币的值小 int remainAmount = currentAmount - coins[j]; if (remainAmount &lt; 0) &#123; break; &#125; // remainAmount无法由硬币组成 if (result[remainAmount] == Integer.MAX_VALUE) &#123; continue; &#125; // 取更小的值 minNum = Math.min(minNum, result[remainAmount] + 1); &#125; result[currentAmount] = minNum; &#125; return result[amount] == Integer.MAX_VALUE ? -1 : result[amount]; &#125;&#125; 提交OK，执行用时：12 ms，内存消耗：36 MB，只超过了83.24%的 java 提交，看来还有优化的空间。 动态规划优化倒不是说动态规划就一定比上面的方法更加优秀，只是也是一种思想，可以利用 dfs(深度优先搜索) 或者 bfs(广度优先搜索)，我下面这种写法是 dfs，因为是一路进行到底之后，再去考虑其他情况的。(补充一点，如果使用 bfs 的话，可以借助队列来实现非递归的形式。) 所谓的优化，就是从硬币的使用上来说，从面值大的开始，并且从可以使用数量最大的开始。与此同时，我们也记录了最小使用数量，如果用当前面值最大的硬币并且使用最多时，依旧大于最小值，那么就不用继续查找了。 以上的优化，其实是和题目相关联，虽然不能用在其他的题目上，但也可以作为一种思想，值得我们借鉴和学习。 接下来我们看看代码：1234567891011121314151617181920212223242526272829303132class Solution &#123; private int minCount = Integer.MAX_VALUE; public int coinChange(int[] coins, int amount) &#123; // 排序 Arrays.sort(coins); // 从大往小找 helper(coins, coins.length - 1, 0, amount); return minCount == Integer.MAX_VALUE ? -1 : minCount; &#125; private void helper(int[] coins, int coinIndex, int curCount, int amount) &#123; if (coinIndex &lt; 0) &#123; return; &#125; // 如果能整除，直接取完 if (amount % coins[coinIndex] == 0) &#123; minCount = Math.min(minCount, curCount + amount / coins[coinIndex]); return; &#125; // 数量上也是从大往小找 for (int i = amount / coins[coinIndex]; i &gt;= 0; i--) &#123; // 因为接下来至少还会用1个币 if (curCount + i + 1 &gt;= minCount) &#123; break; &#125; helper(coins, coinIndex - 1, curCount + i, amount - i * coins[coinIndex]); &#125; &#125;&#125; 提交OK，执行用时：2 ms，内存消耗：34.7 MB，超过了100.00%的 java 提交，有种又快又好的感觉。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要利用动态规划就可以解决，优化的时候需要注意边界条件，从大到小取值，在时间复杂度上能更加优化。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣309——最佳买卖股票时机含冷冻期]]></title>
    <url>%2F2020%2F01%2F14%2F%E5%8A%9B%E6%89%A3309%E2%80%94%E2%80%94%E6%9C%80%E4%BD%B3%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E6%97%B6%E6%9C%BA%E5%90%AB%E5%86%B7%E5%86%BB%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[这道题主要涉及状态转移方程，想清楚所有状态后，就可以轻松解决。 原题给定一个整数数组，其中第 i 个元素代表了第 i 天的股票价格 。 设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 示例:123输入: [1,2,3,0,2]输出: 3 解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] 原题url：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/ 解题暴力解法一开始我就是想着一共有几种状态，这几种状态分别可以转换为哪些状态： 当前是”持股状态”，可以选择继续不交易，保持”持股状态”，或者选择卖掉之后变成”冷冻状态”； 当前是”冷冻状态”，只能选择继续不交易，变成”不持股状态”； 当前是”不持股状态”，可以选择继续不交易，保持”不持股状态”，或者选择买股票之后变成”持股状态”； 我增加了最后终止条件：如果是最后一天，并且手上持有股票的话，必须卖出，这样可以保证最终利益最大。 接下来看看代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123; int max = 0; public int maxProfit(int[] prices) &#123; if (prices.length == 0) &#123; return 0; &#125; recursiveBuy(-1, false, 0, 0, prices); return max; &#125; public void recursiveBuy( int prePrice, boolean cooldown, int profit, int index, int[] prices) &#123; // 如果到了最后一天，并且手上持有股票的话，必须卖掉 if (index == prices.length - 1) &#123; if (prePrice &gt;= 0 &amp;&amp; !cooldown) &#123; profit = profit + prices[index]; &#125; max = Math.max(max, profit); return; &#125; // 当前持有股票 if (prePrice &gt;= 0) &#123; // 此时可以选择不交易，或者卖掉 // 不交易 recursiveBuy(prePrice, cooldown, profit, index + 1, prices); // 卖掉 recursiveBuy(-1, true, profit + prices[index], index + 1, prices); return; &#125; // 当前不持有股票，可以被动不交易、主动不交易、买 // 如果处于冷冻期，只能被动不交易 if (cooldown) &#123; recursiveBuy(prePrice, false, profit, index + 1, prices); return; &#125; // 不交易 recursiveBuy(prePrice, cooldown, profit, index + 1, prices); // 买 recursiveBuy(prices[index], cooldown, profit - prices[index], index + 1, prices); &#125;&#125; 报了超出时间限制，好的，我们想想怎么优化。 状态转移方程上面暴力解法之所以会超时，因为重复计算了。我一开始的想法是想着记录中间结果，但越想越复杂，忍不住看了别人的思路，真的是让我豁然开朗。那就是状态转移方程。 之前我上面提到的是所有状态可以变成哪些状态，但其实有些地方想的是不清楚的。我们用箭头连接两个状态，箭头开始的那端表示前一天的状态，箭头终止的那端表示当天的状态，那么其内容为： 因为买和卖只是两个操作，我们认为只能在每一天的0点执行，当天的状态就由0点之后的状态来表示。 “冷冻期”状态只能是昨天刚买了股票，也就是”不持股”状态转移过来。 “不持股”状态可以由自己，或者昨天是”持股”状态，今天卖掉，转移过来。 “持股”状态可以由自己，或者昨天是”冷冻期”状态，今天买了，转移过来。 你可能会问，如果这样表示状态转移方程的话，那么第一天可以买入股票就没法解释了。那简单，为了配合这种特殊情况，我们再记录一个更早一天的不持股状态，这样就可以满足了。 接下来看看代码：12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public int maxProfit(int[] prices) &#123; if (prices.length &lt; 2) &#123; return 0; &#125; // 因为每次只涉及到前一天的三个状态值，因此只要三个数字记录即可 /** * 其状态转移方程为： * "冷冻期"只能由"不持股"转换而来。 * "持股"可以由"持股"和"冷冻期"转换而来。 * "不持股"可以由"不持股"和"持股"转换而来。 */ // 定义初始情况 // 不持股 int noStock = 0; // 持股 int hasStock = -prices[0]; // 冷冻期 int cooldown = 0; // 上一次的不持股 int beforeNoStock = 0; for (int i = 1; i &lt; prices.length; i++) &#123; // "不持股"可以由"不持股"和"持股"转换而来。 noStock = Math.max(beforeNoStock, hasStock + prices[i]); // "持股"可以由"持股"和"冷冻期"转换而来。 hasStock = Math.max(hasStock, cooldown - prices[i]); // "冷冻期"只能由"不持股"转换而来。 cooldown = beforeNoStock; // 更新一下"上一次的不持股"状态 beforeNoStock = noStock; &#125; return Math.max(noStock, cooldown); &#125;&#125; 提交OK。 状态转移方程继续优化其实从上面的分析，你隐约可以察觉到，”冷冻期”就是一种特殊的”不持股”状态。根据上面的结论，当你想买股票时，要求的是必须连续两天”不持股”，这点你想通了吗？可能也正因为这一点，我们在上面的代码中才需要记录”上一次的不持股”状态。 既然这样，我们干脆就简化为持股状态和不持股状态两种，其状态转移方程可以描述为： 持股状态可以由自己，或者连续两天为不持股状态，今天买了股票，转移而来。 不持股状态可以由自己，或者前一天为持股状态，今天卖了股票，转移而来。 因为我们记录的是每一天状态所对应的收入，那么所谓的连续两天为不持股状态，就是相当于从两天前收入不变。 接下来看看代码：123456789101112131415161718192021222324252627282930class Solution &#123; public int maxProfit(int[] prices) &#123; if (prices.length &lt; 2) &#123; return 0; &#125; // 这次只有两个状态，但需要记录两天前的不持股收入 // 定义初始情况 // 不持股 int noStock = 0; // 持股 int hasStock = -prices[0]; // 上一次的不持股 int beforeNoStock = 0, temp; for (int i = 1; i &lt; prices.length; i++) &#123; // 记录一下"两天前的不持股"收入 temp = noStock; // "不持股"可以由"不持股"和"持股"转换而来。 noStock = Math.max(noStock, hasStock + prices[i]); // "持股"可以由"持股"和"冷冻期"转换而来。 hasStock = Math.max(hasStock, beforeNoStock - prices[i]); // 更新一下"上一次的不持股"状态 beforeNoStock = temp; &#125; // 最大值一定是最后一天不持股的情况 return noStock; &#125;&#125; 提交OK。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。状态转移应该还是很经典的方法，主要在于是否可以想出所有状态及其转化关系。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>状态转移方程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣300——最长上升子序列]]></title>
    <url>%2F2020%2F01%2F13%2F%E5%8A%9B%E6%89%A3300%E2%80%94%E2%80%94%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[这道题主要涉及动态规划，优化时可以考虑贪心算法和二分查找。 原题给定一个无序的整数数组，找到其中最长上升子序列的长度。 示例:123输入: [10,9,2,5,3,7,101,18]输出: 4 解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。 说明: 可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。 你算法的时间复杂度应该为 O(n2) 。 进阶: 你能将算法的时间复杂度降低到 O(n log n) 吗? 解题暴力法这也是最基础的想法，利用递归，从每一个数开始，一个一个寻找，只要比选中的标准大，那么就以新的数为起点，继续找。全部找完后，找出最长的序列即可。 也看一下代码：123456789101112131415161718192021222324class Solution &#123; public int lengthOfLIS(int[] nums) &#123; // 递归查询 return recursiveSearch(nums, Integer.MIN_VALUE, 0); &#125; public int recursiveSearch(int[] nums, int standard, int index) &#123; if (nums.length == index) &#123; return 0; &#125; // 如果包含当前index的数字，其递增长度 int tokenLength = 0; if (nums[index] &gt; standard) &#123; tokenLength = 1 + recursiveSearch(nums, nums[index], index + 1); &#125; // 如果不包含当前index的数字，其递增长度 int notTokenLength = recursiveSearch(nums, standard, index + 1); // 返回较大的那个值 return tokenLength &gt; notTokenLength ? tokenLength : notTokenLength; &#125;&#125; 提交之后报超出时间限制，这个也是预料到的，那么我们优化一下。 记录中间结果仔细分析一下上面的暴力解法，假设 nums 是：[10,9,2,5,3,7,101,18]，那么从 7 到 101 这个查找，在2、5、3的时候，都曾经查找过一遍。 那么针对这种重复查找的情况，我们可以用一个二维数组，记录一下中间结果，这样就可以达到优化的效果。比如用int[][] result标记为记录中间结果的数组，那么result[i][j]就代表着从 nums[i - 1] 开始，无论包含还是不包含 nums[j] 的最大递增序列长度。这样就能保证不再出现重复计算的情况了。 让我们看看代码：12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public int lengthOfLIS(int[] nums) &#123; // 记录已经计算过的结果 int result[][] = new int[nums.length + 1][nums.length]; for (int i = 0; i &lt; nums.length + 1; i++) &#123; for (int j = 0; j &lt; nums.length; j++) &#123; result[i][j] = -1; &#125; &#125; // 递归查询 return recursiveSearch(nums, -1, 0, result); &#125; public int recursiveSearch(int[] nums, int preIndex, int index, int[][] result) &#123; if (nums.length == index) &#123; return 0; &#125; // 如果已经赋值，说明计算过，因此直接返回 if (result[preIndex + 1][index] &gt; -1) &#123; return result[preIndex + 1][index]; &#125; // 如果包含当前index的数字，其递增序列最大长度 int tokenLength = 0; if (preIndex &lt; 0 || nums[index] &gt; nums[preIndex]) &#123; tokenLength = 1 + recursiveSearch(nums, index, index + 1, result); &#125; // 如果不包含当前index的数字，其递增序列最大长度 int notTokenLength = recursiveSearch(nums, preIndex, index + 1, result); // 返回较大的那个值 result[preIndex + 1][index] = tokenLength &gt; notTokenLength ? tokenLength : notTokenLength; return result[preIndex + 1][index]; &#125;&#125; 提交OK，但是结果感人，几乎是最慢的了，无论时间还是空间上，都只打败了`5%`左右的用户，那就继续优化。 ### 动态规划 假设我知道了从 nums[0] 到 nums[i] 的最大递增序列长度，那么针对 nums[i + 1]，我只要去跟前面的所有数比较一下，找出前面所有数中比 nums[i + 1] 小的数字中最大的递增子序列，再加1就是 nums[i + 1] 对应的最大递增子序列。 这样我只要再记录一个最大值，就可以求出整个数组的最大递增序列了。 让我们看看代码：1234567891011121314151617181920212223242526272829303132class Solution &#123; public int lengthOfLIS(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; // 动态规划，之前几个数字中，有几个比当前数小的，不断更新 // 存储中间结果 int[] dp = new int[nums.length]; // 最大值，因为数组中至少有一个，所以最小是1 int max = 1; // 遍历 for (int i = 0; i &lt; dp.length; i++) &#123; // 当前下标i的最大递增序列长度 int currentMax = 0; for (int j = 0; j &lt; i; j++) &#123; // 如果nums[i]比nums[j]大，那么nums[i]可以加在nums[j]后面，继续构成一个递增序列 if (nums[i] &gt; nums[j]) &#123; currentMax = Math.max(currentMax, dp[j]); &#125; &#125; // 加上当前的数 dp[i] = currentMax + 1; max = Math.max(dp[i], max); &#125; return max; &#125;&#125; 提交OK，执行用时：9 ms，只战胜了75.15%的 java 提交，看来还是可以继续优化的。 贪心算法 + 二分查找贪心算法意味着不需要是最完美的结果，只要针对当前是有效的，就可以了。 我们之前在构造递增序列的时候，其实是在不断根据之前的值进行更新的，并且十分准确。但其实并不需要如此，只要保证序列中每个数都相对较小，就可以得出最终的最大长度。 还是以[10,9,2,5,3,7,101,18,4,8,6,12]举例： 从10到2，都是无法构成的，因为每一个都比之前的小。 当以最小的2作为起点后，2,5、2,3都是可以作为递增序列，但明显感觉2,3更合适，因为3更小。 因为7大于3，因此递增序列增长为2,3,7。 因为101也大于7，因此递增序列增长为2,3,7,101。 因为18小于101，但是大于7，因此我们可以用18替换101，因为18更小，序列更新为2,3,7,18 此时遇到4，4大于3但是小于7，我们可以用它替换7，虽然此时新的序列2,3,4,18并不是真正的结果，但首先长度上没有问题，其次如果出现新的可以排在最后的数，一定是大于4的，因为要先大于现在的最大值18。序列更新为2,3,4,18。 同理，8大于4小于18，替换18，此时新的序列2,3,4,8，这样是不是大家开始懂得了这个规律。 遇到6之后，更新为2,3,4,6。 遇到12后，更新为2,3,4,6,12。 这样也就求出了最终的结果。 结合一下题目说明里提到的O(nlogn)，那么就可以想到二分查找，运用到这里也就是找到当前数合适的位置。 接下来让我们看看代码：12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public int lengthOfLIS(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; // 贪心 + 二分查找 // 一个空数组，用来存储最长递增序列 int[] result = new int[nums.length]; result[0] = nums[0]; // 空数组的长度 int resultLength = 1; // 遍历 for (int i = 1; i &lt; nums.length; i++) &#123; int num = nums[i]; // 如果num比当前最大数大，则直接加在末尾 if (num &gt; result[resultLength - 1]) &#123; result[resultLength] = num; resultLength++; continue; &#125; // 如果和最大数相等，直接跳过 if (num == result[resultLength - 1]) &#123; continue; &#125; // num比最大值小，则找出其应该存在的位置 int shouldIndex = Arrays.binarySearch(result, 0, resultLength, num); if (shouldIndex &lt; 0) &#123; shouldIndex = -(shouldIndex + 1); &#125; // 更新，此时虽然得出的result不一定是真正最后的结果，但首先其resultLength不会变，之后就算resultLength变大，也是相对正确的结果 // 这里的更新，只是为了让result数组中每个位置上的数，是一个相对小的数字 result[shouldIndex] = num; &#125; return resultLength; &#125;&#125; 提交OK，执行用时：2 ms，差不多了。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目用动态规划其实就已经能解决了，但为了优化，还需要用到贪心算法和二分查找。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>贪心算法</tag>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣287——寻找重复数]]></title>
    <url>%2F2020%2F01%2F12%2F%E5%8A%9B%E6%89%A3287%E2%80%94%E2%80%94%E5%AF%BB%E6%89%BE%E9%87%8D%E5%A4%8D%E6%95%B0%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，基于之前142题环形链表II的规律，就能解决了。 原题给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。 示例 1:12输入: [1,3,4,2,2]输出: 2 示例 2:12输入: [3,1,3,4,2]输出: 3 说明： 不能更改原数组（假设数组是只读的）。 只能使用额外的 O(1) 的空间。 时间复杂度小于 O(n2) 。 数组中只有一个重复的数字，但它可能不止重复出现一次。 原题url：https://leetcode-cn.com/problems/find-the-duplicate-number/ 解题普通思路针对说明里的前两条，其实就分别对应了解题的两种思路： 先原地排序，再遍历寻找。 使用集合记录已经出现过的数字。 当然了，既然已经在说明里被禁止了， 那么就应该想想别的思路了。 我还想到了利用 bitMap 的思路，相当于用一个数字的二进制，各个位上是否为1来表示该数字是否出现过。但考虑到 java 里 int 的最大值是 (2^31 - 1)，long 的最大值也不过是(2^63 - 1)，那都是要求 n 不能大于100的。因此，这种思路也是暂时不可取的。 抽象为环形链表II如果将数组的下标和值抽象成链表的话，出现重复数字也就意味着出现链表中有环，那么这道题就是之前做到的力扣142——环形链表II一模一样了。 我们举例说明一下，假设数组是[1, 5, 7, 3, 2, 4, 6, 7]，那么将其转化成的链表就是：0 -&gt; 1 -&gt; 5 -&gt; 7 -&gt; 3 -&gt; 2 -&gt; 4 -&gt; 6 -&gt; 7 ....无限循环。 那我们就可以借助快慢指针： 快指针一次走两步，慢指针一次走一步，直到他们相遇； 快指针再次从起点出发，但此时两个指针都是一次走一步； 当他们再次相遇后，相遇点就是重复的整数。 接下来让我们看看代码：1234567891011121314151617181920212223class Solution &#123; public int findDuplicate(int[] nums) &#123; // 参考环形链表II，利用快慢指针 // 快指针一次走两步，慢指针一次走一步，当他们相遇后 // 快指针再次从起点出发，但此时两个指针都是一次走一步 // 当他们再次相遇后，相遇点就是重复的整数 int slow = nums[0]; int fast = nums[nums[0]]; while (slow != fast) &#123; slow = nums[slow]; fast = nums[nums[fast]]; &#125; fast = 0; while (slow != fast) &#123; slow = nums[slow]; fast = nums[fast]; &#125; return fast; &#125;&#125; 提交OK，时间复杂度为：O(n)，空间复杂度是：O(1)。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目主要还是在于寻找其中的规律，转化为环形链表来思考。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣279——完全平方数]]></title>
    <url>%2F2020%2F01%2F11%2F%E5%8A%9B%E6%89%A3279%E2%80%94%E2%80%94%E5%AE%8C%E5%85%A8%E5%B9%B3%E6%96%B9%E6%95%B0%2F</url>
    <content type="text"><![CDATA[这道题主要利用广度优先搜索进行动态规划，就可以解决了，也可以推导出关系解决。 原题给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, …）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。 示例 1:123输入: n = 12输出: 3 解释: 12 = 4 + 4 + 4. 示例 2:123输入: n = 13输出: 2解释: 13 = 4 + 9. 原题url：https://leetcode-cn.com/problems/perfect-squares/ 解题动态规划 + 广度优先搜索因为累加的都是完全平方数，这样的组合会有很多种，比如12，可以9 + 1 + 1 + 1或者4 + 4 + 4.，甚至可以12个1相加。那么我们进行在进行动态规划的时候，肯定不是算出所有可能，所以就不是使用深度优先搜索了，因为要求个数最少，那么就自然想到广度优先搜索了。优化的话，自然就是先算最大的数，也就是离 n 最近的且比它小的平方数了。 编码的时候需要注意，一般我们使用队列实现广度优先搜索，因为它是先进先出。 其次，我们需要考虑顺序问题，我们让大的平方数尽量靠前出现，比如上面12 = 9 + 1 + 1 + 1，按照广度优先搜索也会出现12 = 1 + 1 + 1 + 9或者12 = 1 + 9 + 1 + 1等等，后面出现的这些情况理论上都可以直接排除掉，我们可以认为后面那些情况都是第一轮是1之后剩余值为11的衍生，那么都可以合并掉。因此我们又增加一个优化条件：如果曾经出现过的剩余值再次出现，可以不用再次考虑，因为一定会在最开始出现的情况中被考虑到。 接下来我们来看看代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.*;class Solution &#123; public int numSquares(int n) &#123; // 利用队列实现广度优先搜索，进行查询。 TreeNode root = new TreeNode(n, 0); // 利用队列进行广度优先搜索 Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); // 存储已经出现过的剩余值 boolean record[] = new boolean[n]; int val, level; int i, result; while (!queue.isEmpty()) &#123; // 移除头节点 TreeNode node = queue.remove(); val = node.val; level = node.level; // 从最大的平方数开始 for (i = (int) Math.sqrt(val); i &gt;= 1; i--) &#123; result = val - i * i; // 剩余值为0，说明可以直接在这一层结束 if (result == 0) &#123; return level + 1; &#125; // result如果之前没有出现过，那么就要算上当前的数字，进入下一层中 if (!record[result]) &#123; record[result] = true; TreeNode nextNode = new TreeNode(result, level + 1); queue.add(nextNode); &#125; &#125; &#125; return -1; &#125;&#125;class TreeNode &#123; // 当前节点的值 int val; // 当前属于第几层，也就是当前是第几个数 int level; public TreeNode(int val, int level) &#123; this.val = val; this.level = level; &#125;&#125; 提交OK。 找规律还有一种就是利用递推公式了，但可能我数学不好，并没有看懂，给大家看一下：12341. 首先初始化长度为 n+1 的数组dp，每个位置都为02. 如果 n 为0，则结果为03. 对数组进行遍历，下标为 i，每次都将当前数字先更新为最大的结果，即 dp[i]=i，比如 i=4，最坏结果为 4=1+1+1+1 即为4个数字4. 动态转移方程为：dp[i] = MIN(dp[i], dp[i - j * j] + 1)，i表示当前数字，j*j表示平方数 这个思路相当于求出了从1到n所有数字的最小平方数的个数。关键在于第4点，也就是后面的计算可以依赖于前面求出的结果，每一个数都找出其所有基于以前求过的数，加上1个完全平方数后，最小的的平方数的个数。 接下来看看代码：123456789101112class Solution &#123; public int numSquares(int n) &#123; int[] dp = new int[n + 1]; // 默认初始化值都为0 for (int i = 1; i &lt;= n; i++) &#123; dp[i] = i; // 最坏的情况就是每次+1 for (int j = 1; i - j * j &gt;= 0; j++) &#123; dp[i] = Math.min(dp[i], dp[i - j * j] + 1); // 动态转移方程 &#125; &#125; return dp[n]; &#125;&#125; 提交OK，但这种方法并没有上面有效率，因为上面可以从最大的平方数找起，只要满足当前n的情况即可，这个的话需要求出所有从1到n的情况，因此你需要看情况选择合适的方法。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目主要还是在于利用广度优先搜索实现动态规划，也可以找规律，本质其实都是动态规划。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>广度优先搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣240——搜索二维矩阵]]></title>
    <url>%2F2020%2F01%2F11%2F%E5%8A%9B%E6%89%A3240%E2%80%94%E2%80%94%E6%90%9C%E7%B4%A2%E4%BA%8C%E7%BB%B4%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[这道题主要是利用搜索二维矩阵本身的特性，找到其中的规律，就可以解决了。 原题编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性： 每行的元素从左到右升序排列。 每列的元素从上到下升序排列。 示例: 现有矩阵 matrix 如下：1234567[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] 给定 target = 5，返回 true。 给定 target = 20，返回 false。 原题url：https://leetcode-cn.com/problems/search-a-2d-matrix-ii/ 解题这道题相比之前的二维矩阵，可能有序性没有之前那么强，所以没法直接拉成一个一维数组利用二分法查找，需要结合其特性，进行查找。 暴力解法就是一行行、一列列慢慢找。假设是一个m * n的二维数组，那么时间复杂度就是O(mn)，这个方法没什么好说的，贴个代码看看：12345678910111213class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; for (int i = 0; i &lt; matrix.length; i++) &#123; for (int j = 0; j &lt; matrix[0].length; j++) &#123; if (matrix[i][j] == target) &#123; return true; &#125; &#125; &#125; return false; &#125;&#125; 行列同时寻找这是我自己想的方法，就是每次查找行列，只查每一行每一列最大值和最小值。根据这个二维数组的特性，找出可能存在 target 的行列的范围，然后逐渐缩小，如果行列相同时，则开始遍历寻找。 让我们看看代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix.length == 0) &#123; return false; &#125; return search(0, matrix.length - 1, 0, matrix[0].length - 1, matrix, target); &#125; private boolean search( int rowStart, int rowEnd, int colStart, int colEnd, int[][] matrix, int target) &#123; if (rowStart == rowEnd || colStart == colEnd) &#123; if (rowStart == rowEnd) &#123; for (int i = colStart; i &lt;= colEnd; i++) &#123; if (matrix[rowStart][i] == target) &#123; return true; &#125; &#125; &#125; else &#123; for (int i = rowStart; i &lt;= rowEnd; i++) &#123; if (matrix[i][colStart] == target) &#123; return true; &#125; &#125; &#125; return false; &#125; // 是否有合适的行，默认没有 boolean has = false; // 新的rowStart、rowEnd int newRowStart = 0, newRowEnd = 0; // 筛选行 for (int i = rowStart; i &lt;= rowEnd; i++) &#123; // 如果相等，说明找到了 if (matrix[i][colStart] == target || matrix[i][colEnd] == target) &#123; return true; &#125; // target大于这一行的最大值，直接跳过 if (target &gt; matrix[i][colEnd]) &#123; continue; &#125; // target小于这一行的最小值，说明之后的就不用找了 if (target &lt; matrix[i][colStart]) &#123; break; &#125; // 如果是第一次找到 if (!has) &#123; has = true; newRowStart = i; &#125; newRowEnd = i; &#125; // 如果没有找到 if (!has) &#123; return false; &#125; has = false; int newColStart = 0, newColEnd = 0; // 筛选列 for (int i = colStart; i &lt;= colEnd; i++) &#123; // 如果相等，说明找到了 if (matrix[newRowStart][i] == target || matrix[newRowEnd][i] == target) &#123; return true; &#125; // target大于这一列的最大值，直接跳过 if (target &gt; matrix[newRowEnd][i]) &#123; continue; &#125; // target小于这一列的最小值，说明之后的就不用找了 if (target &lt; matrix[newRowStart][i]) &#123; break; &#125; // 如果是第一次找到 if (!has) &#123; has = true; newColStart = i; &#125; newColEnd = i; &#125; // 如果没有找到 if (!has) &#123; return false; &#125; return search(newRowStart, newRowEnd, newColStart, newColEnd, matrix, target); &#125;&#125; 时间复杂度上，应该是比最基础的暴力法好一些，但应该也是没有本质区别。 行列同时二分查找以行列总数中较小的那个数，选择构成正方形的正对角线，每一次按照二分法，查找相应的行列，可以参考下面这张图： 每次都会对行和列各用一次二分法，逐步排查。让我们看看代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix == null || matrix.length == 0) &#123; return false; &#125; // 利用二分法搜索 // 寻找出行数、列数中较小的那个值，作为迭代次数 int count = Math.min(matrix.length, matrix[0].length); int low, high, middle; // 迭代循环 for (int i = 0; i &lt; count; i++) &#123; // 从(i,i)开始，寻找这一行是否有target low = i; high = matrix[0].length - 1; while (low &lt;= high) &#123; middle = (low + high) / 2; if (matrix[i][middle] == target) &#123; return true; &#125; if (matrix[i][middle] &gt; target) &#123; high = middle - 1; &#125; else &#123; low = middle + 1; &#125; &#125; // 从(i,i)开始，寻找这一列是否有target low = i; high = matrix.length - 1; while (low &lt;= high) &#123; middle = (low + high) / 2; if (matrix[middle][i] == target) &#123; return true; &#125; if (matrix[middle][i] &gt; target) &#123; high = middle - 1; &#125; else &#123; low = middle + 1; &#125; &#125; &#125; return false; &#125;&#125; 这个方法的时间复杂度为O(lg(n!))。123456789101112这个算法产生的时间复杂度并不是特别明显的是 O(lg(n!)) ，所以让我们一步一步地分析它。在主循环中执行的工作量逐渐最大，它运行 min(m,n)次迭代，其中 m 表示行数，n 表示列数。在每次迭代中，我们对长度为 m-i 和 n-i 的数组执行两次二分查找。因此，循环的每一次迭代都以 O(lg(m-i)+lg(n-i)) 时间运行，其中 i 表示当前迭代。我们可以将其简化为 O(2 lg(n-i))= O(lg(n-i)) ，在最坏的情况是 n≈m 。当 n≪m 时，n 将在渐近分析中占主导地位。通过汇总所有迭代的运行时间，我们得到以下表达式：O(lg(n)+lg(n−1)+lg(n−2)+…+lg(1))然后，我们可以利用对数乘法规则（lg(a)+lg(b)=lg(ab)）将复杂度改写为：O(lg(n)+lg(n−1)+lg(n−2)+…+lg(1))=O(lg(n⋅(n−1)⋅(n−2)⋅…⋅1))=O(lg(1⋅…⋅(n−2)⋅(n−1)⋅n))=O(lg(n!)) 划分为四个二维数组这是一种递归查找，同样也是利用了这个二维搜索数组的特性。我们在当前矩阵中间的列上进行比较，如果小于 target 就继续向下比较，直到找到比 target 大的数，此时递归查找左上和右上的矩阵。 以下面这张图为例，我们假设target = 9，那么中间列找到 10 以后，发现比 9 大，因此寻找左上和右上两个标红的矩阵： 接下来我们看看代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix == null || matrix.length == 0) &#123; return false; &#125; // 利用搜索二维矩阵的特性，划分为2个搜索矩阵进行递归搜索 return searchRecursive(0, 0, matrix[0].length - 1, matrix.length - 1, matrix, target); &#125; /** * 左上角和右下角的坐标，在这个二维矩阵中进行搜索 */ private boolean searchRecursive( int left, int up, int right, int down, int[][] matrix, int target) &#123; // 此时矩阵的长度或者宽度为0，那么就没有必要搜索了 if (left &gt; right || up &gt; down) &#123; return false; &#125; // 如果target小于该矩阵的最小值(左上角)或者大于该矩阵的最大值(右下角)，也没有必要搜索了 if (target &lt; matrix[up][left] || target &gt; matrix[down][right]) &#123; return false; &#125; // 利用列进行拆分 int mid = (left + right) / 2; // 从上到下开始寻找 int row = up; for (; row &lt;= down; row++) &#123; if (matrix[row][mid] == target) &#123; return true; &#125; // matrix[row][mid]作为一个标准点，如果小于target，则继续往下一行寻找 if (matrix[row][mid] &lt; target) &#123; continue; &#125; // 如果matrix[row][mid]大于target，则停止增加 break; &#125; return searchRecursive(left, row, mid - 1, down, matrix, target) || searchRecursive(mid + 1, up, right, row - 1, matrix, target); &#125;&#125; 时间复杂度：O(nlgn)。分析如下： 单向寻找结合该二维数组的特性，我们希望在进行比较的时候，只往一个方向寻找，这样可以简化查询步骤。如果从左上(最小)开始寻找时，如果 target 比当前值大，我们不知道是该往右移还是往下移，右下(最大)同理。 此时我们可以换一种思路，从左下开始，因为比它小的数一定在它右边，比它大的数一定在它右边，这样寻找的时候就简单多了。 接下来我们看看代码：123456789101112131415161718192021222324252627282930313233class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix == null || matrix.length == 0) &#123; return false; &#125; // 从左下角开始寻找，如果当前值大于target，则向上移动一行；如果当前值小于target，则向右移动一列。 // 直到找到target，或者超出矩阵边界 // 列的最大值 int colMax = matrix[0].length - 1; // 行列开始的下标 int row = matrix.length - 1; int col = 0; while (row &gt;= 0 &amp;&amp; col &lt;= colMax) &#123; if (matrix[row][col] == target) &#123; return true; &#125; // 如果大于target，则向上移一行 if (matrix[row][col] &gt; target) &#123; row--; &#125; // 如果小于target，则向右一列 else &#123; col++; &#125; &#125; return false; &#125;&#125; 时间复杂度：O(n+m)。如果和上面一样，假设 n &lt;&lt; m，那么就是 O(m)，总的来说，还是比较高效的。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目主要还是在于利用二维搜索数组的特性，完成解题。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣221——最大正方形]]></title>
    <url>%2F2020%2F01%2F09%2F%E5%8A%9B%E6%89%A3221%E2%80%94%E2%80%94%E6%9C%80%E5%A4%A7%E6%AD%A3%E6%96%B9%E5%BD%A2%2F</url>
    <content type="text"><![CDATA[这道题主要是利用动态规划，注意好边界条件，就可以解决。 原题在一个由 0 和 1 组成的二维矩阵内，找到只包含 1 的最大正方形，并返回其面积。 示例:12345678输入: 1 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0输出: 4 原题url：https://leetcode-cn.com/problems/maximal-square/ 解题动态规划这道题应该很快会让我们想起使用动态规划，从左上角往右下角开始找。 假设我们用一个二维数组dp，记录每一个位置所能构成的最大正方形的边长（从左上角开始算）。位置(i, j)是 1，则其可能构成的正方形的边长是Min(dp(i - 1, j - 1), dp(i - 1, j), dp(i, j - 1)) + 1。看到这个，相信你也会理解了，每一个位置所能构成的最大边长的条件，其实是要求包围着它的左上角三个位置都是 1 才可以。 一直递推回到最初点，也就意味着，第一行和第一列需要单独判断。 之前应该也有做过类似空间复杂度上的优化，我们真的需要一个真正的二维数据dp来记录中间结果吗？其实我们发现，当一个位置用过之后，这个位置本身的数字已经不再重要，关键是该位置所能构成的最大正方形的边长，也就是我们记录的中间结果。因此可以直接更新原数组上的数字。 还有就是需要考虑一些特殊情况，比如只有一行或者一列的时候，我们应该如何做。 接下来让我们看看代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class Solution &#123; public int maximalSquare(char[][] matrix) &#123; // 高 int height = matrix.length; if (height == 0) &#123; return 0; &#125; // 宽 int width = matrix[0].length; // 检查第一行和第一列，是否有'1' int result = checkFirstRowAndCol(matrix, height, width); // 只有一行或一列，或者只有一个 if (height == 1 || width == 1) &#123; return result; &#125; // result现在做为记录最大边的长度 // 中间结果 int temp; for (int i = 1; i &lt; height; i++) &#123; for (int j = 1; j &lt; width; j++) &#123; if (matrix[i][j] == '0') &#123; continue; &#125; // 求出matrix[i - 1][j - 1]、matrix[i - 1][j]、matrix[i][j - 1]中的最小值 temp = matrix[i - 1][j] &lt; matrix[i][j - 1] ? matrix[i - 1][j] : matrix[i][j - 1]; temp = temp &lt; matrix[i - 1][j - 1] ? temp : matrix[i - 1][j - 1]; // 更新当前节点的值 matrix[i][j] = (char) (temp + 1); temp = temp + 1 - '0'; if (result &lt; temp) &#123; result = temp; &#125; &#125; &#125; return result * result; &#125; private int checkFirstRowAndCol(char[][] matrix, int height, int width) &#123; // 检查第一列 for (int i = 0; i &lt; height; i++) &#123; if (matrix[i][0] == '1') &#123; return 1; &#125; &#125; // 检查第一行 for (int i = 0; i &lt; width; i++) &#123; if (matrix[0][i] == '1') &#123; return 1; &#125; &#125; return 0; &#125;&#125; 提交OK，执行用时：5 ms，内存消耗：41.1 MB。 上面的这个代码，我并不是一次性就直接写出来的，也是在不断的提交中，发现有一些特殊情况没有考虑，幸运的是力扣会每次提交完之后会告诉我不满足时的输入的是什么样的，但这也会让我经常考虑不足，还需要努力。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目利用动态规划其实就差不多了，但重点还是在于特殊情况的判断，需要注意。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣208——实现 Trie (前缀树)]]></title>
    <url>%2F2020%2F01%2F08%2F%E5%8A%9B%E6%89%A3208%E2%80%94%E2%80%94%E5%AE%9E%E7%8E%B0%20Trie%20(%E5%89%8D%E7%BC%80%E6%A0%91)%2F</url>
    <content type="text"><![CDATA[这道题主要是构造前缀树节点的数据结构，帮助解答问题。 原题实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。 示例:12345678Trie trie = new Trie();trie.insert(&quot;apple&quot;);trie.search(&quot;apple&quot;); // 返回 truetrie.search(&quot;app&quot;); // 返回 falsetrie.startsWith(&quot;app&quot;); // 返回 truetrie.insert(&quot;app&quot;); trie.search(&quot;app&quot;); // 返回 true 说明: 你可以假设所有的输入都是由小写字母 a-z 构成的。 保证所有输入均为非空字符串。 原题url：https://leetcode-cn.com/problems/implement-trie-prefix-tree/ 解题前缀树的意义我们用前缀树这种数据结构，主要是用在在字符串数据集中搜索单词的场景，但针对这种场景，我们也可以使用平衡树和哈希表，而且哈希表可以在O(1)时间内寻找到键值。那为什么还要前缀树呢？ 原因有3： 前缀树可以找到具有同意前缀的全部键值。 前缀树可以按词典枚举字符串的数据集。 前缀树在存储多个具有相同前缀的键时可以使用较少的空间，只需要O(m)的时间复杂度，其中 m 为键长。在平衡树中查找键值却需要O(m log n)，其中 n 是插入的键的数量；而哈希表随着大小的增加，会出现大量的冲突，时间复杂度可能增加到O(n)。 构造前缀树的节点结构既然是树，肯定也是有根节点的。至于其节点结构，需要有以下特点： 最多 R 个指向子结点的链接，其中每个链接对应字母表数据集中的一个字母。本题中假定 R 为 26，小写拉丁字母的数量。 布尔字段，以指定节点是对应键的结尾还是只是键前缀。 接下来让我们看看节点结构的代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class TrieNode &#123; TrieNode[] nodes; boolean isEnd; public TrieNode() &#123; // 26个小写英文字母 nodes = new TrieNode[26]; // 当前是否已经结束 isEnd = false; &#125; /** * 当前节点是否包含字符 ch */ public boolean contains(char ch) &#123; return nodes[ch - 'a'] != null; &#125; /** * 设置新的下一个节点 */ public TrieNode setNode(char ch, TrieNode node) &#123; // 判断当前新的节点是否已经存在 TrieNode tempNode = nodes[ch - 'a']; // 如果存在，就直接返回已经存在的节点 if (tempNode != null) &#123; return tempNode; &#125; // 否则就设置为新的节点，并返回 nodes[ch - 'a'] = node; return node; &#125; /** * 获取 ch 字符 */ public TrieNode getNode(char ch) &#123; return nodes[ch - 'a']; &#125; /** * 设置当前节点为结束 */ public void setIsEnd() &#123; isEnd = true; &#125; /** * 当前节点是否已经结束 */ public boolean isEnd() &#123; return isEnd; &#125;&#125; 接下来就是真正的前缀树的结构：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Trie &#123; /** * 根节点 */ TrieNode root; /** Initialize your data structure here. */ public Trie() &#123; root = new TrieNode(); &#125; /** Inserts a word into the trie. */ public void insert(String word) &#123; TrieNode before = root; TrieNode node; // 遍历插入单词中的每一个字母 for (int i = 0; i &lt; word.length(); i++) &#123; node = new TrieNode(); node = before.setNode(word.charAt(i), node); before = node; &#125; // 设置当前为终点 before.setIsEnd(); &#125; /** Returns if the word is in the trie. */ public boolean search(String word) &#123; TrieNode before = root; TrieNode temp; // 遍历查找 for (int i = 0; i &lt; word.length(); i++) &#123; temp = before.getNode(word.charAt(i)); if (temp == null) &#123; return false; &#125; before = temp; &#125; // 且最后一个节点也是终点 return before.isEnd(); &#125; /** Returns if there is any word in the trie that starts with the given prefix. */ public boolean startsWith(String prefix) &#123; TrieNode before = root; TrieNode temp; // 遍历查找 for (int i = 0; i &lt; prefix.length(); i++) &#123; temp = before.getNode(prefix.charAt(i)); if (temp == null) &#123; return false; &#125; before = temp; &#125; return true; &#125;&#125; 提交OK，执行用时：43 ms，内存消耗：55.3 MB，虽然只战胜了87.40%的提交，但试了一下最快的那个代码，和我这个方法在时间上基本没什么差别，应该是当初提交的时候测试用例没有那么多吧。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目可能需要专门去理解一下前缀树的用途，这样可以有助于构造前缀树的结构。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>前缀树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣207——课程表]]></title>
    <url>%2F2020%2F01%2F07%2F%E5%8A%9B%E6%89%A3207%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[这道题主要利用拓扑排序，判断该图是否有环，其中还会涉及到邻接矩阵。 原题现在你总共有 n 门课需要选，记为 0 到 n-1。 在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们: [0,1] 给定课程总量以及它们的先决条件，判断是否可能完成所有课程的学习？ 示例 1:123输入: 2, [[1,0]] 输出: true解释: 总共有 2 门课程。学习课程 1 之前，你需要完成课程 0。所以这是可能的。 示例 2:123输入: 2, [[1,0],[0,1]]输出: false解释: 总共有 2 门课程。学习课程 1 之前，你需要先完成​课程 0；并且学习课程 0 之前，你还应先完成课程 1。这是不可能的。 说明: 输入的先决条件是由边缘列表表示的图形，而不是邻接矩阵。详情请参见图的表示法。 你可以假定输入的先决条件中没有重复的边。 提示: 这个问题相当于查找一个循环是否存在于有向图中。如果存在循环，则不存在拓扑排序，因此不可能选取所有课程进行学习。 通过 DFS 进行拓扑排序 - 一个关于Coursera的精彩视频教程（21分钟），介绍拓扑排序的基本概念。 拓扑排序也可以通过 BFS 完成。 原题url：https://my.openwrite.cn/user/article/write 解题这是我第一次遇到图相关的题目，讲道理，有向图、无向图、出度、入度之类的概念还能记得，但是拓扑排序、邻接矩阵、逆邻接矩阵却只是知道有这么一个概念，但具体内容也已经忘光了。我会在下面的解题过程中为大家呈现这些概念。 先介绍一下拓扑排序：123456在图论中，拓扑排序（Topological Sorting）是一个有向无环图（DAG, Directed Acyclic Graph）的所有顶点的线性序列。且该序列必须满足下面两个条件：1. 每个顶点出现且只出现一次。2. 若存在一条从顶点 A 到顶点 B 的路径，那么在序列中顶点 A 出现在顶点 B 的前面。有向无环图（DAG）才有拓扑排序，非DAG图没有拓扑排序一说。 从上面的概念中可以看出，这道题目就是要判断给定的图是否是有向无环图，也就是其是否有拓扑排序。 求一个图是否有拓扑排序，我们一般有两种办法：广度优先搜索 + 邻接矩阵、深度优先搜索 + 逆邻接矩阵。接下来我们逐一来为大家分析： 广度优先搜索 + 邻接矩阵首先看一下什么是邻接矩阵： 在图论中，邻接矩阵（英语：adjacency matrix）是表示一种图结构的常用表示方法。它用数字方阵记录各点之间是否有边相连，数字的大小可以表示边的权值大小。 这么看有点抽象，简单点说：就是一个图中各个节点的后继节点链表。 举个例子： 这样一个图，其邻接矩阵为：12341 -&gt; 2 -&gt; 3 -&gt; null2 -&gt; 4 -&gt; null3 -&gt; 4 -&gt; null4 -&gt; null 好了，弄懂了邻接矩阵，我们来想想如何使用广度优先搜索？ 假设有向图无环，那么从入度为 0 的点，依次删除，这里并不是真正意义上的删除，只是如果该节点消失后，其后继节点的入度需要减1，此时再判断是否又有新的入度为0的节点，如果最终所有节点都会被减到0，那么说明有向图无环，让我们看一下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; public boolean canFinish(int numCourses, int[][] prerequisites) &#123; // 利用入度表和DFS进行拓扑排序 // 下标i对应的节点，其入度的数组 int[] indegressArray = new int[numCourses]; // key:节点i，value:其所有出度节点 Map&lt;Integer, List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;(numCourses * 4 / 3 + 1); for (int[] pre : prerequisites) &#123; indegressArray[pre[0]] += 1; List&lt;Integer&gt; list = map.get(pre[1]); if (list == null) &#123; list = new LinkedList&lt;&gt;(); map.put(pre[1], list); &#125; list.add(pre[0]); &#125; // 将入度为0的入队列 LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); for (int i = 0; i &lt; indegressArray.length; i++) &#123; int indegress = indegressArray[i]; if (indegress != 0) &#123; continue; &#125; list.add(i); &#125; while (!list.isEmpty()) &#123; // 获取第一个节点，并将这个节点"删除" int node = list.removeFirst(); numCourses--; // 那么以node作为前驱节点的节点，其入度-1 List&lt;Integer&gt; preList = map.get(node); if (preList == null) &#123; continue; &#125; for (Integer suffixNode : preList) &#123; indegressArray[suffixNode] -= 1; // 如果该节点入度减为0，则也入队 if (indegressArray[suffixNode] == 0) &#123; list.add(suffixNode); &#125; &#125; &#125; // 如果最终所有节点都入队并且也出队，那么说明该图无环。 return numCourses == 0; &#125;&#125; 提交OK，执行用时：8 ms，内存消耗：45 MB，执行用时只战胜了84.74%的 java 提交记录，我们再优化优化试试。 广度优先搜索 + 邻接矩阵 优化map 虽然理论上查找速度为 O(1)，但需要先计算 hash 值，而数组的话，其获取地址是根据下标的。而我们这里的数字是连续的，并且从 0 开始，因此很适用数组的情况，因此做一个改造：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution &#123; public boolean canFinish(int numCourses, int[][] prerequisites) &#123; // 利用入度表DFS进行拓扑排序 // 下标i对应的节点，其入度的数组 int[] indegressArray = new int[numCourses]; // 邻接矩阵，数组下标:节点i，list:其所有后继节点 List&lt;Integer&gt;[] adjacencyMatrix = new LinkedList[numCourses]; for (int[] pre : prerequisites) &#123; // 入度表相应节点的入度+1 indegressArray[pre[0]] += 1; // 邻接矩阵 List&lt;Integer&gt; list = adjacencyMatrix[pre[1]]; if (list == null) &#123; list = new LinkedList&lt;&gt;(); adjacencyMatrix[pre[1]] = list; &#125; list.add(pre[0]); &#125; // 将入度为0的入队列 LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); for (int i = 0; i &lt; indegressArray.length; i++) &#123; int indegress = indegressArray[i]; if (indegress != 0) &#123; continue; &#125; list.add(i); &#125; while (!list.isEmpty()) &#123; // 获取第一个节点，并将这个节点"删除" int node = list.removeFirst(); numCourses--; // 那么以node作为前驱节点的后继节点，入度-1 List&lt;Integer&gt; preList = adjacencyMatrix[node]; if (preList == null) &#123; continue; &#125; for (Integer prefixNode : preList) &#123; indegressArray[prefixNode] -= 1; // 如果该节点入度减为0，则也入队 if (indegressArray[prefixNode] == 0) &#123; list.add(prefixNode); &#125; &#125; &#125; // 如果最终所有节点都入队并且也出队，那么说明该图无环。 return numCourses == 0; &#125;&#125; 提交OK，执行用时：5 ms，内存消耗：45 MB，执行用时战胜了94.01%的 java 提交记录，应该差不多了。 深度优先搜索既然知道了邻接矩阵，那么逆连接矩阵就是指的各个节点的前驱节点链表。还是以之前的那个例子： 这样一个图，其逆逆邻接矩阵为：12341 -&gt; null2 -&gt; 1 -&gt; null3 -&gt; 1 -&gt; null4 -&gt; 2 -&gt; 3 -&gt; null 那么如何进行深度优先遍历呢？也就是以一个节点出发，访问其相邻节点，一直遍历下去，如果发现一个节点被访问两次，说明有环，那么返回失败，否则就标记该节点已经全部访问完成。当访问完全部节点成功后，说明有向图无环。 这么一看，深度优先遍历的时候，其实只要保证相邻即可，无所谓邻接矩阵还是逆邻接矩阵。 我们来看看代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution &#123; public boolean canFinish(int numCourses, int[][] prerequisites) &#123; // 利用逆邻接矩阵，进行深度优先搜索 HashSet&lt;Integer&gt;[] inadjacencyMatrix = new HashSet[numCourses]; for (int i = 0; i &lt; numCourses; i++) &#123; inadjacencyMatrix[i] = new HashSet&lt;&gt;(); &#125; // 构造逆连接矩阵（每个节点的所有后继节点） for (int[] array : prerequisites) &#123; inadjacencyMatrix[array[1]].add(array[0]); &#125; // 所有节点的被使用情况，如果正在使用的节点，再次被访问，说明有环 // 0：未使用；1：正在使用；2：已经使用完成； int[] used = new int[numCourses]; for (int i = 0; i &lt; numCourses; i++) &#123; if (dfs(i, inadjacencyMatrix, used)) &#123; return false; &#125; &#125; return true; &#125; public boolean dfs(int index, HashSet&lt;Integer&gt;[] inadjacencyMatrix, int[] used) &#123; // 如果当前节点已经处于正在被访问的状态，现在又再次访问，说明有环 if (used[index] == 1) &#123; return true; &#125; // 如果当前节点已经访问结束，则可以不用再次被访问 if (used[index] == 2) &#123; return false; &#125; // used[index] == 0，说明该节点从未被访问过，那么现在开始访问该节点 used[index] = 1; // 深度遍历该节点的后继节点 HashSet&lt;Integer&gt; suffixNodes = inadjacencyMatrix[index]; for (int suffixNode : suffixNodes) &#123; if (dfs(suffixNode, inadjacencyMatrix, used)) &#123; return true; &#125; &#125; // 深度遍历完成该节点，直接结束 used[index] = 2; return false; &#125;&#125; 提交OK，执行用时：8 ms，内存消耗：44.6 MB，优化的话，暂时并没有想到好方法。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这也是我第一次解决图相关的题目，涉及的知识点有些多，需要好好消化。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>拓扑排序</tag>
        <tag>广度优先搜索</tag>
        <tag>深度优先搜索</tag>
        <tag>邻接矩阵</tag>
        <tag>逆邻接矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣152——乘积最大子序列]]></title>
    <url>%2F2020%2F01%2F06%2F%E5%8A%9B%E6%89%A3152%E2%80%94%E2%80%94%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[这道题主要就是利用动态规划进行解答，如果要进行优化，就需要找规律了。 原题给定一个整数数组 nums ，找出一个序列中乘积最大的连续子序列（该序列至少包含一个数）。 示例 1:123输入: [2,3,-2,4]输出: 6解释: 子数组 [2,3] 有最大乘积 6。 示例 2:123输入: [-2,0,-1]输出: 0解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 原题url：https://leetcode-cn.com/problems/maximum-product-subarray/ 解题暴力求解看到这道题，第一眼想到的就是暴力求解，从第一个数字开始，一直连续着求到最后。稍微增加了对于 0 的判断，因为 0 乘以任何数都等于 0，所以只要碰到 0，当前的这次求解就可以停止。让我们看看代码：1234567891011121314151617181920212223242526272829303132333435class Solution &#123; int max = Integer.MIN_VALUE; public int maxProduct(int[] nums) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; if (nums[i] == 0) &#123; if (max &lt; 0) &#123; max = 0; &#125; continue; &#125; dfs(nums, i + 1, nums[i]); &#125; return max; &#125; public void dfs(int[] nums, int index, int total) &#123; // 当前乘积是否最大 if (total &gt; max) &#123; max = total; &#125; // 有没有越界 if (index &gt;= nums.length) &#123; return; &#125; // 当前数字是否是0，是0的话就没有必要继续下去，因为乘积永远为0 if (nums[index] == 0) &#123; return; &#125; dfs(nums, index + 1, total * nums[index]); &#125;&#125; 提交之后，报超出时间限制。看来暴力求解果然不可取，让我们再想想。 动态规划既然不能暴力求解，那我们能不能利用上之前求解的结果呢？没错，这就是动态规划了。 原本想着是逐个求出当前下标下的最大值，但因为是乘积，考虑到负负得正的情况，只记录最大值可能还不够，需要最大值和最小值一起记录。 但根据之前优化的经验，并不需要申请额外的数组存储最大值和最小值，只需要用常数量的空间存储之前的结果，因为题目要求的是连续，只需要记录上一个序号的结果就够了。 接下来看看代码：123456789101112131415161718192021222324class Solution &#123; public int maxProduct(int[] nums) &#123; int n = nums.length; if (n == 0) &#123; return 0; &#125; // 包含上一个位置的数，得出来的最大值和最小值 int dpMax = nums[0], dpMin = nums[0]; // 最终结果的最大值 int max = nums[0]; // 遍历求解 for (int i = 1; i &lt; n; i++) &#123; // 更新 dpMin 的时候需要 dpMax 之前的信息，所以先保存起来 int preMax = dpMax; // 求出 (dpMin * nums[i])、(dpMax * nums[i])、nums[i] 这三个数的最大值和最小值 dpMax = Math.max(dpMin * nums[i], Math.max(dpMax * nums[i], nums[i])); dpMin = Math.min(dpMin * nums[i], Math.min(preMax * nums[i], nums[i])); // 更新最终的最大值 max = Math.max(max, dpMax); &#125; return max; &#125;&#125; 提交OK，执行用时：2 ms，内存消耗：38.1 MB。但似乎还有稳定耗时只要1 ms的解法，看来可以继续优化。 找规律我们设想一下，如果这个整数数组只有正数，那么最大值就只需要将所有数字相乘即可。 如果包含负数，那么需要分成两种情况： 负数为偶数个，因为负负得正，所以依旧将所有数字相乘即可。 负数为奇数个，要么从前往后乘到最后一个负数之前，要么从后往前乘到第一个负数之前。 如果包含 0，那么依旧只需要从前往后和从后往前各乘一遍，只是在遇到 0 的时候，将之前相乘所得到的结果置为 1 即可，这样就可以达到单独计算中间数字连续相乘的效果。 根据上面的规律，其实就是从后往前、从前往后，各乘一遍，找出最大结果即可。接下来看看代码：1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public int maxProduct(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; // 记录中间相乘的结果 int max = 1; // 记录最终的结果 int res = nums[0]; // 从前往后乘一遍 for (int i = 0; i &lt; nums.length; i++) &#123; max *= nums[i]; res = Math.max(res, max); // 如果遇到 0，则将中间记录的结果置为 1 if (max == 0) &#123; max = 1; &#125; &#125; max = 1; // 从后往前乘一遍 for (int i = nums.length - 1; i &gt;= 0; i--) &#123; max *= nums[i]; res = Math.max(res, max); // 如果遇到 0，则将中间记录的结果置为 1 if (max == 0) &#123; max = 1; &#125; &#125; return res; &#125;&#125; 提交OK，执行用时：1 ms，内存消耗：36.3 MB。这个方法真的是又快又省空间，只是需要我们耐心寻找其中的规律。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。一般来说利用动态规划就够了，如果想继续优化，就需要寻找其中的规律了。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣148——排序链表]]></title>
    <url>%2F2020%2F01%2F05%2F%E5%8A%9B%E6%89%A3148%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[这道题主要就是在链表上利用归并排序和快速排序的变形，解决排序问题。 原题在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。 示例 1:12输入: 4-&gt;2-&gt;1-&gt;3输出: 1-&gt;2-&gt;3-&gt;4 示例 2:12输入: -1-&gt;5-&gt;3-&gt;4-&gt;0输出: -1-&gt;0-&gt;3-&gt;4-&gt;5 原题url：https://leetcode-cn.com/problems/sort-list/ 解决题目很明确，排序，对于时间复杂度和空间复杂度有要求，针对O(n log n)，让我想到了归并排序和快速排序，接下来我们各自来看看。 对了，这里先统一放一下节点类，单向链表中的节点，存储当前节点的值和后一个节点的引用。123456Definition for singly-linked list.public class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; &#125;&#125; 归并排序归并排序，说白了，就是先分解到最小单元，然后逐个进行合并并排序，这样在合并的时候，其实两个链表本身就是有序的，那么当有一个全部取完后，另一个可以直接拼接在最后面。 让我们看一下代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Solution &#123; public ListNode sortList(ListNode head) &#123; // 归并排序 if (head == null || head.next == null) &#123; return head; &#125; // 先分隔，利用快慢指针分隔。 // 快指针先走，因为只有当空节点或1个节点才是终止条件，2个节点的时候，如果不让快指针先走，而是也指向head，那么2个节点永远不会被分隔，会陷入死循环 ListNode fast = head.next.next; ListNode slow = head; while (true) &#123; if (fast == null || fast.next == null) &#123; break; &#125; fast = fast.next.next; slow = slow.next; &#125; // 后半部分的开头 ListNode second = slow.next; second = sortList(second); // 前半部分的开头 slow.next = null; ListNode first = head; first = sortList(first); // 合并 ListNode result = new ListNode(0); head = result; while (first != null &amp;&amp; second != null) &#123; if (first.val &lt; second.val) &#123; result.next = first; first = first.next; &#125; else &#123; result.next = second; second = second.next; &#125; result = result.next; &#125; if (first != null) &#123; result.next = first; &#125; else &#123; result.next = second; &#125; return head.next; &#125;&#125; 提交OK，执行用时：5 ms，内存消耗：39.6 MB，执行用时只战胜了59.07%的 java 提交记录，应该还有优化的空间。 归并排序——优化针对上面的代码，在分隔的时候，设置fast = head.next.next，这是因为我们设置的递归终止条件是针对null或者单个节点的。其实当只剩下两个节点的时候，就可以进行排序了，这样应该可以节省近一半的时间，当然了，从时间复杂度上来说并没有改变。 我们看一下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Solution &#123; public ListNode sortList(ListNode head) &#123; // 归并排序 if (head == null || head.next == null) &#123; return head; &#125; // 说明只有两个节点 if (head.next.next == null) &#123; ListNode second = head.next; if (head.val &gt; second.val) &#123; return head; &#125; else &#123; second.next = head; head.next = null; return second; &#125; &#125; // 先分隔，利用快慢指针分隔。 ListNode fast = head; ListNode slow = head; while (true) &#123; if (fast == null || fast.next == null) &#123; break; &#125; fast = fast.next.next; slow = slow.next; &#125; // 后半部分的开头 ListNode second = slow.next; second = sortList(second); // 前半部分的开头 slow.next = null; ListNode first = head; first = sortList(first); // 合并 ListNode result = new ListNode(0); head = result; while (first != null &amp;&amp; second != null) &#123; if (first.val &lt; second.val) &#123; result.next = first; first = first.next; &#125; else &#123; result.next = second; second = second.next; &#125; result = result.next; &#125; if (first != null) &#123; result.next = first; &#125; else &#123; result.next = second; &#125; return head.next; &#125;&#125; 执行用时，有的时候是4 ms，有的时候是3 ms，看来归并排序这条路差不多就是这样了。 快速排序快速排序的思想就是选择一个标准值，将比它大的和比它的小的，做交换。针对链表这种结构，就是将比它大的放在一个链表中，比它小的放在一个链表中，和它一样大的，放在另一个链表中。然后针对小的和大的链表，继续排序。最终将三个链表按照小、相等、大进行连接。 接下来让我们看看代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123; public ListNode sortList(ListNode head) &#123; // 利用快排 // 单个节点是终止节点 if (head == null || head.next == null) &#123; return head; &#125; // 比标准值小的节点 ListNode lowHead = new ListNode(0); ListNode low = lowHead; // 和标准值一样的节点 ListNode midHead = new ListNode(0); ListNode mid = midHead; // 比标准值大的节点 ListNode highHead = new ListNode(0); ListNode high = highHead; // 标准值 int val = head.val; ListNode node = head; // 遍历 while (node != null) &#123; // 比标准值大的节点 if (node.val &gt; val) &#123; high.next = node; high = high.next; &#125; // 比标准值小的节点 else if (node.val &lt; val) &#123; low.next = node; low = low.next; &#125; // 和标准值一样的节点 else &#123; mid.next = node; mid = mid.next; &#125; node = node.next; &#125; // 终止，避免造成环 low.next = null; high.next = null; lowHead.next = sortList(lowHead.next); highHead.next = sortList(highHead.next); // 找出小节点链表的末尾 low = lowHead; while (low.next != null) &#123; low = low.next; &#125; // 拼接 low.next = midHead.next; mid.next = highHead.next; return lowHead.next; &#125; &#125; 提交OK，执行用时：2 ms，内存消耗：40.01 MB。 和归并排序相比，时间更短，至于原因，我确实是没有想明白，因为都需要比较，然后重新构造新链表。我猜测是测试数据离散程度更高，这样归并排序的话，并没有充分利用其特性：1当两个链表合并时，如果一个链表已经全部结束，另一个链表剩余的部分可以直接拼接。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。针对它的时间复杂度要求，利用归并排序或者快速排序解决。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>归并排序</tag>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣142——环形链表 II]]></title>
    <url>%2F2020%2F01%2F04%2F%E5%8A%9B%E6%89%A3142%E2%80%94%E2%80%94%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8%20II%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，推导出路径之间的关系，从而解决问题。 原题给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 说明：不允许修改给定的链表。 示例 1：123输入：head = [3,2,0,-4], pos = 1输出：tail connects to node index 1解释：链表中有一个环，其尾部连接到第二个节点。 示例 2：123输入：head = [1,2], pos = 0输出：tail connects to node index 0解释：链表中有一个环，其尾部连接到第一个节点。 示例 3：123输入：head = [1], pos = -1输出：no cycle解释：链表中没有环。 进阶： 你是否可以不用额外空间解决此题？ 原题url：https://leetcode-cn.com/problems/linked-list-cycle-ii/ 解题在这里贴一下题目所提供的节点结构，这样下面的代码就不重复贴了：123456789Definition for singly-linked list.class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; next = null; &#125;&#125; 利用集合拿到题目的时候，一开始想到的就是利用集合，存储已经遍历过的节点，如果访问到 null，说明不是环；如果添加失败，说明已经添加过，那么一定是环，并且该节点就是环的入口； 顺便说一句，我认为集合所占空间应该不是很大，因为它只是存储对象的应用地址，当然了，集合本身也是一个新的对象，也会占用额外的空间。 让我们看看代码：12345678910111213141516171819202122public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; if (head == null) &#123; return null; &#125; ListNode current = head; Set&lt;ListNode&gt; set = new HashSet&lt;&gt;(); while (current != null) &#123; // 添加成功，则继续访问下一个节点 if (set.add(current)) &#123; current = current.next; continue; &#125; // 添加不成功，说明重复 return current; &#125; return null; &#125;&#125; 提交OK，执行用时：5 ms，内存消耗：37.7 MB，但是提交用时只战胜了30.99%的 java 提交记录，看来有必要优化一下。 找规律以前我们判断链表是否有环，都是通过快慢指针最终是否相等。现在的话，因为环可能并不是首尾相连，所以只找一次可能不够了，需要继续寻找规律。 我们假设一开始 slow 指针走过的路程为 x，那么 fast 指针走过的路程就为 2x，即：12s = x;f = 2x; 如果 fast 指针最终为 null，那么说明不是环。 如果 fast、slow 指针最终指向的节点相等，说明有环，并且， fast 指针比 flow 指针多走了 n 圈环的长度，那么我们假设环的长度为 b，那么可以得出：1f = x + nb; 可以得出：s = nb; 以上就是最重要的结论了，slow 指针其实也已经走了 n 圈环的长度了。那么，我们再假设从 head 节点到环入口节点的长度为 a，那么从快慢指针相遇节点再走 a 步，最终会走到哪儿呢？ 最终也会走到环的入口节点，因为(nb + a)可以理解为(a + nb)，相当于从 head 节点出发，达到环的入口节点处，又绕环走了 n 圈，所以也会走到环的入口。所以此时我们也找到环的入口节点了。 接下来让我们看看代码：12345678910111213141516171819202122232425262728293031public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; // 先利用快慢指针，如果最终能相遇，说明有环 ListNode slow = head; ListNode fast = head; while (true) &#123; // 快指针为null，说明没有环 if (fast == null || fast.next == null) &#123; return null; &#125; // 慢指针移动一步 slow = slow.next; // 快指针移动两步 fast = fast.next.next; // 快慢指针相等，说明相遇 if (fast == slow) &#123; break; &#125; &#125; // 再用两个指针，一个从头结点出发，一个从相遇点出发，两个指针每次移动1步，两个指针相遇的地方为环的入口 slow = head; while (slow != fast) &#123; slow = slow.next; fast = fast.next; &#125; return slow; &#125;&#125; 提交OK，执行用时：1 ms，内存消耗：37.8 MB，但是提交用时只战胜了55.14%的 java 提交记录，难道还有更加高效的方法？ 我找了一个执行用时 0 ms 的代码，发现就是和我这个类似的，我将它的代码再次提交后，发现和我这个提交结果一样。看来那些比我们快的算法，可能是因为提交时间比较早，测试案例并不像现在那么多，所以不必担心了。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目不仅要利用快慢指针，还要总结规律，最终也能解决，总的来说是一道很考验逻辑思维的题目。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣139——单词拆分]]></title>
    <url>%2F2020%2F01%2F03%2F%E5%8A%9B%E6%89%A3139%E2%80%94%E2%80%94%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%2F</url>
    <content type="text"><![CDATA[这道题主要就是利用动态规划，能够利用上之前的结果，进行优化。 原题给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。 说明： 拆分时可以重复使用字典中的单词。 你可以假设字典中没有重复的单词。 示例 1：123输入: s = &quot;leetcode&quot;, wordDict = [&quot;leet&quot;, &quot;code&quot;]输出: true解释: 返回 true 因为 &quot;leetcode&quot; 可以被拆分成 &quot;leet code&quot;。 示例 2：1234输入: s = &quot;applepenapple&quot;, wordDict = [&quot;apple&quot;, &quot;pen&quot;]输出: true解释: 返回 true 因为 &quot;applepenapple&quot; 可以被拆分成 &quot;apple pen apple&quot;。 注意你可以重复使用字典中的单词。 示例 3：12输入: s = &quot;catsandog&quot;, wordDict = [&quot;cats&quot;, &quot;dog&quot;, &quot;sand&quot;, &quot;and&quot;, &quot;cat&quot;]输出: false 原题url：https://leetcode-cn.com/problems/word-break/ 解题暴力法我拿到这道题目时，第一个想到的就是暴力法，从下标0开始，每一个字母都去找，如果找到，就以当前的结尾下标作为下一次的开始下标，继续查找，看起来还是很简单的。 让我们来看看代码：123456789101112131415161718192021222324252627class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; // 暴力法，从第一个字母开始尝试，长度递增，如果wordDict中存在，则继续往后找，直到找到最后一个字母 return dfs(s, new HashSet&lt;&gt;(wordDict), 0); &#125; /** * s：待查找的字符串 * wordDict：字典集合 * start：本次查找开始的下标 */ public boolean dfs(String s, Set&lt;String&gt; wordDict, int start) &#123; // 全部查找完成 if (start == s.length()) &#123; return true; &#125; for (int end = start + 1; end &lt;= s.length(); end++) &#123; // 当前单词包含在字典表中，并且一路查下去，都能找到 if (wordDict.contains(s.substring(start, end)) &amp;&amp; dfs(s, wordDict, end)) &#123; return true; &#125; &#125; return false; &#125;&#125; 提交之后，报超出时间限制，我们分析一下，既然是暴力解法，针对特殊情况，一定效率很低。 在我们这里，那就是如果原字符串s里包含一连串相同的字母，比如：aaaaaaaaaaaaaaaaaaqqqq，并且字典里有一项就是相匹配的单个字母，比如：a，那么这个递归查找，就会针对每一个下标都去寻找一次，并且一旦失败，在下一次遍历中，又会重新查找一遍。 这时我们进行复杂度分析： 时间复杂度为：O(n^n)，其中，n代表原字符串s的长度，此时就是最坏的情况。 空间复杂度为：O(n)，这代表递归回溯时，最多只会有 n 个调用栈同时存在。 初步优化——利用之前的结果既然上面说每次失败后，都会重新进行查找，并且明显存在重复，那么我们这一次优化的目标，就是尽可能利用之前的结果，那么该如何利用呢？ 因为我们这是在从前向后查找，那么可以想到的就是后缀匹配，也就是利用一个记忆数组，记录每一个节点到最后一个节点是否是可以被查到的，这样在进行第一次遍历的时候，就可以得出一些中间结果，被使用到之后的查找中。 让我们来看看代码：123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; // 记忆回溯，增加一个后缀记忆的Boolean数组，当下标i的值为true，说明从i开始可以一直找到最后，这样针对后缀可以避免重复查找 return dfs(s, new HashSet&lt;&gt;(wordDict), 0, new Boolean[s.length()]); &#125; /** * s：待查找的字符串 * wordDict：字典集合 * start：本次查找开始的下标 * suffixMemoryArray：记忆后缀数组，当下标i的值为true，说明从i开始可以一直找到最后 */ public boolean dfs(String s, Set&lt;String&gt; wordDict, int start, Boolean[] suffixMemoryArray) &#123; // 全部查找完成 if (start == s.length()) &#123; return true; &#125; // 从当前start是否可以直接找到最后，不等于null，说明已经查找过了，直接利用之前的结果 if (suffixMemoryArray[start] != null) &#123; return suffixMemoryArray[start]; &#125; for (int end = start + 1; end &lt;= s.length(); end++) &#123; // 当前单词包含在字典表中，并且一路查下去，都能找到 if (wordDict.contains(s.substring(start, end)) &amp;&amp; dfs(s, wordDict, end, suffixMemoryArray)) &#123; // 说明从当前start能一直从字典中找完 suffixMemoryArray[start] = true; return true; &#125; &#125; suffixMemoryArray[start] = false; return false; &#125;&#125; 提交OK，执行用时：6 ms，内存消耗：37 MB，执行用时只战胜了73.92%的 java 提交记录，看来还可以继续优化。 让我们再看一下复杂度： 时间复杂度：O(n^2)，因为可以利用之前的结果，最多只是进行双重 for 循环。 空间复杂度为：O(n)，这个和之前一样的。 继续优化——后缀改为前缀之前用的是后缀记忆，那么能不能用前缀记忆呢？这就相当于把问题拆分，原本的字符串s，拆分为前部分s1和后部分s2，如果s1和s2都能被找到，则说明s能被找到，然后继续拆分。 将前缀查找过的部分，也记录下来。我们来看看代码：123456789101112131415161718192021class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; // 动态规划，将原本的s拆分为s1和s2，如果s1和s2都包含，则认为s是可以的，本质是前缀匹配 // 字典集合 Set&lt;String&gt; wordDictSet = new HashSet&lt;&gt;(wordDict); // 后缀记忆，默认prefixMemoryArray[0]是存在于字典中的，因为0代表着空，下标为i时，代表从0到i的字符可以直接或者被拆分后在wordDict中找到 boolean[] prefixMemoryArray = new boolean[s.length() + 1]; prefixMemoryArray[0] = true; // 遍历 for (int i = 1; i &lt;= s.length(); i++) &#123; for (int j = 0; j &lt; i; j++) &#123; if (prefixMemoryArray[j] &amp;&amp; wordDictSet.contains(s.substring(j, i))) &#123; prefixMemoryArray[i] = true; &#125; &#125; &#125; return prefixMemoryArray[s.length()]; &#125;&#125; 提交OK，执行用时：7 ms，内存消耗：36.4 MB，执行用时只战胜了56.96%的 java 提交记录，还不如上面那种，只是在空间上有所优化，毕竟将递归改造了一下，看来还可以继续优化。 让我们再看一下复杂度： 时间复杂度：O(n^2)，还是和之前一样的。 空间复杂度为：O(n)，这个和之前一样的。 最终优化——找出快速失败的条件感觉思路上应该没有问题，估计在边界判定时有一些点我们可能还没有看到。 既然是要在字典集合中存在，那么如果需要查找的字符串太长或太短肯定都不可以，即不可以超过字典里最长的单词，也不可以少于字典里最短的单词。 针对上面的解法，就是： j 的初始值应该是 0 和 (i - max) 中的最大值，因为如果 j 太小，那么后续需要查找的字符串可能太长，那么就没必要找了。 j 的最大值应该是 (i - j &gt;= min)，因为如果后续需要查找的字符串可能太短，也没必要查找了。 让我们看看代码：1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; // 动态规划，将原本的s拆分为s1和s2，如果s1和s2都包含，则认为s是可以的，本质是前缀匹配 // 字典集合 Set&lt;String&gt; wordDictSet = new HashSet&lt;&gt;(); // 找出wordDict中最大长度和最小长度 int max = 0; int min = Integer.MAX_VALUE; for (String word : wordDict) &#123; if (word.length() &gt; max) &#123; max = word.length(); &#125; if (word.length() &lt; min) &#123; min = word.length(); &#125; wordDictSet.add(word); &#125; // 后缀记忆，默认prefixMemoryArray[0]是存在于字典中的，因为0代表着空，下标为i时，代表从0到i的字符可以直接或者被拆分后在wordDict中找到 boolean[] prefixMemoryArray = new boolean[s.length() + 1]; prefixMemoryArray[0] = true; // 遍历 for (int i = 1; i &lt;= s.length(); i++) &#123; // 如果需要查找的长度大于最大值或者小于最小值，就没必要继续找了 for (int j = Math.max(0, i - max); i - j &gt;= min; j++) &#123; if (prefixMemoryArray[j] &amp;&amp; wordDictSet.contains(s.substring(j, i))) &#123; prefixMemoryArray[i] = true; &#125; &#125; &#125; return prefixMemoryArray[s.length()]; &#125;&#125; 提交OK，执行用时：2 ms，内存消耗：34.4 MB，执行用时战胜了97.96%的 java 提交记录，这应该没什么问题了。 时间复杂度和空间复杂度还是和上面一样，没有本质区别。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目主要利用动态规划，复用之前计算的结果，再找出更加合适的边界条件，快速失败，最终得到比较合适的方案。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣105——从前序与中序遍历序列构造二叉树]]></title>
    <url>%2F2020%2F01%2F02%2F%E5%8A%9B%E6%89%A3105%E2%80%94%E2%80%94%E4%BB%8E%E5%89%8D%E5%BA%8F%E4%B8%8E%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，能否思考出前序遍历、中序遍历与二叉树之间的关系。 原题根据一棵树的前序遍历与中序遍历构造二叉树。 注意:你可以假设树中没有重复的元素。 例如，给出12前序遍历 preorder = [3,9,20,15,7]中序遍历 inorder = [9,3,15,20,7] 返回如下的二叉树：12345 3 / \9 20 / \ 15 7 原题url：https://leetcode-cn.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/ 解题这道题目，主要就是在于大家对于二叉树这个数据结构的熟悉程度了，根据其前序遍历和中序遍历，推算出原本的二叉树。 我们想想，如果不是写代码，只是通过手写的话，我们是如何查找的，就用题目给出的例子： 根据前序遍历，第一个一定是根节点，那么 3 就是根节点。 从中序遍历中寻找 3，在它左边的，都是其左子树上的节点，在它右边的，都是其右子树上的节点。 因为中序遍历中，3 的左边只有9，那么 9 就是 3 的左子节点。 根据前序遍历先根然后左子节点，然后再右子节点的规律，3 、9 之后的 20 一定是 3 的右子节点。 20 在中序遍历中，其左右两边就是 15 和 7，因此15 和 7 就分别是它的左右子节点。 根据上面的分析，你就可以画出例子中的二叉树了。 那么我们寻找的顺序是，先从前序遍历的第一个节点开始，在中序遍历中找出它的位置，其左右两边就是其左右子树了， 接着从左子树入手，前序遍历根节点之后的两个节点应该就是其左右子树，但需要考虑没有左右子树的情况，然后再以其子树为根，在中序遍历中找其左右子树。 需要注意的是，只有针对根节点，其左右子节点是在前序遍历中紧跟着根节点的，其他都是有距离的，需要根据左子树递推。 接下来看看代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; int[] preorder; int[] inorder; // preorder中遍历过的数量，这样找完左子树中的节点，剩下的第一个节点，必然是右子节点 int preorderIndex = 0; // key为inorder中的值，value为inorder中的下标 Map&lt;Integer, Integer&gt; inorderMap; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; if (preorder.length == 0) &#123; return null; &#125; TreeNode node = new TreeNode(preorder[0]); if (preorder.length == 1) &#123; return node; &#125; this.preorder = preorder; this.inorder = inorder; this.inorderMap = new HashMap&lt;&gt;(inorder.length * 4 / 3 + 1); for (int i = 0; i &lt; inorder.length; i++) &#123; this.inorderMap.put(inorder[i], i); &#125; return generateNode(0, inorder.length); &#125; /** * 寻找当前节点的左右子节点 * start：inorder里开始寻找的节点下标 * end：inorder里终止寻找的节点下标 * preorderIndex：当前节点在preorder中的下标 */ public TreeNode generateNode(int start, int end) &#123; if (start &gt;= end) &#123; return null; &#125; // 当前节点 int value = preorder[preorderIndex]; TreeNode node = new TreeNode(value); // 当前节点的值，在inorder中的下标 int inorderIndex = inorderMap.get(value); // 当前节点已经找到，寻找下一个节点。 // 因为会先去寻找左节点，当该节点左子树中所有节点全部找完后，前序遍历中，剩下节点的第一个节点，一定是该节点的右节点。 preorderIndex++; // 寻找左节点 node.left = generateNode(start, inorderIndex); // 寻找右节点 node.right = generateNode(inorderIndex + 1, end); return node; &#125;&#125; 提交OK，执行用时：3 ms，内存消耗：40.1 MB。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目主要就是寻找规律，优化的话，可能就是利用 map 构造中序遍历中节点值和顺序的关系。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣96——不同的二叉搜索树]]></title>
    <url>%2F2020%2F01%2F01%2F%E5%8A%9B%E6%89%A396%E2%80%94%E2%80%94%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，找出递推关系。 原题给定一个整数 n，求以 1 … n 为节点组成的二叉搜索树有多少种？ 示例:12345678910输入: 3输出: 5解释:给定 n = 3, 一共有 5 种不同结构的二叉搜索树: 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 原题url：https://leetcode-cn.com/problems/unique-binary-search-trees/ 解题这道题看到的第一眼，就和之前的格雷编码一样，又想用动态规划，每次都是遍历所有情况去检查是否有效，但感觉时间复杂度会很高，找找看有没有什么更高效的做法。 所谓高效，也就是寻找规律了，最好的是可以递推，下一次运算可以利用之前的结果，而本题就是用这种规律的。我们来试试 n 等于0、1、2、3的情况：123456789101112131415n = 0，只有1种。n = 1，也只有1种。n = 2，有2种1 2 \ / 2 1n = 3，有5种 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 让我们回想一下什么叫二叉搜索树，就是针对每个节点，其左子树中所有节点都比它小，其右子树中所有节点都比它大。 再想一下，如果我们针对根选中的情况下，左右子树节点的个数其实也已经定下来了，那么假设同样是 3 个节点，”1、2、3”和”4、5、6”可以组成二叉搜索树，从数量上讲是一样的，因为大小关系没有变。 因此，我们可以说，针对二叉搜索树，其不用考虑值具体是多少，只需要考虑其大小关系即可，那么这就符合上面我所希望的场景了，下一次的运算可以利用之前的结果。 以这道题来说，其具体规律就是： 从 1 开始遍历直至 n，以每个节点作为根节点，这样就能计算出左右各个子树的所有节点数。 当我们知道了个数，也就可以利用之前计算的结果，获得左右子树可能的情况，两者相乘，也就是在当前根的情况，所有二叉搜索树的情况。 将所有根节点的总计算出的数量做累加，也就得出了当前节点数的总情况。 让我们看看代码：12345678910111213141516171819class Solution &#123; public int numTrees(int n) &#123; // 存放中间结果 int[] result = new int[n + 1]; result[0] = 1; result[1] = 1; for (int i = 2; i &lt;= n; i++) &#123; int count = 0; for (int j = 1; j &lt;= i; j++) &#123; // 左子树总节点数 + 右子树总节点数 count += (result[j - 1] * result[i - j]); &#125; result[i] = count; &#125; return result[n]; &#125;&#125; 提交OK， 执行用时：0 ms，内存消耗：33.2 MB。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目只要利用规律，构造递推关系，也就能解决了。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>递推</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年终总结]]></title>
    <url>%2F2019%2F12%2F31%2F2019%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[2019即将过去，2020即将，看看这一年我们做了什么，下一年我们要去做什么 2019年终于要过去了，不知道大家过的如何，我感觉2019年对于真的是不一样的一年。 1、从差不多9月份开始写公众号文章，粉丝数终于在今天突破了100人，这对于我来说真的是一份不小的收获。有一种说法叫”输出倒逼输入”，为了写这些文章，我也是看了不少内容，真的感觉收获满满。 2、换了一份工作。其实从夏天就在南京面试了不少公司，但一直没有找到合适的，年末又来上海找工作，想着一定要进入大厂，终于在11月份入职携程。南京到上海，换了工作，也换了居住地，希望可以有一个不一样的自己。 3、对于即将到来的2020，也给自己定下了三个目标：好好工作、学英语、刷力扣。 “好好工作”是希望能够学习大厂里优秀之处，提高自己解决问题、与人沟通的能力； “学英语”是希望自己能在未来找到一份外企的工作； “刷力扣”是针对现在的大厂或者外企面试，算法题是绕不开的； 4、针对公众号，现在开启的力扣系列，也是督促自己不停刷题。同时也希望粉丝数能在2020年底超过300，我这么佛系推广，如果能达到300，也是说明我的文章真的是有一定价值的。这也是我对于自己副业的一种培养，现今社会讲究”斜杠青年”，也是因为如今资本环境不好，裁员屡见不鲜，我们程序员也是该为自己多找几条后路了。 2019对于我来说是一个新的开始，希望能在2020，我们能够成长为更好的自己。]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣90——子集 II]]></title>
    <url>%2F2019%2F12%2F31%2F%E5%8A%9B%E6%89%A390%E2%80%94%E2%80%94%E5%AD%90%E9%9B%86%20II%2F</url>
    <content type="text"><![CDATA[这道题主要就是利用递归，优化的时候需要利用数据结构的特性。 原题给定一个可能包含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 说明：解集不能包含重复的子集。 示例:12345678910输入: [1,2,2]输出:[ [2], [1], [1,2,2], [2,2], [1,2], []] 原题url：https://leetcode-cn.com/problems/subsets-ii/ 解题递归这道题，针对已经刷了不少题目的我们而言，应该第一想到的就是递归了，从第1个数开始，每次遍历1个数，如果和之前的数相同则跳过，然后以下一个数为起点，继续遍历。让我们来看看代码：12345678910111213141516171819202122232425262728293031323334class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; // 从小到大排序 Arrays.sort(nums); // 最终结果 List&lt;List&lt;Integer&gt;&gt; result = new LinkedList&lt;&gt;(); result.add(new LinkedList&lt;&gt;()); // 回溯 dfs(0, nums, new Stack&lt;&gt;(), result); return result; &#125; public void dfs(int index, int[] nums, Stack&lt;Integer&gt; stack, List&lt;List&lt;Integer&gt;&gt; result) &#123; if (index &gt;= nums.length) &#123; return; &#125; for (int i = index; i &lt; nums.length; i++) &#123; // 在这一次总的查找中，如果当前节点和上一个节点相同，则跳过 if (i &gt; index &amp;&amp; nums[i] == nums[i - 1]) &#123; continue; &#125; // 添加该数 stack.push(nums[i]); // 作为一种情况，放进结果中 result.add(new LinkedList&lt;&gt;(stack)); // 继续回溯 dfs(i + 1, nums, stack, result); // 回退 stack.pop(); &#125; &#125;&#125; 提交OK，执行用时：2 ms，内存消耗：36.5 MB，但执行用时只战胜40.16%，那就来优化一下。 优化看了第一眼，我真的不知道该如何优化。我先是想到将递归改成迭代，但感觉并没有从时间上做出优化，不过还是给大家看一下：12345678910111213141516171819202122232425262728293031323334class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return new LinkedList&lt;&gt;(); &#125; // 从小到大排序 Arrays.sort(nums); // 最终结果 List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(1 &lt;&lt; nums.length); result.add(0, new LinkedList&lt;&gt;()); // 上一步新解的开始下标 int newStartIndex = 1; // 遍历添加 for (int i = 0; i &lt; nums.length; i++) &#123; int j = 0; // 和上一个数字相同，则只针对上一步的新解增加 if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1]) &#123; j = newStartIndex; &#125; int length = result.size(); newStartIndex = length; for (;j &lt; length; j++) &#123; List&lt;Integer&gt; tempList = result.get(j); List&lt;Integer&gt; newList = new LinkedList&lt;&gt;(tempList); newList.add(nums[i]); result.add(newList); &#125; &#125; return result; &#125;&#125; 提交之后，果然不出所料，和之前一样，那就再让我们想想。 还记得在之前文章中曾经说过，new LinkedList&lt;&gt;(Collection&lt;? extends E&gt; c)其内部依旧是遍历，很耗性能。因此我专门看了一下new ArrayList&lt;&gt;(Collection&lt;? extends E&gt; c)，其内部最终会调用Systemp.arraycopy。让我们再试一次：12345678910111213141516171819202122232425262728293031323334class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; // 从小到大排序 Arrays.sort(nums); // 最终结果 List&lt;List&lt;Integer&gt;&gt; result = new LinkedList&lt;&gt;(); result.add(new ArrayList&lt;&gt;()); // 回溯 dfs(0, nums, new Stack&lt;&gt;(), result); return result; &#125; public void dfs(int index, int[] nums, Stack&lt;Integer&gt; stack, List&lt;List&lt;Integer&gt;&gt; result) &#123; if (index &gt;= nums.length) &#123; return; &#125; for (int i = index; i &lt; nums.length; i++) &#123; // 在这一次总的查找中，如果当前节点和上一个节点相同，则跳过 if (i &gt; index &amp;&amp; nums[i] == nums[i - 1]) &#123; continue; &#125; // 添加该数 stack.push(nums[i]); // 作为一种情况，放进结果中 result.add(new ArrayList&lt;&gt;(stack)); // 继续回溯 dfs(i + 1, nums, stack, result); // 回退 stack.pop(); &#125; &#125;&#125; 提交之后，果然OK了，执行用时：1 ms，战胜100%的 java 提交记录。 我这里再说明一下，LinkedList 的遍历拷贝，每个元素都需要重新计算内存位置，而 ArrayList 的拷贝，可以直接一次性申请一大片空间，写入和遍历的速度会更快。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目只要利用递归就可以解决了，但优化的时候，需要注意数据结构（是不是我之前用一些的 LinkedList 换成 ArrayList 会效果更好呢）。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣89——格雷编码]]></title>
    <url>%2F2019%2F12%2F30%2F%E5%8A%9B%E6%89%A389%E2%80%94%E2%80%94%E6%A0%BC%E9%9B%B7%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，优化的时候，需要找好边界条件。 原题格雷编码是一个二进制数字系统，在该系统中，两个连续的数值仅有一个位数的差异。 给定一个代表编码总位数的非负整数 n，打印其格雷编码序列。格雷编码序列必须以 0 开头。 示例 1:123456789101112131415输入: 2输出: [0,1,3,2]解释:00 - 001 - 111 - 310 - 2对于给定的 n，其格雷编码序列并不唯一。例如，[0,2,3,1] 也是一个有效的格雷编码序列。00 - 010 - 211 - 301 - 1 示例 2:12345输入: 0输出: [0]解释: 我们定义格雷编码序列必须以 0 开头。 给定编码总位数为 n 的格雷编码序列，其长度为 2n。当 n = 0 时，长度为 20 = 1。 因此，当 n = 0 时，其格雷编码序列为 [0]。 原题url：https://leetcode-cn.com/problems/gray-code/ 解题初始想法一开始拿到这题目，我想到的是从0开始，每次从右边第一位开始改变，放入结果集中（集合），如果添加成功，就以当前数为新的起点，继续变化和添加，否则就变化下一位。 这个想法看似美好，实际有两个需要解决的难点： 如何保证转化出的二进制，精确修改每一位呢？如果用 for 循环的话，效率很低。 每次这样改变的话，很可能效率会很低，时间复杂度会很高，如何优化？ 基于以上两点，我稍微尝试了一下，感觉这个方法不靠谱，决定换一个思路。 找规律那现在我就来尝试找找规律，自己写写0位、1位、2位、3位的格雷编码：123456789101112131415161718192021220位：01位：012位：000111103位：000001011010110111101100 写完之后，应该能找到以下几个规律： 第 n 位格雷编码是第 n+1 位格雷编码的子集，意味着可以利用上一次的结果。 根据2进制来看，每多一位，其实就是在多的那一位置1即可。 基于第2点，需要做补充，格雷编码是需要考虑顺序的，并不能只是简单置1，应该是针对上一次的结果集倒序输出并在最高位置1。 上面3个规律，只有最后一个规律，可能比较难找，不过考虑到顺序这个特性，应该也能想出来。接下来我们看一下代码：1234567891011121314151617181920212223242526272829class Solution &#123; public List&lt;Integer&gt; grayCode(int n) &#123; // 动态规划，第n种情况是由：第n-1种正序各数字前加0 + 第n-1种倒序各数字前加1 // 先给出一开始的情况 LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); list.add(0); if (n == 0) &#123; return list; &#125; LinkedList&lt;Integer&gt; result = new LinkedList&lt;&gt;(); result.add(0); // 最高位置1，可以理解为增加 int add = 1; for (int i = 1; i &lt;= n; i++) &#123; // 倒序 Collections.reverse(list); for (int item : list) &#123; result.add(item + add); &#125; list = new LinkedList&lt;&gt;(result); add = add &lt;&lt; 1; &#125; return result; &#125;&#125; 提交OK，执行用时：3 ms，内存消耗：34.5 MB，但是用时只战胜9.59%的 java 提交记录，看来还是有优化必要的。 优化我第一眼想到的优化是倒序，我并不需要真的将上一次的结果集真的倒置，而是只要倒序遍历即可。因为我存放的结果集是 LinkedList，可以利用它的descendingIterator，直接进行倒序遍历，其代码是：12345678910111213141516171819202122232425262728class Solution &#123; public List&lt;Integer&gt; grayCode(int n) &#123; // 动态规划，第n种情况是由：第n-1种正序各数字前加0 + 第n-1种倒序各数字前加1 // 先给出一开始的情况 LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); list.add(0); if (n == 0) &#123; return list; &#125; LinkedList&lt;Integer&gt; result = new LinkedList&lt;&gt;(); result.add(0); int add = 1; for (int i = 1; i &lt;= n; i++) &#123; // 倒序 Iterator&lt;Integer&gt; descIterator = list.descendingIterator(); while (descIterator.hasNext()) &#123; result.add(descIterator.next() + add); &#125; list = new LinkedList&lt;&gt;(result); add = add &lt;&lt; 1; &#125; return result; &#125;&#125; 提交OK，执行用时：2 ms，内存消耗：34.3 MB，但是用时只战胜21.07%的 java 提交记录，看来还可以继续优化。 继续优化这次我盯上的是list = new LinkedList&lt;&gt;(result);，看起来是1行代码，实际内部利用了addAll方法，还是需要遍历这个结果集。而这里需要定义一个新的结果集是为了记录中间结果，是否真的有必要的呢？ 如果还是利用 LinkedList 结构，还真就需要新建结果集。因为它想获取中间结果，用get(int index)这样的方法，依旧需要遍历。这时候我就想到，可以换一种数据结构ArrayList，这样无论是倒序遍历还是添加，都是只要记录上一次的总数量，就可以达到同样的效果了。接下来让我们看看代码：1234567891011121314151617181920212223242526class Solution &#123; public List&lt;Integer&gt; grayCode(int n) &#123; // 动态规划，第n种情况是由：第n-1种正序各数字前加0 + 第n-1种倒序各数字前加1 // 先给出一开始的情况 ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(1 &lt;&lt; n); list.add(0); if (n == 0) &#123; return list; &#125; int add = 1; int tempCount; for (int i = 1; i &lt;= n; i++) &#123; tempCount = list.size(); // 倒序 for (int j = tempCount - 1; j &gt;= 0; j--) &#123; list.add(2 * tempCount - j - 1, list.get(j) + add); &#125; add = add &lt;&lt; 1; &#125; return list; &#125;&#125; 提交OK，执行用时：1 ms，内存消耗：34.2 MB，用时战胜了97.89%的 java 提交记录，这下应该可以了。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题目的难点主要在于找规律以及之后的优化。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣86——分隔链表]]></title>
    <url>%2F2019%2F12%2F29%2F%E5%8A%9B%E6%89%A386%E2%80%94%E2%80%94%E5%88%86%E9%9A%94%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[这道题主要就是处理指针问题的一般思路，利用辅助节点即可。 原题给定一个链表和一个特定值 x，对链表进行分隔，使得所有小于 x 的节点都在大于或等于 x 的节点之前。 你应当保留两个分区中每个节点的初始相对位置。 示例:12输入: head = 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;2, x = 3输出: 1-&gt;2-&gt;2-&gt;4-&gt;3-&gt;5 原题url：https://leetcode-cn.com/problems/partition-list/ 解题题目很好理解，重点在于区分大于等于和小于目标值的节点，判断其实是很简单的，主要在于如何拼接链表，以及最终如何返回。 我发现，针对链表拼接的这种题目，常常可以通过添加辅助节点（辅助头结点或者辅助尾结点）来简化拼接操作。 这道题的话，需要针对两个区间都添加辅助头结点和尾结点，然后利用一个 current 节点进行遍历，扫描到大于等于目标值的节点，添加到相应区间的尾结点，再将尾结点后移；小于目标值的节点，添加到相应区间的尾结点，再将尾结点后移。 遍历完成后，利用辅助节点将两个区间拼接，再返回。让我们看下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode partition(ListNode head, int x) &#123; if (head == null || head.next == null) &#123; return head; &#125; // 小于x的节点，开始节点和结束节点 ListNode lessStart = new ListNode(0); ListNode lessEnd = lessStart; // 大于等于x的节点，开始节点和结束节点 ListNode moreStart = new ListNode(0); ListNode moreEnd = moreStart; // 利用current节点扫描 ListNode current = head; while (current != null) &#123; // 小于x的节点 if (current.val &lt; x) &#123; // 添加到相应区间的尾结点，再将尾结点后移 lessEnd.next = current; lessEnd = current; &#125; // 大于等于x的节点 else &#123; // 添加到相应区间的尾结点，再将尾结点后移 moreEnd.next = current; moreEnd = current; &#125; current = current.next; &#125; // 将两个区间拼接 lessEnd.next = moreStart.next; // 需要让最终尾结点指向null，因为该尾结点不一定是原链表尾结点，如果指向别的节点，可能会造成循环链表 moreEnd.next = null; // 返回现在的头结点 return lessStart.next; &#125;&#125; 提交OK，执行用时：1 ms，内存消耗：35.9 MB。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。正如上面所说，针对链接这样的题目，可以借用辅助节点，简化拼接过程，方便使用。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣80——删除排序数组中的重复项 II]]></title>
    <url>%2F2019%2F12%2F28%2F%E5%8A%9B%E6%89%A380%E2%80%94%E2%80%94%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9%20II%2F</url>
    <content type="text"><![CDATA[这道题主要就是要判断好边界情况，和一般的删除重复项不同，它需要保留重复项最多两遍。 原题给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素最多出现两次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例 1:12345给定 nums = [1,1,1,2,2,3],函数应返回新长度 length = 5, 并且原数组的前五个元素被修改为 1, 1, 2, 2, 3 。你不需要考虑数组中超出新长度后面的元素。 示例 2:12345给定 nums = [0,0,1,1,1,1,2,3,3],函数应返回新长度 length = 7, 并且原数组的前五个元素被修改为 0, 0, 1, 1, 2, 3, 3 。你不需要考虑数组中超出新长度后面的元素。 说明: 为什么返回数值是整数，但输出的答案是数组呢? 请注意，输入数组是以“引用”方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。 你可以想象内部操作如下:12345678// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝int len = removeDuplicates(nums);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中该长度范围内的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; 原题url：https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array-ii/ 解题本题比较恶心的地方在于针对重复的数字，可以最多留2个，而并不是全部删除，因此在这点上需要注意。可以用一个专门的变量记录当前数字重复的次数，当重复次数大于2的时候则直接删除该数字，当不同后，再将该变量重置。 让我们看一下代码：123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public int removeDuplicates(int[] nums) &#123; if (nums.length &lt;= 1) &#123; return nums.length; &#125; int start = 1, current = 1; // 已经出现过的数字before，及其出现的次数 int before = nums[0]; int times = 1; while (current &lt; nums.length) &#123; // 相同并且超过2个，则直接跳过 if (nums[current] == before &amp;&amp; times == 2) &#123; current++; continue; &#125; // 相同但是不超过2个 // 如果和之前一个数相同，则增加times if (nums[current] == before) &#123; times++; &#125; // 如果不相同，则重置times else &#123; times = 1; &#125; // 赋值，即拷贝该数到合适的位置 nums[start] = nums[current]; before = nums[current]; // 移动指针 current++; start++; &#125; return start; &#125;&#125; 提交OK，执行用时：1 ms，内存消耗：37.3 MB。应该没什么问题了。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。 这已经是第9篇刷题的文章了，都是 medium 难度，感觉 medium 难度的话，重点关注的是一些边界判断，思考是否严谨，至于算法，还是比较基础的。不知道大家感觉如何。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[力扣79——单词搜索]]></title>
    <url>%2F2019%2F12%2F27%2F%E5%8A%9B%E6%89%A379%E2%80%94%E2%80%94%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[这道题主要就是利用递归，优化时考虑失败情况。 原题给定一个二维网格和一个单词，找出该单词是否存在于网格中。 单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。 示例:12345678910board =[ [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;], [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;], [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]]给定 word = &quot;ABCCED&quot;, 返回 true.给定 word = &quot;SEE&quot;, 返回 true.给定 word = &quot;ABCB&quot;, 返回 false. 原题url：https://leetcode-cn.com/problems/word-search/ 解题回溯拿到这题，我一开始想到的方法就是： 以每一格为起点，开始寻找，寻找的条件是要保证当前的字母和下一个和它连接的字母（上下左右）都符合条件，那么就继续查找。 只要当前不符合，立刻返回 false，快速失败。 利用一个二维 boolean 数组记录每一格的使用情况，记住，如果从当前格出发都不成功的话，则需要回退。 接下来看看代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class Solution &#123; // 总行数 int row; // 总列数 int col; // 原数组 char[][] board; // 需要寻找的字符数组 char[] wordArray; public boolean exist(char[][] board, String word) &#123; this.board = board; this.row = board.length; this.col = board[0].length; this.wordArray = word.toCharArray(); // 标记每一格是否用过的二维数组 boolean[][] used = new boolean[row][col]; // 以每一格为起点开始搜索 for (int i = 0; i &lt; row; i++) &#123; for (int j = 0; j &lt; col; j++) &#123; if (dfs(i, j, 0, used)) &#123; return true; &#125; &#125; &#125; return false; &#125; public boolean dfs(int x, int y, int index, boolean[][] used) &#123; // 当前位置是否符合条件 if (board[x][y] != wordArray[index]) &#123; return false; &#125; // 全部找完了 if (index == wordArray.length - 1) &#123; return true; &#125; // 设置当前格使用过了 used[x][y] = true; // 寻找上下左右是否有符合下一个的情况 // 上一格是否存在并且没有被使用过 if (x &gt; 0 &amp;&amp; !used[x - 1][y]) &#123; if (dfs(x - 1, y, index + 1, used)) &#123; return true; &#125; &#125; // 下一格是否存在并且没有被使用过 if (x &lt; row - 1 &amp;&amp; !used[x + 1][y]) &#123; if (dfs(x + 1, y, index + 1, used)) &#123; return true; &#125; &#125; // 左一格是否存在并且没有被使用过 if (y &gt; 0 &amp;&amp; !used[x][y - 1]) &#123; if (dfs(x, y - 1, index + 1, used)) &#123; return true; &#125; &#125; // 右一格是否存在并且没有被使用过 if (y &lt; col - 1 &amp;&amp; !used[x][y + 1]) &#123; if (dfs(x, y + 1, index + 1, used)) &#123; return true; &#125; &#125; // 上下左右的情况都走完了，因此回退，设置当前格没有使用过 used[x][y] = false; return false; &#125;&#125; 提交OK，执行用时：19 ms，内存消耗：38.3 MB。从时间上看起来还有不少优化的空间，那该怎么做呢？ 似乎无用的优化我看了别人更优的解法，发现思想都是一致的，只是在判断上可能会更加简洁一些，如果是判断快速失败的话，似乎没有什么本质上的区别。我将自己的写法稍微优化了一下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solution &#123; // 总行数 int row; // 总列数 int col; // 原数组 char[][] board; // 需要寻找的字符数组 char[] wordArray; public boolean exist(char[][] board, String word) &#123; this.board = board; this.row = board.length; this.col = board[0].length; this.wordArray = word.toCharArray(); // 标记每一格是否用过的二维数组 boolean[][] used = new boolean[row][col]; // 以每一格为起点开始搜索 for (int i = 0; i &lt; row; i++) &#123; for (int j = 0; j &lt; col; j++) &#123; if (dfs(i, j, 0, used)) &#123; return true; &#125; &#125; &#125; return false; &#125; public boolean dfs(int x, int y, int index, boolean[][] used) &#123; // 当前位置不存在或者使用过，则返回失败 if (x &lt; 0 || x &gt;= row || y &lt; 0 || y &gt;= col || used[x][y]) &#123; return false; &#125; // 当前位置是否符合条件 if (board[x][y] != wordArray[index]) &#123; return false; &#125; // 全部找完了 if (index == wordArray.length - 1) &#123; return true; &#125; // 设置当前格使用过了 used[x][y] = true; // 寻找上下左右是否有符合下一个的情况 boolean flag = dfs(x - 1, y, index + 1, used) || dfs(x + 1, y, index + 1, used) || dfs(x, y - 1, index + 1, used) || dfs(x, y + 1, index + 1, used); // 上下左右的情况都走完了，因此回退，设置当前格没有使用过 used[x][y] = false; return flag; &#125;&#125; 提交OK，执行用时：5 ms，内存消耗：38.4 MB。用时上少了很多，应该在于判断上： 针对位置是否存在的判断，之前的写法是判断下一个位置是否存在，分散在四个 if 判断中，现在是写在一个里面，用于判断当前位置。 寻找上下左右时，因为逻辑运算||是支持短路的，所以和之前分在四个 if 中效果是差不多的，但看起来更加简洁。 好吧，其实我自己也没有看懂为什么这样写时间上会减少，大家如果知道的话，欢迎在下方留言。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要就是回溯，针对边界情况需要注意，应该就没有其他问题了。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣77——组合]]></title>
    <url>%2F2019%2F12%2F26%2F%E5%8A%9B%E6%89%A377%E2%80%94%E2%80%94%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[这道题主要利用栈就可以解决，优化时需要考虑那些失败情况。 原题给定两个整数 n 和 k，返回 1 … n 中所有可能的 k 个数的组合。 示例:12345678910输入: n = 4, k = 2输出:[ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4],] 原题url：https://leetcode-cn.com/problems/combinations/ 解题递归获取一开始的想法就是遍历递归获取，利用一个 stack 存储中间结果，不断进行出栈入栈，这样肯定就能拿全。 让我们来看看代码：12345678910111213141516171819202122232425262728293031class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) &#123; if (k == 0) &#123; return new LinkedList&lt;&gt;(); &#125; if (n == 0) &#123; return new LinkedList&lt;&gt;(); &#125; List&lt;List&lt;Integer&gt;&gt; result = new LinkedList&lt;&gt;(); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); dfs(n, k, 1, stack, result); return result; &#125; public void dfs(int n, int remain, int index, Stack&lt;Integer&gt; stack, List&lt;List&lt;Integer&gt;&gt; result) &#123; for (int i = index; i &lt;= n; i++) &#123; // 加入stack中 stack.push(i); // 是否加到k个数 if (remain - 1 == 0) &#123; result.add(new LinkedList&lt;&gt;(stack)); &#125; else &#123; dfs(n, remain - 1, i + 1, stack, result); &#125; // 将数从stack中拿出 stack.pop(); &#125; &#125;&#125; 提交OK，执行用时：73 ms，内存消耗：44 MB。是否还可以优化呢？ 剪枝今天看到了一个词剪枝，其实这个词是回溯剪枝，回溯大家都懂，剪枝其实就是一种优化，减少回溯中不需要的情况。 从上面的代码可以看出，在回溯中的遍历，并不需要一直遍历到 n。比如：从 7 个数中取 4 个数，开始的时候遍历到 4 就足够了，因为从 5 开始凑不齐 4 个数，之后的遍历也是同样如此。 明知失败的事不需要一直进行到最后，和快速失败有些类似，接下来看看优化后的代码：12345678910111213141516171819202122232425262728293031class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) &#123; if (k == 0) &#123; return new LinkedList&lt;&gt;(); &#125; if (n == 0) &#123; return new LinkedList&lt;&gt;(); &#125; List&lt;List&lt;Integer&gt;&gt; result = new LinkedList&lt;&gt;(); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); dfs(n, k, 1, stack, result); return result; &#125; public void dfs(int n, int remain, int index, Stack&lt;Integer&gt; stack, List&lt;List&lt;Integer&gt;&gt; result) &#123; // 当剩余没有遍历的数，比还需要遍遍历的数少时，也可以不用继续了。 for (int i = index; i &lt;= n &amp;&amp; (n - i + 1 &gt;= remain); i++) &#123; // 加入stack中 stack.push(i); // 是否加到k个数 if (remain - 1 == 0) &#123; result.add(new LinkedList&lt;&gt;(stack)); &#125; else &#123; dfs(n, remain - 1, i + 1, stack, result); &#125; stack.pop(); &#125; &#125;&#125; 代码更改极少，我们看看结果：执行用时：5 ms，内存消耗：40.7 MB。 看似很小的变化，但效果却很好，看来这些细节确实是需要注意的。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要教会了需要剪枝，找到边界情况，边界找的更细，那么需要执行的时间也可能会越少。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣75——颜色分类]]></title>
    <url>%2F2019%2F12%2F25%2F%E5%8A%9B%E6%89%A375%E2%80%94%E2%80%94%E9%A2%9C%E8%89%B2%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[这道题主要在于空间复杂度的优化，利用指针，可以完成这一点。 原题给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。 此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。 注意: 不能使用代码库中的排序函数来解决这道题。 示例:12输入: [2,0,2,1,1,0]输出: [0,0,1,1,2,2] 进阶：123* 一个直观的解决方案是使用计数排序的两趟扫描算法。首先，迭代计算出0、1 和 2 元素的个数，然后按照0、1、2的排序，重写当前数组。* 你能想出一个仅使用常数空间的一趟扫描算法吗？ 原题url：https://leetcode-cn.com/problems/sort-colors/ 解题两趟扫描我当时想到的第一种想法就是排序，后来感觉没有必要，因为只有3种元素，我完全就可以按照进阶提示中的第一条，先扫描统计出各元素个数，然后第二遍扫描时，直接进行赋值。 让我们直接来看代码：123456789101112131415161718192021222324252627282930class Solution &#123; public void sortColors(int[] nums) &#123; // 记录0,1,2的个数 int num0 = 0, num1 = 0, num2 = 0; // 计算各个数字出现的次数 for (int i : nums) &#123; switch(i) &#123; case 0: num0++; break; case 1: num1++; break; case 2: num2++; break; &#125; &#125; // 重新赋值 for (int i = 0; i &lt; nums.length; i++) &#123; if (i &lt; num0) &#123; nums[i] = 0; &#125; else if (i &lt; num0 + num1) &#123; nums[i] = 1; &#125; else &#123; nums[i] = 2; &#125; &#125; &#125;&#125; 提交OK，执行用时：0 ms，内存消耗：35.2 MB。是否还可以优化呢？ 优化参考进阶提示中的第二条，上面的方法使用了常数空间，但一遍扫描该如何做到呢？ 我其实并没有想出来，参考了别人的解法：利用三个指针进行一次遍历并交换。 具体来说，就是增加一个当前指针current（从下标0开始）、一个指向数字0区间的末尾指针p0（从下标0开始）、一个指向数字2区间的开始指针p2（从下标 nums.length - 1）。下标0 到下标 p0 之间存放数字0，下标 p0 到 p2 之间存放数字1，下标 p2 到 下标 (nums.length - 1) 之间存放数字2。current 指针从下标0开始遍历，如果值为0，则和 p0 交换，如果值为2，则和 p2 交换。 让我们来看看代码：123456789101112131415161718192021222324252627282930313233class Solution &#123; public void sortColors(int[] nums) &#123; // 利用3个指针current、p0、p2 int current = 0, p0 = 0, p2 = nums.length - 1; while (current &lt;= p2) &#123; // 如果当前值为1，current指针往后移动 if (nums[current] == 1) &#123; current++; continue; &#125; // 如果当前值为0，则和 p0 交换，p0指针往后移动 if (nums[current] == 0) &#123; nums[current] = nums[p0]; nums[p0] = 0; // 因为p0一开始和current相同 if (p0 == current) &#123; current++; &#125; p0++; continue; &#125; // 如果当前值为2，则和 p2 交换，p2指针往前移动 if (nums[current] == 2) &#123; nums[current] = nums[p2]; nums[p2] = 2; p2--; continue; &#125; &#125; &#125;&#125; 提交OK，执行用时：0 ms，内存消耗：35 MB。这结果，感觉好像没有多少优化，但对于我们而言，最重要的是增加了一种解题思路。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题主要在于利用指针一遍扫描得出结果，优化解题。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣74——搜索二维矩阵]]></title>
    <url>%2F2019%2F12%2F24%2F%E5%8A%9B%E6%89%A374%E2%80%94%E2%80%94%E6%90%9C%E7%B4%A2%E4%BA%8C%E7%BB%B4%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[这道题主要就是利用二分查找，将矩阵做变换进行优化。 原题编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性： 每行中的整数从左到右按升序排列。 每行的第一个整数大于前一行的最后一个整数。 示例 1:12345678输入:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 3输出: true 示例 2:12345678输入:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 13输出: false 原题url：https://leetcode-cn.com/problems/search-a-2d-matrix/ 解题二分查找既然是已经排好序的，那么我第一想到的就是二分查找了。每次进行对半查找，时间复杂度O(log(mn))，应该还是很高效。 我准备将查找分成两部分，第一是二分查找找到 target 在哪一行，然后从那一行再利用二分查找找到 target 在这一行的哪个位置。 让我们来看看代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Solution &#123; // 总行数 int row; // 总列数 int col; public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix.length == 0 || matrix[0].length == 0) &#123; return false; &#125; row = matrix.length; col = matrix[0].length; // 利用二分法查询 return binarySearchMatrix(matrix, 0, matrix.length - 1, target); &#125; /** * 查找这是矩阵的哪一行 */ public boolean binarySearchMatrix(int[][] matrix, int left, int right, int target) &#123; if (left &gt; right) &#123; return false; &#125; if (left == right) &#123; int[] array = matrix[left]; if (target &lt; array[0] || target &gt; array[col - 1]) &#123; return false; &#125; // 从数据中心查找 return binarySearchArray(array, 0, col - 1, target); &#125; int middle = (left + right) / 2; int[] middleArray = matrix[middle]; if (middleArray[0] &gt; target) &#123; return binarySearchMatrix(matrix, left, middle - 1, target); &#125; else if (middleArray[col - 1] &lt; target) &#123; return binarySearchMatrix(matrix, middle + 1, right, target); &#125; else &#123; return binarySearchArray(middleArray, 0, col - 1, target); &#125; &#125; /** * 查找这是数组中的哪个位置 */ public boolean binarySearchArray(int[] array, int left, int right, int target) &#123; if (left &gt; right) &#123; return false; &#125; if (left == right) &#123; return array[left] == target; &#125; int middle = (left + right) / 2; if (array[middle] &gt; target) &#123; return binarySearchArray(array, left, middle - 1, target); &#125; else if (array[middle] == target) &#123; return true; &#125; else &#123; return binarySearchArray(array, middle + 1, right, target); &#125; &#125;&#125; 提交OK，其执行用时：1 ms，内存消耗：38.3 MB。 优化虽然解题成功，且效果不错，但总感觉这样写的太过。因为我将二维数组的查找分为两种情况，如果以后变成三维、四维数组，岂不是代码更长？其实这种查找都是利用的同一种思想——二分查找，那我们是否可以将其进行合并呢？ 可以的，我们可以将二维数组拉长。拉成1层后，就可以直接用一维数组中的二分查找了。 让我们看看代码：1234567891011121314151617181920212223242526272829class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix.length == 0 || matrix[0].length == 0) &#123; return false; &#125; int col = matrix[0].length; // 将二维数组拉成一维数组，利用二分法解决 int left = 0; int right = matrix.length * col - 1; while (left &lt;= right) &#123; // 计算中间数的下标和值 int middleIndex = (left + right) / 2; int middleVal = matrix[middleIndex / col][middleIndex % col]; if (middleVal == target) &#123; return true; &#125; if (middleVal &lt; target) &#123; left = middleIndex + 1; &#125; else &#123; right = middleIndex - 1; &#125; &#125; return false; &#125;&#125; 提交OK，其执行用时：0 ms，内存消耗：40.9 MB。 大家知道这多余的内存消耗在哪儿呢？按道理说，跟前面的方法相比，这里不涉及递归，理论上其调用栈更少，内存消耗应该更少才对。如果大家知道的话，可以在下方评论，我将感激不尽。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题应该主要就是涉及到二分查找，然后加一些优化，应该就没有问题了。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣73——矩阵置零]]></title>
    <url>%2F2019%2F12%2F23%2F%E5%8A%9B%E6%89%A360%E2%80%94%E2%80%94%E7%AC%ACk%E4%B8%AA%E6%8E%92%E5%88%97%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，考虑好边界情况。 原题给出集合 [1,2,3,…,n]，其所有元素共有 n! 种排列。 按大小顺序列出所有排列情况，并一一标记，当 n = 3 时, 所有排列如下：1234561. &quot;123&quot;2. &quot;132&quot;3. &quot;213&quot;4. &quot;231&quot;5. &quot;312&quot;6. &quot;321&quot; 给定 n 和 k，返回第 k 个排列。 说明： 给定 n 的范围是 [1, 9]。 给定 k 的范围是[1, n!]。 示例 1:12输入: n = 3, k = 3输出: &quot;213&quot; 示例 2:12输入: n = 4, k = 9输出: &quot;2314&quot; 解法按照题目所描述的，其实就是按照排列规律，找出相应的数字。 每一位上可以存在的可能数字范围逐渐减少，因此我们需要记录一下当前用过哪些数字。 每一位上前缀数字最终对应的可能性也是一个全排列，比如 n 为4时，当第1位定下来一个数字，其对应的所有数字组合有 3!，当第2位定下来后，其对应的数字组合就是2!。当你确认的数字越多，其组合也越少。 直接上代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; // 当前数字是否用过，默认为false，代表没有用过 boolean[] used; public String getPermutation(int n, int k) &#123; used = new boolean[n]; int all = 1; for (int i = n - 1; i &gt; 1; i--) &#123; all *= i; &#125; StringBuilder sb = dfs(n, all, k); return sb.toString(); &#125; /** * n:当前还剩几个数字没有添加 * all:为了计算出当前数字属于第几组，例如n等于5时，all是4!，这样k/n就知道是第几组了 * k:所求结果是当前组的第几个 */ public StringBuilder dfs(int n, int all, int k) &#123; // 组内偏移量 int offset = k % all; // 当前是第几组 int groupIndex = k / all + (offset == 0 ? 0 : 1); // 在当前没有被访问过的数字里，找第groupIndex个数字 int i = 0; for (; i &lt; used.length &amp;&amp; groupIndex &gt; 0; i++) &#123; if (!used[i]) &#123; groupIndex--; &#125; &#125; // 用当前数字 StringBuilder result = new StringBuilder().append(i); // 标记当前数字已经用过 used[i - 1] = true; // 说明是最后一个数字 if (n == 1) &#123; return result; &#125; // 确认一位数字后，其对应的可能性就在减少 return result.append(dfs(n - 1, all / (n - 1), (offset == 0 ? all : offset))); &#125;&#125; 提交OK，执行用时：2ms，内存消耗：34.4MB。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。这道题应该主要就是找规律了，确认好边界情况就应该没什么问题。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣64——最小路径和]]></title>
    <url>%2F2019%2F12%2F22%2F%E5%8A%9B%E6%89%A364%E2%80%94%E2%80%94%E6%9C%80%E5%B0%8F%E8%B7%AF%E5%BE%84%E5%92%8C%2F</url>
    <content type="text"><![CDATA[这道题主要就是找规律，逆向思考，进行优化。 原题给定一个包含非负整数的 m x n 网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 说明：每次只能向下或者向右移动一步。 示例:12345678输入:[ [1,3,1], [1,5,1], [4,2,1]]输出: 7解释: 因为路径 1→3→1→1→1 的总和最小。 解法错误的正向思路我一开始的想法是正向思路，从起点开始，每个点都有两种后续走法——向下或者向右，当然其中需要判断是否可以向下或者向右以及到达终点就停止。我想到的优化是当走到终点后，将当前走过的路径和记录下来，找出最小值，别的路径上在走的时候，如果比当前最小和大，就没必要继续了。 来看看我的代码：12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; private long min = Long.MAX_VALUE; private int[][] map; public int minPathSum(int[][] grid) &#123; map = grid; dfs(0, 0, 0); return (int)min; &#125; public void dfs(int x, int y, long sum) &#123; sum += Long.valueOf(map[x][y]); // 如果已经大于等于当前的最小值，那么就没有必要继续走了 if (sum &gt;= min) &#123; return; &#125; // 是否到达终点 if (x == map.length - 1 &amp;&amp; y == map[x].length - 1) &#123; if (sum &lt; min) &#123; min = sum; &#125; return; &#125; // 是否可以向右 if (y &lt; map[x].length - 1) &#123; dfs(x, y + 1, sum); &#125; // 是否可以向下 if (x &lt; map.length - 1) &#123; dfs(x + 1, y, sum); &#125; &#125;&#125; 感觉很理想，然而现实是超时了，确实效率不高，除了第一列和第一行的点，其他点都有可能存在重复计算的可能。 逆向思路既然正向不行，那咋们就逆向，从终点出发，以终点为起点，计算当前点到终点的最小值，最后推算出到达起点的最小值（这也是我看了别人的解法才知道的，看来自己的思路果然有问题）。这样就能保证每个点只计算一次，时间效率就是O(m * n)，看起来就高效多了。 接下来看看代码：123456789101112131415161718192021222324252627282930313233class Solution &#123; public int minPathSum(int[][] grid) &#123; // 从终点开始找起，算当前节点到终点的最小值 int right, down; for (int i = grid.length - 1; i &gt;= 0; i--) &#123; for (int j = grid[i].length - 1; j &gt;= 0; j--) &#123; // 如果是终点，则保持不变 if (i == grid.length - 1 &amp;&amp; j == grid[i].length - 1) &#123; continue; &#125; // 如果没有右节点 if (j == grid[i].length - 1) &#123; // 那么就设置当前节点的值加上下节点的值 grid[i][j] += grid[i + 1][j]; continue; &#125; // 如果没有下节点 if (i == grid.length - 1) &#123; // 那么就设置当前节点的值加上右节点的值 grid[i][j] += grid[i][j + 1]; continue; &#125; // 如果下节点和右节点都有的话，则加上其中较小的那个 grid[i][j] += grid[i + 1][j] &lt; grid[i][j + 1] ? grid[i + 1][j] : grid[i][j + 1]; &#125; &#125; return grid[0][0]; &#125;&#125; OK，通过了，执行用时：3ms，内存消耗：39.5MB。 核心思想就是： 从终点出发，每个点到终点的最小值 = 每个点当前的值 + Min(该点下一个点值, 该点右一个点)。 你想想，是否是如此呢？ 既然知道了反向思路，我们可以优化一下我们之前的正向思路解法。 优化正向思路之前的超时，是因为每个点可能会被计算多次，那么我们如果计算出，从起点出发，到每个节点的最小值，最终计算到终点，也应该是终点的最小值，你想想是不是这样呢？ 来看看代码12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public int minPathSum(int[][] grid) &#123; // 从起点开始找起，算当前节点到起点的最小值 // 总行数 int row = grid.length; // 总列数 int col = grid[0].length; // 左节点和上节点计算出的最小值 int left, up; // 遍历并计算 for (int i = 0; i &lt; row; i++) &#123; for (int j = 0; j &lt; col; j++) &#123; // 起点不计算 if (i == 0 &amp;&amp; j == 0) &#123; continue; &#125; // 如果没有左节点 if (j == 0) &#123; // 就设置当前节点的值加上节点 grid[i][j] += grid[i - 1][j]; continue; &#125; // 如果没有上节点 if (i == 0) &#123; // 就设置当前节点的值加上左节点 grid[i][j] += grid[i][j - 1]; continue; &#125; // 如果左节点和上节点都存在，就加上其中最小的值 grid[i][j] += (grid[i][j - 1] &lt; grid[i - 1][j] ? grid[i][j - 1] : grid[i - 1][j]); &#125; &#125; return grid[row - 1][col - 1]; &#125;&#125; OK，也通过了，执行用时：3ms，内存消耗：42.4MB。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。算法本来就是就是在做优化，如何能比之前更好更合适，就是优化了。希望能与大家共同进步。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>找规律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣71——简化路径]]></title>
    <url>%2F2019%2F12%2F21%2F%E5%8A%9B%E6%89%A371%E2%80%94%E2%80%94%E7%AE%80%E5%8C%96%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[这应该是一个比较基础的题目，利用基本数据结构——栈，应该就够了。 原题以 Unix 风格给出一个文件的绝对路径，你需要简化它。或者换句话说，将其转换为规范路径。 在 Unix 风格的文件系统中，一个点（.）表示当前目录本身；此外，两个点 （..） 表示将目录切换到上一级（指向父目录）；两者都可以是复杂相对路径的组成部分。更多信息请参阅：Linux / Unix中的绝对路径 vs 相对路径 请注意，返回的规范路径必须始终以斜杠 / 开头，并且两个目录名之间必须只有一个斜杠 /。最后一个目录名（如果存在）不能以 / 结尾。此外，规范路径必须是表示绝对路径的最短字符串。 示例 1：123输入：&quot;/home/&quot;输出：&quot;/home&quot;解释：注意，最后一个目录名后面没有斜杠。 示例 2：123输入：&quot;/../&quot;输出：&quot;/&quot;解释：从根目录向上一级是不可行的，因为根是你可以到达的最高级。 示例 3：123输入：&quot;/home//foo/&quot;输出：&quot;/home/foo&quot;解释：在规范路径中，多个连续斜杠需要用一个斜杠替换。 示例 4：12输入：&quot;/a/./b/../../c/&quot;输出：&quot;/c&quot; 示例 5：12输入：&quot;/a/../../b/../c//.//&quot;输出：&quot;/c&quot; 示例 6：12输入：&quot;/a//b////c/d//././/..&quot;输出：&quot;/a/b/c&quot; 原题url:https://leetcode-cn.com/problems/simplify-path/ 解法看起也不难，但一开始我拿到的时候也是无从下手。 利用栈看到..的逻辑就特别容易让人想到后退，而记录后退最方便的数据结构应该就是栈了。至于.就可以想象成可以忽略的内容，而/则可以作为分隔符了，来看看代码：12345678910111213141516171819202122232425262728293031323334class Solution &#123; public String simplifyPath(String path) &#123; // 存储路径 Stack&lt;String&gt; stack = new Stack&lt;&gt;(); // 分隔 String[] array = path.split("/"); for (String str : array) &#123; // 忽略 if (str.equals("") || str.equals(".")) &#123; continue; &#125; // 后退 if (str.equals("..")) &#123; stack.pop(); continue; &#125; // 需要存储的内容 stack.push(str); &#125; StringBuilder sb = new StringBuilder(); for (String str : stack) &#123; sb.append("/").append(str); &#125; // 如果内容为空，则需要输出"/" if (sb.length() == 0) &#123; sb.append("/"); &#125; return sb.toString(); &#125;&#125; 看起来很美好，提交之后报错了。说是stack.push(str);这行抛出了异常java.util.EmptyStackException，确实，如果栈为空，依旧还是需要在最顶层的（看来还是没有把问题想全面）。让我们来优化一下代码：123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public String simplifyPath(String path) &#123; // 存储路径 Stack&lt;String&gt; stack = new Stack&lt;&gt;(); // 分隔 String[] array = path.split("/"); for (String str : array) &#123; // 忽略 if (str.equals("") || str.equals(".")) &#123; continue; &#125; // 后退 if (str.equals("..")) &#123; // 判断是否为空，不为空，才需要回退 if (!stack.empty()) &#123; stack.pop(); &#125; // 无论stack空不空，都需要结束 continue; &#125; stack.push(str); &#125; StringBuilder sb = new StringBuilder(); for (String str : stack) &#123; sb.append("/").append(str); &#125; if (sb.length() == 0) &#123; sb.append("/"); &#125; return sb.toString(); &#125;&#125; OK，通过了，执行用时：6ms，内存消耗：36.2MB。 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。因为之前已经刷了一些题目了，所以会把这些解题过程都补上去，也当做复习了。我刷题都是选 medium 的，所以不会很难，主要我看外企的面试大多也是以这样的题目为主，刷 hard 的题目确实感觉太费脑子，打击积极性。希望能与大家共同进步。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
      <tags>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣73——矩阵置零]]></title>
    <url>%2F2019%2F12%2F20%2F%E5%8A%9B%E6%89%A373%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E7%BD%AE%E9%9B%B6%2F</url>
    <content type="text"><![CDATA[准备开一个力扣解题的系列，督促自己每天刷题，就从今天开始。 原题给定一个 m x n 的矩阵，如果一个元素为 0，则将其所在行和列的所有元素都设为 0。请使用原地算法。 示例 1:123456789101112输入: [ [1,1,1], [1,0,1], [1,1,1]]输出: [ [1,0,1], [0,0,0], [1,0,1]] 示例 2:123456789101112输入: [ [0,1,2,0], [3,4,5,2], [1,3,1,5]]输出: [ [0,0,0,0], [0,4,5,0], [0,3,1,0]] 进阶: 一个直接的解决方案是使用 O(mn) 的额外空间，但这并不是一个好的解决方案。 一个简单的改进方案是使用 O(m + n) 的额外空间，但这仍然不是最好的解决方案。 你能想出一个常数空间的解决方案吗？ 原题url:https://leetcode-cn.com/problems/set-matrix-zeroes/ 解法其实题目本身不难，只要判断出哪些数字是0，将其所在行和列记录一下， 最终全部置0即可，关键在于你所需要消耗的空间是多少。 用一个数字首先我想到的是用一个数字进行表示，用二进制表示，一共m + n位，其中前m位表示行，后n位表示列，矩阵中哪个数字为0，则其行列所在位的数字为1，也就是加上相应的二进制数。为了不重复添加，可以用&amp;进行判断。来看看代码是什么：12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public void setZeroes(int[][] matrix) &#123; // 转化为二进制后，前m位表示列，后n位表示行 int temp = 0; for (int i = 0; i &lt; matrix.length; i++) &#123; for (int j = 0; j &lt; matrix[i].length; j++) &#123; if (matrix[i][j] != 0) &#123; continue; &#125; // 第j列是否已经被设置为0 int num = 1 &lt;&lt; (matrix.length + j); if ((temp &amp; num) != num) &#123; // 如果没有，则加上 temp += num; &#125; // 第i行是否已经被设置为0 num = 1 &lt;&lt; i; if ((temp &amp; num) != num) &#123; // 如果没有，则加上 temp += num; &#125; &#125; &#125; for (int i = 0; i &lt; matrix.length; i++) &#123; for (int j = 0; j &lt; matrix[i].length; j++) &#123; // 第j列是否已经被设置为0 int numCol = 1 &lt;&lt; (matrix.length + j); // 第i行是否已经被设置为0 int numRow = 1 &lt;&lt; i; if ((temp &amp; numRow) == numRow || (temp &amp; numCol) == numCol) &#123; // 如果有，则设置当前值为0 matrix[i][j] = 0; &#125; &#125; &#125; &#125;&#125; 理论上没什么问题，提交之后报错。当m和n很大时，数字会很大，这个时候temp会越界。我想着是不是求2的幂用Math.pow()，并且 temp 的类型改为 long ，是不是就可以了，说干就干：12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public void setZeroes(int[][] matrix) &#123; // 转化为二进制后，前m位表示列，后n位表示行 long temp = 0; for (int i = 0; i &lt; matrix.length; i++) &#123; for (int j = 0; j &lt; matrix[i].length; j++) &#123; if (matrix[i][j] != 0) &#123; continue; &#125; // 第j列是否已经被设置为0 long num = (long)Math.pow(2, matrix.length + j); if ((temp &amp; num) != num) &#123; // 如果没有，则加上 temp += num; &#125; // 第i行是否已经被设置为0 num = (long)Math.pow(2, i); if ((temp &amp; num) != num) &#123; // 如果没有，则加上 temp += num; &#125; &#125; &#125; for (int i = 0; i &lt; matrix.length; i++) &#123; for (int j = 0; j &lt; matrix[i].length; j++) &#123; // 第j列是否已经被设置为0 long numCol = (long)Math.pow(2, matrix.length + j); // 第i行是否已经被设置为0 long numRow = (long)Math.pow(2, i); if ((temp &amp; numRow) == numRow || (temp &amp; numCol) == numCol) &#123; // 如果有，则设置当前值为0 matrix[i][j] = 0; &#125; &#125; &#125; &#125;&#125; 好吧，依然不可以，看来确实很大，最终还是溢出变成负数了。看来得另寻他法了。 利用矩阵本身如果1个数字不够，那么多来几个数字应该也是不够用的，而且如果用的太多也可能会增长到m + n，空间依旧比较多。这个时候我也想不出来，看了看别人的解法，让我顿时领悟——利用矩阵本身。 就是利用矩阵的第一行和第一列来记录需要置零的行和列，至于第一行和第一列是否需要置零，则可以单独拿两个 boolean 对象来表示。（怎么好的思路，为啥我就是没想到呢）来看看代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution &#123; public void setZeroes(int[][] matrix) &#123; // 用第一行和第一列表示当前行和当前列是否需要置0 // 单独计算第一行和第一列是否需要置0 int row = matrix.length; int col = matrix[0].length; // 第一行是否需要置0 boolean row0 = false; for (int i = 0; i &lt; col; i++) &#123; if (matrix[0][i] == 0) &#123; row0 = true; break; &#125; &#125; // 第一列是否需要置0 boolean col0 = false; for (int i = 0; i &lt; row; i++) &#123; if (matrix[i][0] == 0) &#123; col0 = true; break; &#125; &#125; // 判断每一行每一列是否需要置0 for (int i = 1; i &lt; row; i++) &#123; for (int j = 1; j &lt; col; j++) &#123; if (matrix[i][j] != 0) &#123; continue; &#125; matrix[i][0] = matrix[0][j] = 0; &#125; &#125; // 置0 for (int i = 1; i &lt; row; i++) &#123; for (int j = 1; j &lt; col; j++) &#123; if (matrix[i][0] == 0 || matrix[0][j] == 0) &#123; matrix[i][j] = 0; &#125; &#125; &#125; // 第一行是否需要都置0 if (row0) &#123; for (int i = 0; i &lt; col; i++) &#123; matrix[0][i] = 0; &#125; &#125; // 第一列是否需要都置0 if (col0) &#123; for (int i = 0; i &lt; row; i++) &#123; matrix[i][0] = 0; &#125; &#125; &#125;&#125; 终于通过了，执行用时：2ms，内存消耗：43.5MB。那么是否可以继续优化呢？ 利用矩阵本身 优化首先，需要第一行和第一列都判断一遍的吗？可以只判断其中一个即可，比如只判断第一列是否需要置零，那么第一行是否需要置零就可以依赖matrix[0][0]了。在置零的时候，也是将第一列单独判断即可。 需要注意的是，置零操作需要从后往前，因为matrix[0][0]会有双重含义，所以最后判断即可。来看看代码： 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public void setZeroes(int[][] matrix) &#123; // 第一列是否需要置零 boolean col0 = false; int row = matrix.length; int col = matrix[0].length; // 判断是否需要置零 for (int i = 0; i &lt; row; i++) &#123; // 如果第一列不需要置零，并且第一列有数字是0，则col0设置为true if (!col0 &amp;&amp; matrix[i][0] == 0) &#123; col0 = true; &#125; for (int j = 1; j &lt; col; j++) &#123; if (matrix[i][j] == 0) &#123; matrix[i][0] = matrix[0][j] = 0; &#125; &#125; &#125; // 置零，从后往前开始，因为如果从前往后，第一行如果因为第一列置为0，会对之后结果误导 for (int i = row - 1; i &gt;= 0; i--) &#123; // 第一列不动 for (int j = col - 1; j &gt;= 1; j--) &#123; if (matrix[i][0] == 0 || matrix[0][j] == 0) &#123; matrix[i][j] = 0; &#125; &#125; // 第一列置零 if (col0) &#123; matrix[i][0] = 0; &#125; &#125; &#125;&#125; 总结以上就是这道题目我的解答过程了，不知道大家是否理解了。我准备把我刷力扣的过程记录下来，作为这个系列的内容，希望能和大家多多分享。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/ 公众号：健程之道]]></content>
      <categories>
        <category>力扣</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal的进化——TransmittableThreadLocal]]></title>
    <url>%2F2019%2F12%2F15%2FThreadLocal%E7%9A%84%E8%BF%9B%E5%8C%96%E2%80%94%E2%80%94TransmittableThreadLocal%2F</url>
    <content type="text"><![CDATA[上一篇文章中，我们谈到了 InheritableThreadLocal，它解决了 ThreadLocal 针对父子线程无法共享上下文的问题。但我们可能听说过阿里的开源产品TransmittableThreadLocal，那么它又是做什么的呢？ 线程池中的共享我们在多线程中，很少会直接 new 一个线程，更多的可能是利用线程池处理任务，那么利用 InheritableThreadLocal 可以将生成任务线程的上下文传递给执行任务的线程吗？废话不多说，直接上代码测试一下：123456789101112131415161718192021222324252627282930313233343536373839public class InheritableThreadLocalContext &#123; private static InheritableThreadLocal&lt;Context&gt; context = new InheritableThreadLocal&lt;&gt;(); static class Context &#123; String name; int value; &#125; public static void main(String[] args) &#123; // 固定线程池 ExecutorService executorService = Executors.newFixedThreadPool(4); for (int i = 1; i &lt;= 10; i++) &#123; int finalI = i; new Thread( () -&gt; &#123; // 生成任务的线程对context进行赋值 Context contextMain = new Context(); contextMain.name = String.format("Thread%s name", finalI); contextMain.value = finalI * 20; InheritableThreadLocalContext.context.set(contextMain); // 提交任务 for (int j = 1; j &lt;= 10; j++) &#123; System.out.println("Thread" + finalI + " produce task " + (finalI * 20 + j)); executorService.execute(() -&gt; &#123; // 执行任务的子线程 Context contextChild = InheritableThreadLocalContext.context.get(); System.out.println(Thread.currentThread().getName() + " execute task, name : " + contextChild.name + " value : " + contextChild.value); &#125;); &#125; &#125; ).start(); &#125; &#125;&#125; 我们希望的结果是，子线程输出的内容能够和父线程对应上。然而，实际的结果却出乎所料，我将结果整理一下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Thread1 produce task 21// 省略8行Thread1 produce task 30Thread2 produce task 41// 省略8行Thread2 produce task 50pool-1-thread-1 execute task, name : Thread2 name value : 40// 省略47行pool-1-thread-1 execute task, name : Thread2 name value : 40Thread3 produce task 61// 省略8行Thread3 produce task 70Thread4 produce task 81// 省略8行Thread4 produce task 90Thread5 produce task 101// 省略8行Thread5 produce task 110Thread6 produce task 121// 省略8行Thread6 produce task 130Thread7 produce task 141// 省略8行Thread7 produce task 150pool-1-thread-2 execute task, name : Thread7 name value : 140// 省略6行pool-1-thread-2 execute task, name : Thread7 name value : 140Thread8 produce task 161// 省略8行Thread8 produce task 170Thread9 produce task 181// 省略8行Thread9 produce task 190pool-1-thread-4 execute task, name : Thread9 name value : 180pool-1-thread-4 execute task, name : Thread9 name value : 180Thread10 produce task 201// 省略8行Thread10 produce task 210pool-1-thread-3 execute task, name : Thread10 name value : 200// 省略39行pool-1-thread-3 execute task, name : Thread10 name value : 200 虽然生产总数和消费总数都是100，但是明显有的消费多了，有的消费少了。合理推测一下，应该是在主线程放进任务后，子线程才生成。为了验证这个猜想，将线程池用 ThreadPoolExecutor 生成，并在用子线程生成任务之前，先赋值 context 并开启所有线程：123456789101112131415161718192021222324252627282930313233343536373839public static void main(String[] args) &#123; // 固定线程池 ThreadPoolExecutor executorService = new ThreadPoolExecutor( 4, 4, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;&gt;() ); // 在main线程中赋值 Context context = new Context(); context.name = "Thread0 name"; context.value = 0; InheritableThreadLocalContext.context.set(context); // 开启所有线程 executorService.prestartAllCoreThreads(); for (int i = 1; i &lt;= 10; i++) &#123; int finalI = i; new Thread( () -&gt; &#123; // 生成任务的线程对context进行赋值 Context contextMain = new Context(); contextMain.name = String.format("Thread%s name", finalI); contextMain.value = finalI * 20; InheritableThreadLocalContext.context.set(contextMain); // 提交任务 for (int j = 1; j &lt;= 10; j++) &#123; System.out.println("Thread" + finalI + " produce task " + (finalI * 20 + j)); executorService.execute(() -&gt; &#123; // 执行任务的子线程 Context contextChild = InheritableThreadLocalContext.context.get(); System.out.println(Thread.currentThread().getName() + " execute task, name : " + contextChild.name + " value : " + contextChild.value); &#125;); &#125; &#125; ).start(); &#125;&#125; 结果不出所料，执行任务的线程输出的，都是最外面主线程设置的值。 那么我们该如何才能达到最初想要的效果呢？就是利用线程池执行任务时，如何能够让执行者线程能够获取调用者线程的 context 呢？ 使用 TransmittableThreadLocal 解决上面的问题主要是因为执行任务的线程是被线程池管理，可以被复用（可以称为池化复用）。那复用了之后，如果还是依赖于父线程的 context，自然是有问题的，因为我们想要的效果是执行线程获取调用线程的 context，这时候就是TransmittableThreadLocal出场了。 TransmittableThreadLocal 是阿里提供的工具类，其主要解决的就是上面遇到的问题。那么该如何使用呢？ 首先，你需要引入相应的依赖：12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;transmittable-thread-local&lt;/artifactId&gt; &lt;version&gt;2.11.0&lt;/version&gt;&lt;/dependency&gt; 具体代码，就拿上文提到的情况，我们用 TransmittableThreadLocal 做一个改造：1234567891011121314151617181920212223242526272829303132333435363738394041public class TransmittableThreadLocalTest &#123; private static TransmittableThreadLocal&lt;Context&gt; context = new TransmittableThreadLocal&lt;&gt;(); static class Context &#123; String name; int value; &#125; public static void main(String[] args) &#123; // 固定线程池 ExecutorService executorService = Executors.newFixedThreadPool(4); for (int i = 1; i &lt;= 10; i++) &#123; int finalI = i; new Thread( () -&gt; &#123; // 生成任务的线程对context进行赋值 Context contextMain = new Context(); contextMain.name = String.format("Thread%s name", finalI); contextMain.value = finalI * 20; TransmittableThreadLocalTest.context.set(contextMain); // 提交任务 for (int j = 1; j &lt;= 10; j++) &#123; System.out.println("Thread" + finalI + " produce task " + (finalI * 20 + j)); Runnable task = () -&gt; &#123; // 执行任务的子线程 Context contextChild = TransmittableThreadLocalTest.context.get(); System.out.println(Thread.currentThread().getName() + " execute task, name : " + contextChild.name + " value : " + contextChild.value); &#125;; // 额外的处理，生成修饰了的对象ttlRunnable Runnable ttlRunnable = TtlRunnable.get(task); executorService.execute(ttlRunnable); &#125; &#125; ).start(); &#125; &#125;&#125; 此时再次运行，就会发现执行线程运行时的输出内容是完全可以和调用线程对应上的了。当然了，我这种方式是修改了 Runnable 的写法，阿里也提供了线程池的写法，简单如下：12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; // 固定线程池 ExecutorService executorService = Executors.newFixedThreadPool(4); // 额外的处理，生成修饰了的对象executorService executorService = TtlExecutors.getTtlExecutorService(executorService); ExecutorService finalExecutorService = executorService; for (int i = 1; i &lt;= 10; i++) &#123; int finalI = i; new Thread( () -&gt; &#123; // 生成任务的线程对context进行赋值 Context contextMain = new Context(); contextMain.name = String.format("Thread%s name", finalI); contextMain.value = finalI * 20; TransmittableThreadLocalTest.context.set(contextMain); // 提交任务 for (int j = 1; j &lt;= 10; j++) &#123; System.out.println("Thread" + finalI + " produce task " + (finalI * 20 + j)); Runnable task = () -&gt; &#123; // 执行任务的子线程 Context contextChild = TransmittableThreadLocalTest.context.get(); System.out.println(Thread.currentThread().getName() + " execute task, name : " + contextChild.name + " value : " + contextChild.value); &#125;; finalExecutorService.execute(task); &#125; &#125; ).start(); &#125;&#125; 其实还有更加简单的写法，具体可以参考其github:https://github.com/alibaba/transmittable-thread-local 总结其实两篇 ThreadLocal 升级文章的出现，都是因为周三听了一个部门关于 TTL 的分享会，也是介绍了 TransmittableThreadLocal，但因为携程商旅面临国际化的改动，当前的语种信息肯定是存储在线程的 context 中最方便，但涉及到线程传递的问题（因为会调用异步接口等等），所以自然就需要考虑这个了。性能方面的话，他们有做过测试，但我也只是一个听者，并没有具体使用过，大家也可以一起交流。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://jjcoder.top/ 公众号：健程之道]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ThreadLocal</tag>
        <tag>TransmittableThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal的进化——InheritableThreadLocal]]></title>
    <url>%2F2019%2F12%2F13%2FThreadLocal%E7%9A%84%E8%BF%9B%E5%8C%96%E2%80%94%E2%80%94InheritableThreadLocal%2F</url>
    <content type="text"><![CDATA[之前有介绍过 ThreadLocal，JDK 后来针对此做了一个升级版本 InheritableThreadLocal，今天就来好好介绍下。 为什么要升级首先我们来想想，为什么要升级？这就要说起 ThreadLocal 的功能了。 我们知道，ThreadLocal 设计初衷是为了在多线程环境下，针对每一个线程能有一个自己的副本，这样可以在一定程度上解决多线程并发修改的问题。但是，我们可以在此基础上做一个拓展，比如context，我们可以利用 ThreadLocal 针对每一个线程都有一个自己的上下文，一般都是写成ThreadLocal&lt;Context&gt;，这样在这个线程上做的所有修改都可以被大家利用到。 此时设想一下，假如我们新建一个子线程，那这个子线程可以获取到父线程的context吗？理论上希望可以达成这样的效果，实际上呢？让我们看看：123456789101112131415161718192021222324252627282930public class ThreadLocalContext &#123; private static ThreadLocal&lt;Context&gt; context = new ThreadLocal&lt;&gt;(); static class Context &#123; String name; int value; &#125; public static void main(String[] args) &#123; Context context = new Context(); context.name = "mainName"; context.value = 10; ThreadLocalContext.context.set(context); Thread childThread = new Thread( new Runnable() &#123; @Override public void run() &#123; Context childContext = ThreadLocalContext.context.get(); System.out.println(childContext.name); System.out.println(childContext.value); &#125; &#125; ); childThread.start(); &#125;&#125; 运行 main 方法之后，直接在子线程中抛错，这样确实符合我们的预期，但如果我们想达到子线程可以获取到父线程的 context这样的效果该如何做呢？ 首先想到的就是在生成子线程的时候，将父线程 ThreadLocal 里的值传给子线程。这样做虽然能达到效果，但过程比较繁杂，且代码侵入性强。 这个时候就可以用InheritableThreadLocal了。 什么是 InheritableThreadLocal看源码先让我们看看它的源码，大家不要怕，它的源码很少：1234567891011121314public class InheritableThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; protected T childValue(T parentValue) &#123; return parentValue; &#125; ThreadLocalMap getMap(Thread t) &#123; return t.inheritableThreadLocals; &#125; void createMap(Thread t, T firstValue) &#123; t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); &#125;&#125; 首先它继承自 ThreadLocal，那么它其实就是 ThreadLocal 的一个拓展版本，接下来就是这三个方法，其实这三个方法在 ThreadLocal 都是有的，我们来看看：1234567891011T childValue(T parentValue) &#123; throw new UnsupportedOperationException();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 除了childValue方法在 ThreadLocal 中是抛出异常的，其余两个方法在两个类中都几乎是一样，只是针对的对象不同而已，但threadLocals和inheritableThreadLocals都是ThreadLocal.ThreadLocalMap类型，这个在之前的文章中有说过，就是一个 key 为弱引用的 Entry，这个倒不是重点。 我们再来看看 inheritableThreadLocals 是在何时被初始化的，从源码可以得知：12345678910111213private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc) &#123; // 省略无关代码 ... Thread parent = currentThread(); ... // 省略无关代码 ... if (parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); ... &#125; 当我们通过父线程调用 Thread 的构造方法生成一个子线程时，其构造方法最终会调用这个 init 方法。从这儿可以看出， inheritableThreadLocals 是来自于父线程的 inheritableThreadLocals，那这样也就解释了为什么 inheritableThreadLocals 支持在子线程中使用父线程中存储的变量。 如何使用让我们还是回到上文提到的 context 的例子，用 InheritableThreadLocal 进行改造：123456789101112131415161718192021222324252627282930public class ThreadLocalContext &#123; private static InheritableThreadLocal&lt;Context&gt; context = new InheritableThreadLocal&lt;&gt;(); static class Context &#123; String name; int value; &#125; public static void main(String[] args) &#123; Context context = new Context(); context.name = "mainName"; context.value = 10; ThreadLocalContext.context.set(context); Thread childThread = new Thread( new Runnable() &#123; @Override public void run() &#123; Context childContext = ThreadLocalContext.context.get(); System.out.println(childContext.name); System.out.println(childContext.value); &#125; &#125; ); childThread.start(); &#125;&#125; 运行后，不仅没有抛出异常，而且在子线程中输出了父线程设置好的值。皆大欢喜！ 总结今天分享了 InheritableThreadLocal，主要是因为周三在携程的分享会上听到了别人谈了这方面的分享，主讲人讲了一个更加普遍的问题，如果我们用线程池提交任务的话，线程池中的线程在执行任务时，如何能够获得提交任务的线程的 context，这时就要用到阿里的开源组件 TTL，我会在之后进行介绍。 加入携程也有1个月了，虽然感受到大公司有不少的弊端，比如沟通难等，但也有不少的优点，比如技术分享会，虽然也是忙里偷闲去参加的，但有了更多和技术相关的可以学习和交流的机会，也挺好的。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://jjcoder.top/ 公众号：健程之道]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ThreadLocal</tag>
        <tag>InheritableThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java——内部类详解]]></title>
    <url>%2F2019%2F11%2F26%2FJava%E2%80%94%E2%80%94%E5%86%85%E9%83%A8%E7%B1%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[说起内部类，大家肯定感觉熟悉又陌生，因为一定在很多框架源码中有看到别人使用过，但又感觉自己使用的比较少，今天我就带你具体来看看内部类。 内部类基础 所谓内部类就是在类的内部继续定义其他内部结构类。 在 Java 中，广泛意义上的内部类一般来说包括这四种：成员内部类、局部内部类、匿名内部类和静态内部类。下面就先来了解一下这四种内部类的用法。 成员内部类成员内部类是最普通的内部类，它的定义为位于另一个类的内部，具体使用如下： 12345678910111213141516class Circle &#123; double radius = 0; public Circle(double radius) &#123; this.radius = radius; &#125; /** * 内部类 */ class Draw &#123; public void drawSahpe() &#123; System.out.println("drawshape"); &#125; &#125;&#125; 这样看起来，类 Draw 像是类 Circle 的一个成员， Circle 称为外部类。成员内部类可以无条件访问外部类的所有成员属性和成员方法（包括 private 成员和静态成员），例如： 12345678910111213141516171819class Circle &#123; private double radius = 0; public static int count =1; public Circle(double radius) &#123; this.radius = radius; &#125; /** * 内部类 */ class Draw &#123; public void drawSahpe() &#123; // 外部类的private成员 System.out.println(radius); // 外部类的静态成员 System.out.println(count); &#125; &#125;&#125; 不过要注意的是，当成员内部类拥有和外部类同名的成员变量或者方法时，会发生隐藏现象，即默认情况下访问的是成员内部类的成员。如果要访问外部类的同名成员，需要采取以下形式进行访问：12外部类.this.成员变量外部类.this.成员方法 虽然成员内部类可以无条件地访问外部类的成员，而外部类想访问成员内部类的成员却不是这么随心所欲了。在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问，其具体形式为： 1234567891011121314151617181920212223class Circle &#123; private double radius = 0; public Circle(double radius) &#123; this.radius = radius; // 必须先创建成员内部类的对象，再进行访问 getDrawInstance().drawSahpe(); &#125; private Draw getDrawInstance() &#123; return new Draw(); &#125; /** * 内部类 */ class Draw &#123; public void drawSahpe() &#123; // 外部类的private成员 System.out.println(radius); &#125; &#125;&#125; 成员内部类是依附外部类而存在的，也就是说，如果要创建成员内部类的对象，前提是必须存在一个外部类的对象。创建成员内部类对象的一般方式如下： 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; // 第一种方式 Outter outter = new Outter(); // 必须通过Outter对象来创建 Outter.Inner inner = outter.new Inner(); // 第二种方式 Outter.Inner inner1 = outter.getInnerInstance(); &#125;&#125;class Outter &#123; private Inner inner = null; public Outter() &#123; &#125; public Inner getInnerInstance() &#123; if(inner == null) inner = new Inner(); return inner; &#125; class Inner &#123; public Inner() &#123; &#125; &#125;&#125; 内部类可以拥有 private 访问权限、 protected 访问权限、 public 访问权限及包访问权限。 比如上面的例子，如果成员内部类 Inner 用 private 修饰，则只能在外部类的内部访问；如果用 public 修饰，则任何地方都能访问；如果用 protected 修饰，则只能在同一个包下或者继承外部类的情况下访问；如果是默认访问权限，则只能在同一个包下访问。 这一点和外部类有一点不一样，外部类只能被 public 和包访问两种权限修饰。 我个人是这么理解的，由于成员内部类看起来像是外部类的一个成员，所以可以像类的成员一样拥有多种权限修饰。 局部内部类局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内。 12345678910111213141516171819class People&#123; public People() &#123; &#125;&#125;class Man&#123; public Man()&#123; &#125; public People getWoman()&#123; /** * 局部内部类 */ class Woman extends People&#123; int age =0; &#125; return new Woman(); &#125;&#125; 注意，局部内部类就像是方法里面的一个局部变量一样，是不能用 public 、 protected 、 private 以及 static 修饰的。 匿名内部类匿名内部类应该是平时我们编写代码时用得最多的，比如创建一个线程的时候： 1234567891011121314class Test &#123; public static void main(String[] args) &#123; Thread thread = new Thread( // 匿名内部类 new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread run"); &#125; &#125; ); &#125;&#125; 同样的，匿名内部类也是不能有访问修饰符和 static 修饰符的。 匿名内部类是唯一一种没有构造器的类。正因为其没有构造器，所以匿名内部类的使用范围非常有限，大部分匿名内部类用于接口回调。 匿名内部类在编译的时候由系统自动起名为Outter$1.class。一般来说，匿名内部类用于继承其他类或是实现接口，并不需要增加额外的方法，只是对继承方法的实现或是重写。 静态内部类静态内部类也是定义在另一个类里面的类，只不过在类的前面多了一个关键字 static 。 静态内部类是不需要依赖于外部类的，这点和类的静态成员属性有点类似，并且它不能使用外部类的非 static 成员变量或者方法，这点很好理解，因为在没有外部类的对象的情况下，可以创建静态内部类的对象，如果允许访问外部类的非 static 成员就会产生矛盾，因为外部类的非 static 成员必须依附于具体的对象。 例如：123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; Outter.Inner inner = new Outter.Inner(); &#125;&#125;class Outter &#123; public Outter() &#123; &#125; /** * 静态 */ static class Inner &#123; public Inner() &#123; &#125; &#125;&#125; 深入理解内部类通过上面的介绍，相比你已经大致了解的内部类的使用，那么你的心里想必会有一个疑惑： 为什么成员内部类可以无条件访问外部类的成员？首先我们先定义一个内部类：1234567891011121314151617public class Outter &#123; private Inner inner = null; public Outter() &#123; &#125; public Inner getInnerInstance() &#123; if (inner == null) inner = new Inner(); return inner; &#125; protected class Inner &#123; public Inner() &#123; &#125; &#125;&#125; 先用 javac 进行编译，你可以发现会生成两个文件： Outter$Inner.class 和 Outter.class 。接下来利用javap -p反编译 Outter$Inner.class ，其结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Classfile /D:/project/Test/src/test/java/test/Outter$Inner.class Last modified 2019-11-25; size 408 bytes MD5 checksum b936e37bc77059b83951429e28f3f225 Compiled from "Outter.java"public class Outter$Inner minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Fieldref #3.#13 // test/Outter$Inner.this$0:Ltest/Outter; #2 = Methodref #4.#14 // java/lang/Object."&lt;init&gt;":()V #3 = Class #16 // test/Outter$Inner #4 = Class #19 // java/lang/Object #5 = Utf8 this$0 #6 = Utf8 Ltest/Outter; #7 = Utf8 &lt;init&gt; #8 = Utf8 (Ltest/Outter;)V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 SourceFile #12 = Utf8 Outter.java #13 = NameAndType #5:#6 // this$0:Ltest/Outter; #14 = NameAndType #7:#20 // "&lt;init&gt;":()V #15 = Class #21 // test/Outter #16 = Utf8 test/Outter$Inner #17 = Utf8 Inner #18 = Utf8 InnerClasses #19 = Utf8 java/lang/Object #20 = Utf8 ()V #21 = Utf8 test/Outter&#123; final Outter this$0; descriptor: Ltest/Outter; flags: ACC_FINAL, ACC_SYNTHETIC public Outter$Inner(Outter); descriptor: (Ltest/Outter;)V flags: ACC_PUBLIC Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Ltest/Outter; 5: aload_0 6: invokespecial #2 // Method java/lang/Object."&lt;init&gt;":()V 9: return LineNumberTable: line 16: 0 line 17: 9&#125;SourceFile: "Outter.java"InnerClasses: protected #17= #3 of #15; //Inner=class test/Outter$Inner of class test/Outter 32行的内容为：final Outter this$0; 学过 C 的朋友应该能知道，这是一个指向外部类 Outter 对象的指针，也就是说编译器会默认为成员内部类添加一个指向外部类对象的引用，这样也就解释了为什么成员内部类能够无条件访问外部类了。 那么这个引用是如何赋初值的呢？下面接着看内部类的构造器：public Outter$Inner(Outter); 从这里可以看出，虽然我们在定义的内部类的构造器是无参构造器，但编译器还是会默认添加一个参数，该参数的类型为指向外部类对象的一个引用，所以成员内部类中的 Outter this&amp;0 指针便指向了外部类对象，因此可以在成员内部类中随意访问外部类的成员。 从这里也间接说明了成员内部类是依赖于外部类的，如果没有创建外部类的对象，则无法对 Outter this&amp;0 引用进行初始化赋值，也就无法创建成员内部类的对象了。 为什么局部内部类和匿名内部类只能访问局部final变量？我们还是采用和之前一样的解答方式，先定义一个类：123456789101112131415161718public class Outter &#123; public static void main(String[] args) &#123; Outter outter = new Outter(); int b = 10; outter.test(b); &#125; public void test(final int b) &#123; final int a = 10; new Thread()&#123; public void run() &#123; System.out.println(a); System.out.println(b); &#125;; &#125;.start(); &#125;&#125; 通过 javac 编译 Outter，也会生成两个文件： Outter.class 和 Outter1.class。默认情况下，编译器会为匿名内部类和局部内部类起名为 Outter$x.class（ x 为正整数）。 根据我提供的类，可以思考一个问题： 当 test 方法执行完毕之后，变量 a 的生命周期就结束了，而此时 Thread 对象的生命周期很可能还没有结束，那么在 Thread 的 run 方法中继续访问变量 a 就变成不可能了，但是又要实现这样的效果，怎么办呢？ Java 采用了复制的手段来解决这个问题。将 Outter$1.class 反编译可以得到下面的内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697Classfile /D:/project/Test/src/test/java/test/Outter$1.class Last modified 2019-11-25; size 653 bytes MD5 checksum 2e238dafbd73356eba22d473c6469082 Compiled from "Outter.java"class test.Outter$1 extends java.lang.Thread minor version: 0 major version: 52 flags: ACC_SUPERConstant pool: #1 = Fieldref #6.#23 // test/Outter$1.this$0:Ltest/Outter; #2 = Fieldref #6.#24 // test/Outter$1.val$b:I #3 = Methodref #7.#25 // java/lang/Thread."&lt;init&gt;":()V #4 = Fieldref #26.#27 // java/lang/System.out:Ljava/io/PrintStream; #5 = Methodref #28.#29 // java/io/PrintStream.println:(I)V #6 = Class #30 // test/Outter$1 #7 = Class #32 // java/lang/Thread #8 = Utf8 val$b #9 = Utf8 I #10 = Utf8 this$0 #11 = Utf8 Ltest/Outter; #12 = Utf8 &lt;init&gt; #13 = Utf8 (Ltest/Outter;I)V #14 = Utf8 Code #15 = Utf8 LineNumberTable #16 = Utf8 run #17 = Utf8 ()V #18 = Utf8 SourceFile #19 = Utf8 Outter.java #20 = Utf8 EnclosingMethod #21 = Class #33 // test/Outter #22 = NameAndType #34:#35 // test:(I)V #23 = NameAndType #10:#11 // this$0:Ltest/Outter; #24 = NameAndType #8:#9 // val$b:I #25 = NameAndType #12:#17 // "&lt;init&gt;":()V #26 = Class #36 // java/lang/System #27 = NameAndType #37:#38 // out:Ljava/io/PrintStream; #28 = Class #39 // java/io/PrintStream #29 = NameAndType #40:#35 // println:(I)V #30 = Utf8 test/Outter$1 #31 = Utf8 InnerClasses #32 = Utf8 java/lang/Thread #33 = Utf8 test/Outter #34 = Utf8 test #35 = Utf8 (I)V #36 = Utf8 java/lang/System #37 = Utf8 out #38 = Utf8 Ljava/io/PrintStream; #39 = Utf8 java/io/PrintStream #40 = Utf8 println&#123; final int val$b; descriptor: I flags: ACC_FINAL, ACC_SYNTHETIC final test.Outter this$0; descriptor: Ltest/Outter; flags: ACC_FINAL, ACC_SYNTHETIC test.Outter$1(test.Outter, int); descriptor: (Ltest/Outter;I)V flags: Code: stack=2, locals=3, args_size=3 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Ltest/Outter; 5: aload_0 6: iload_2 7: putfield #2 // Field val$b:I 10: aload_0 11: invokespecial #3 // Method java/lang/Thread."&lt;init&gt;":()V 14: return LineNumberTable: line 10: 0 public void run(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 3: bipush 10 5: invokevirtual #5 // Method java/io/PrintStream.println:(I)V 8: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 11: aload_0 12: getfield #2 // Field val$b:I 15: invokevirtual #5 // Method java/io/PrintStream.println:(I)V 18: return LineNumberTable: line 12: 0 line 13: 8 line 14: 18&#125;SourceFile: "Outter.java"EnclosingMethod: #21.#22 // test.Outter.testInnerClasses: #6; //class test/Outter$1 我们看到在 run 方法中有一条指令：bipush 10 这条指令表示将操作数10压栈，表示使用的是一个本地局部变量。 这个过程是在编译期间由编译器默认进行，如果这个变量的值在编译期间可以确定，则编译器默认会在匿名内部类（局部内部类）的常量池中添加一个内容相等的字面量或直接将相应的字节码嵌入到执行字节码中。 这样一来，匿名内部类使用的变量是另一个局部变量，只不过值和方法中局部变量的值相等，因此和方法中的局部变量完全独立开。 接下来也来看一下 test.Outter$1 的构造方法：test.Outter$1(test.Outter, int); 我们看到匿名内部类 Outter$1 的构造器含有两个参数，一个是指向外部类对象的引用，一个是 int 型变量，很显然，这里是将变量 test 方法中的形参 b 以参数的形式传进来对匿名内部类中的拷贝（变量 b 的拷贝）进行赋值初始化。 也就说如果局部变量的值在编译期间就可以确定，则直接在匿名内部里面创建一个拷贝。如果局部变量的值无法在编译期间确定，则通过构造器传参的方式来对拷贝进行初始化赋值。 从上面可以看出，在 run 方法中访问的变量 b 根本就不是test方法中的局部变量 b 。这样一来就解决了前面所说的 生命周期不一致的问题。但是新的问题又来了，既然在 run 方法中访问的变量 b 和test方法中的变量 b 不是同一个变量，那么当在 run 方法中改变变量 b 的值的话，会出现什么情况？ 会造成数据不一致性，这样就达不到原本的意图和要求。为了解决这个问题， Java 编译器就限定必须将变量 b 限制为 final ，不允许对变量 b 进行更改（对于引用类型的变量，是不允许指向新的对象），这样数据不一致性的问题就得以解决了。 到这里，想必大家应该清楚为何 方法中的局部变量和形参都必须用 final 进行限定了。 静态内部类有特殊的地方吗？从前面可以知道，静态内部类是不依赖于外部类的，也就说可以在不创建外部类对象的情况下创建内部类的对象。 另外，静态内部类是不持有指向外部类对象的引用的，这个读者可以自己尝试反编译 class 文件看一下就知道了，是没有 Outter this&amp;0 引用的。 总结今天介绍了内部类相关的知识，包括其一般的用法以及内部类和外部类的依赖关系，通过对字节码进行反编译详细了解了其实现模式，最后留给大家一个任务自己去实际探索一下静态内部类的实现。希望通过这篇介绍可以帮大家更加深刻了解内部类。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://jjcoder.top/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>内部类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC 知识点补充——CMS]]></title>
    <url>%2F2019%2F11%2F01%2FGC%20%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85%E2%80%94%E2%80%94CMS%2F</url>
    <content type="text"><![CDATA[之前已经讲过了不少有关 GC 的内容，今天准备将之前没有细讲的部分进行补充，首先要提到的就是垃圾收集器。 基础的回收方式有三种：清除、压缩、复制，衍生出来的垃圾收集器有： Serial 收集器新生代收集器，使用停止复制算法，使用一个线程进行 GC ，串行，其它工作线程暂停。 使用-XX:+UseSerialGC开关来控制使用Serial + Serial Old模式运行进行内存回收（这也是虚拟机在 Client 模式下运行的默认值）。 ParNew 收集器新生代收集器，使用停止复制算法，Serial 收集器的多线程版，用多个线程进行 GC ，并行，其它工作线程暂停，关注缩短垃圾收集时间。 使用-XX:+UseParNewGC开关来控制使用ParNew + Serial Old收集器组合收集内存；使用-XX:ParallelGCThreads来设置执行内存回收的线程数。 Parallel Scavenge 收集器新生代收集器，使用停止复制算法，关注 CPU 吞吐量，即运行用户代码的时间/总时间，比如：JVM 运行 100 分钟，其中运行用户代码 99 分钟，垃 圾收集 1 分钟，则吞吐量是 99% ，这种收集器能最高效率的利用 CPU ，适合运行后台运算（其他关注缩短垃圾收集时间的收集器，如 CMS ，等待时间很少，所以适 合用户交互，提高用户体验）。 使用-XX:+UseParallelGC开关控制使用Parallel Scavenge + Serial Old收集器组合回收垃圾（这也是在 Server 模式下的默认值）；使用-XX:GCTimeRatio来设置用户执行时间占总时间的比例，默认 99 ，即 1% 的时间用来进行垃圾回收。使用-XX:MaxGCPauseMillis设置 GC 的最大停顿时间（这个参数只对 Parallel Scavenge 有效），用开关参数-XX:+UseAdaptiveSizePolicy可以进行动态控制，如自动调整 Eden / Survivor 比例，老年代对象年龄，新生代大小等，这个参数在 ParNew 下没有。 Serial Old 收集器老年代收集器，单线程收集器，串行，使用标记-整理算法，使用单线程进行GC，其它工作线程暂停（注意：在老年代中进行标记-整理算法清理，也需要暂停其它线程），在JDK1.5之前，Serial Old 收集器与 ParallelScavenge 搭配使用。 整理的方法是 Sweep （清除）和 Compact （压缩），清除是将废弃的对象干掉，只留幸存的对象，压缩是移动对象，将空间填满保证内存分为2块，一块全是对象，一块空闲）， Parallel Old 收集器老年代收集器，多线程，并行，多线程机制与 Parallel Scavenge 差不错，使用标记-整理算法，在 Parallel Old 执行时，仍然需要暂停其它工作线程。 Parallel Old 收集器的整理，与 Serial Old 不同，这里的整理是Copy（复制）和Compact（压缩），复制的意思就是将幸存的对象复制到预先准备好的区域，而不是像Sweep（清除）那样清除废弃的对象。 Parallel Old 在多核计算中很有用。 Parallel Old 出现后（JDK 1.6），与 Parallel Scavenge 配合有很好的效果，充分体现 Parallel Scavenge 收集器吞吐量优先的效果。使用-XX:+UseParallelOldGC开关控制使用Parallel Scavenge + Parallel Old组合收集器进行收集。 CMS全称 Concurrent Mark Sweep，老年代收集器，致力于获取最短回收停顿时间（即缩短垃圾回收的时间），使用标记-清除算法，多线程，优点是并发收集（用户线程可以和 GC 线程同时工作），停顿小。 使用-XX:+UseConcMarkSweepGC进行ParNew + CMS + Serial Old进行内存回收，优先使用ParNew + CMS（原因见后面），当用户线程内存不足时，采用备用方案Serial Old收集。 如何开始首先来看一下 CMS 是在什么情况下进行 GC： 首先 JVM 根据-XX:CMSInitiatingOccupancyFraction、-XX:+UseCMSInitiatingOccupancyOnly来决定什么时间开始垃圾收集。 如果设置了-XX:+UseCMSInitiatingOccupancyOnly，那么只有当老年代占用确实达到了-XX:CMSInitiatingOccupancyFraction参数所设定的比例时才会触发 CMS GC。 如果没有设置-XX:+UseCMSInitiatingOccupancyOnly，那么系统会根据统计数据自行决定什么时候触发 CMS GC。因此有时会遇到设置了 80% 比例才 CMS GC，但是 50% 时就已经触发了，就是因为这个参数没有设置的原因。 具体执行CMS GC 的执行过程，具体来说就是： 初始标记(CMS-initial-mark)该阶段是 stop the world 阶段，因此此阶段标记的对象只是从 root 集最直接可达的对象。 此阶段会打印 1 条日志：CMS-initial-mark：961330K（1572864K），指标记时，老年代的已用空间和总空间 并发标记(CMS-concurrent-mark)此阶段是和应用线程并发执行的，所谓并发收集器指的就是这个，主要作用是标记可达的对象，此阶段不需要用户线程停顿。 此阶段会打印 2 条日志：CMS-concurrent-mark-start，CMS-concurrent-mark 预清理(CMS-concurrent-preclean)此阶段主要是进行一些预清理，因为标记和应用线程是并发执行的，因此会有些对象的状态在标记后会改变，此阶段正是解决这个问题。因为之后的 CMS-remark 阶段也会 stop the world，为了使暂停的时间尽可能的小，也需要 preclean 阶段先做一部分工作以节省时间。 此阶段会打印 2 条日志：CMS-concurrent-preclean-start，CMS-concurrent-preclean 可控预清理(CMS-concurrent-abortable-preclean)此阶段的目的是使 CMS GC 更加可控一些，作用也是执行一些预清理，以减少 CMS-remark 阶段造成应用暂停的时间。 此阶段涉及几个参数：123-XX:CMSMaxAbortablePrecleanTime：当 abortable-preclean 阶段执行达到这个时间时才会结束。-XX:CMSScheduleRemarkEdenSizeThreshold（默认2m）：控制 abortable-preclean 阶段什么时候开始执行，即当年轻代使用达到此值时，才会开始 abortable-preclean 阶段。-XX:CMSScheduleRemarkEdenPenetratio（默认50%）：控制 abortable-preclean 阶段什么时候结束执行。 此阶段会打印 3 条日志：CMS-concurrent-abortable-preclean-start，CMS-concurrent-abortable-preclean，CMS：abort preclean due to time XXX 重新标记(CMS-remark)此阶段暂停应用线程，停顿时间比并发标记小得多，但比初始标记稍长，因为会对所有对象进行重新扫描并标记。 此阶段会打印以下日志： YG occupancy：964861K（2403008K），指执行时年轻代的情况。 CMS remark：961330K（1572864K），指执行时老年代的情况。 此外，还打印出了弱引用处理、类卸载等过程的耗时。 并发清除(CMS-concurrent-sweep)此阶段进行并发的垃圾清理。 并发重设状态等待下次CMS的触发(CMS-concurrent-reset)此阶段是为下一次 CMS GC 重置相关数据结构。 总结CMS 的收集过程，概括一下就是：2 次标记，2 次预清除，1 次重新标记，1 次清除。 在CMS清理过程中，只有初始标记和重新标记需要短暂停顿用户线程，并发标记和并发清除都不需要暂停用户线程，因此效率很高，很适合高交互的场合。 CMS也有缺点，它需要消耗额外的 CPU 和内存资源。在 CPU 和内存资源紧张，会加重系统负担（CMS 默认启动线程数为( CPU数量 + 3 ) / 4 ）。 另外，在并发收集过程中，用户线程仍然在运行，仍然产生内存垃圾，所以可能产生“浮动垃圾”（本次无法清理，只能下一次Full GC才清理）。因此在 GC 期间，需要预留足够的内存给用户线程使用。 所以使用 CMS 的收集器并不是老年代满了才触发 Full GC ，而是在使用了一大半（默认 68% ，即 2/3 ，使用-XX:CMSInitiatingOccupancyFraction来设置）的时候就要进行 Full GC。如果用户线程消耗内存不是特别大，可以适当调高-XX:CMSInitiatingOccupancyFraction以降低 GC 次数，提高性能。如果预留的用户线程内存不够，则会触发 Concurrent Mode Failure，此时，将触发备用方案：使用 Serial Old 收集器进行收集，但这样停顿时间就长了，因此-XX:CMSInitiatingOccupancyFraction不宜设的过大。 还有，CMS 采用的是标记-清除算法，会导致内存碎片的产生，可以使用-XX：+UseCMSCompactAtFullCollection来设置是否在 Full GC 之后进行碎片整理，用-XX：CMSFullGCsBeforeCompaction来设置在执行多少次不压缩的 Full GC 之后，来一次带压缩的 Full GC。 并发和并行并发收集： 指用户线程与GC线程同时执行（不一定是并行，可能交替，但总体上是在同时执行的），不需要停顿用户线程（其实在 CMS 中用户线程还是需要停顿的，只是非常短，GC 线程在另一个 CPU 上执行）； 并行收集： 指多个 GC 线程并行工作，但此时用户线程是暂停的； 所以，Serial 是串行的，Parallel 收集器是并行的，而 CMS 收集器是并发的。 总结今天了解了一下普通的垃圾收集器，并且详细介绍了 CMS，其特性其实是基于普通的垃圾算法，增加了预处理、预清除的过程，因此效率更加优越。当然它也有自己的缺点，更加消耗资源，因此在选用的时候需要结合实际场景。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://jjcoder.top/]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
        <tag>CMS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 知识点补充——永久代和元空间]]></title>
    <url>%2F2019%2F10%2F31%2FJVM%20%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85%E2%80%94%E2%80%94%E6%B0%B8%E4%B9%85%E4%BB%A3%E5%92%8C%E5%85%83%E7%A9%BA%E9%97%B4%2F</url>
    <content type="text"><![CDATA[之前已经讲过了不少有关 JVM 的内容，今天准备将之前没有细讲的部分进行补充，比如：永久代和元空间。 永久代Java 的内存中有一块称之为方法区的部分，在 JDK8 之前， Hotspot 虚拟机中的实现方式为永久代（Permanent Generation），别的JVM都没有这个东西。 在过去（当自定义类加载器使用不普遍的时候），类几乎是“静态的”并且很少被卸载和回收，因此类也可以被看成“永久的”。另外由于类作为 JVM 实现的一部分，它们不由程序来创建，因为它们也被认为是“非堆”的内存。 永久代是一段连续的内存空间，我们在 JVM 启动之前可以通过设置-XX:MaxPermSize的值来控制永久代的大小，32 位机器默认的永久代的大小为 64M，64 位的机器则为 85M。 永久代的垃圾回收和老年代的垃圾回收是绑定的，一旦其中一个区域被占满，这两个区都要进行垃圾回收。但是有一个明显的问题，由于我们可以通过‑XX:MaxPermSize设置永久代的大小，一旦类的元数据超过了设定的大小，程序就会耗尽内存，并出现内存溢出错误 (java.lang.OutOfMemoryError: PermGen space)。 为什么类的元数据占用内存会那么大？因为在 JDK7 之前的 HotSpot 虚拟机中，纳入字符串常量池的字符串被存储在永久代中，因此导致了一系列的性能问题和内存溢出错误。 为了解决这些性能问题，也为了能够让 Hotspot 能和其他的虚拟机一样管理，元空间就产生了。 元空间元空间是 Hotspot 在 JDK8 中新加的内容，其本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于： 元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小： -XX:MetaspaceSize 初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize最大空间，默认是没有限制的。 除了上面两个指定大小的选项以外，还有两个与 GC 相关的属性： -XX:MinMetaspaceFreeRatio 在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 -XX:MaxMetaspaceFreeRatio 在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 移除永久代的影响由于类的元数据分配在本地内存中，元空间的最大可分配空间就是系统可用内存空间。因此，我们就不会遇到永久代存在时的内存溢出错误，也不会出现泄漏的数据移到交换区这样的事情。最终用户可以为元空间设置一个可用空间最大值，如果不进行设置，JVM 会自动根据类的元数据大小动态增加元空间的容量。 注意：永久代的移除并不代表自定义的类加载器泄露问题就解决了。因此，你还必须监控你的内存消耗情况，因为一旦发生泄漏，会占用你的大量本地内存，并且还可能导致交换区交换更加糟糕。 元空间内存管理元空间的内存管理由元空间虚拟机来完成。 先前，对于类的元数据我们需要不同的垃圾回收器进行处理，现在只需要执行元空间虚拟机的 C++ 代码即可完成。在元空间中，类和其元数据的生命周期和其对应的类加载器是相同的。话句话说，只要类加载器存活，其加载的类的元数据也是存活的，因而不会被回收掉。 准确的来说，每一个类加载器的存储区域都称作一个元空间，所有的元空间合在一起就是我们一直说的元空间。当一个类加载器被垃圾回收器标记为不再存活，其对应的元空间会被回收。在元空间的回收过程中没有重定位和压缩等操作。但是元空间内的元数据会进行扫描来确定 Java 引用。 那具体是如何管理的呢？ 元空间虚拟机负责元空间的分配，其采用的形式为组块分配。组块的大小因类加载器的类型而异。在元空间虚拟机中存在一个全局的空闲组块列表。 当一个类加载器需要组块时，它就会从这个全局的组块列表中获取并维持一个自己的组块列表。 当一个类加载器不再存活时，那么其持有的组块将会被释放，并返回给全局组块列表。 类加载器持有的组块又会被分成多个块，每一个块存储一个单元的元信息。组块中的块是线性分配（指针碰撞分配形式）。组块分配自内存映射区域。这些全局的虚拟内存映射区域以链表形式连接，一旦某个虚拟内存映射区域清空，这部分内存就会返回给操作系统。 运行时常量池运行时常量池在 JDK6 及之前版本的 JVM 中是方法区的一部分，而在 HotSpot 虚拟机中方法区的实现是永久代(Permanent Generation)。所以运行时常量池也是在永久代的。 但是 JDK7 及之后版本的 JVM 已经将字符串常量池从方法区中移了出来，在堆（Heap）中开辟了一块区域存放运行时常量池。 String.intern()是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，则在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用。 存在的问题前面已经提到，元空间虚拟机采用了组块分配的形式，同时区块的大小由类加载器类型决定。类信息并不是固定大小，因此有可能分配的空闲区块和类需要的区块大小不同，这种情况下可能导致碎片存在。元空间虚拟机目前并不支持压缩操作，所以碎片化是目前最大的问题。 总结曾经的永久代，因为容易产生 OOM 而被优化成了元空间，但即便这样，依然存在着问题，不知道 JDK 之后还会怎样优化呢？ 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://jjcoder.top/]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>永久代</tag>
        <tag>元空间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式——原型模式]]></title>
    <url>%2F2019%2F10%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式中，单例模式应该是大家最为熟悉的了，那如果我们需要对一个对象进行多次复制的话，大家会用什么呢？这就要用到今天要讲的原型模式了。 简介其定义为： 使用原型实例指定将要创建的对象类型，通过复制这个实例创建新的对象。 具体来说就是，通过给出一个原型对象来指明所创建的对象的类型，然后使用自身实现的克隆接口来复制这个原型对象，该模式就是用这种方式来创建出更多同类型的对象。 这样的好处是： Object 类的 clone() 方法是一个本地方法，它可以直接操作内存中的二进制流，所以性能相对 new 实例化来说，更加优秀。 一个对象通过 new 实例化创建过程为： 在内存中开辟一块空间。 在开辟的内存空间中创建对象。 调用对象的构造函数进行初始化对象。 而一个对象通过 clone() 创建过程为： 根据原对象内存大小开辟一块内存空间。 复制已有对象，克隆对象中所有属性值。 相对 new 来说，clone() 少了调用构造函数。如果构造函数中存在大量属性初始化或大对象，则使用 clone() 的复制对象的方式性能会好一些。 简单例子让我们通过一个例子来具体了解一下：12345678910111213141516171819202122232425262728293031323334353637383940/** * 实现Cloneable 接口的原型抽象类Prototype */public class Prototype implements Cloneable &#123; /** * 重写 clone() 方法 */ @Override public Prototype clone() &#123; Prototype prototype = null; try &#123; prototype = (Prototype) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return prototype; &#125;&#125;/** * 实现原型类 */public class ConcretePrototype extends Prototype &#123; public void show() &#123; System.out.println(&quot;原型模式实现类&quot;); &#125;&#125;/** * 测试类 */public class Client &#123; public static void main(String[] args) &#123; ConcretePrototype cp = new ConcretePrototype(); for (int i = 0; i &lt; 10; i++) &#123; ConcretePrototype cloneCp = (ConcretePrototype) cp.clone(); cloneCp.show(); &#125; &#125;&#125; 当我们实现原型抽象类时，需要注意三点： 实现 Cloneable 接口：Cloneable 接口与序列化接口的作用类似，它只是告诉虚拟机可以安全地在实现了这个接口的类上使用 clone() 方法。在 JVM 中，只有实现了 Cloneable 接口的类才可以被拷贝，否则会抛出 CloneNotSupportedException 异常。 重写 Object 类中的 clone() 方法：在 Java 中，所有类的父类都是 Object 类，而 Object 类中有一个 clone() 方法，作用是返回对象的一个拷贝。 在重写的 clone() 方法中调用 super.clone()：默认情况下，类不具备复制对象的能力，需要调用 super.clone() 来实现。 深拷贝与浅拷贝谈到了拷贝，就不得不说到一个经典的问题：深拷贝与浅拷贝，有的地方也叫深克隆与浅克隆。 在上面的原型模式中，在调用 super.clone() 方法之后，首先会检查当前对象所属的类是否支持 clone，也就是看该类是否实现了 Cloneable 接口。 如果支持，则创建当前对象所属类的一个新对象，并对该对象进行初始化，使得新对象的成员变量的值与当前对象的成员变量的值一模一样，但对于其它对象的引用以及 List 等类型的成员属性，则只能复制这些对象的引用了。所以简单调用 super.clone() 这种克隆对象方式，就是一种浅拷贝。 为了让大家更加清楚浅拷贝的弊端，举个具体的例子： Student 类中有一个 Teacher 对象，我们让这两个类都实现 Cloneable 接口：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Getter@Setterpublic class Student implements Cloneable&#123; /** * 学生姓名 */ private String name; /** * 学生所属的老师 */ private Teacher teacher; /** * 重写克隆方法，对学生进行克隆 */ public Student clone() &#123; Student student = null; try &#123; student = (Student) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return student; &#125;&#125;@Getter@Setterpublic class Teacher implements Cloneable&#123; /** * 老师姓名 */ private String name; /** * 重写克隆方法，对老师类进行克隆 */ public Teacher clone() &#123; Teacher teacher= null; try &#123; teacher= (Teacher) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return teacher; &#125;&#125; 测试的时候，我们先定义一个学生和一个老师，并让其关联在一起。然后复制之前的学生，生成一个新的学生，修改新学生的老师。12345678910111213141516171819202122public class Test &#123; public static void main(String args[]) &#123; // 定义老师1 Teacher teacher = new Teacher(); teacher.setName("刘老师"); // 定义学生1 Student stu1 = new Student(); stu1.setName("test1"); // 老师1和学生1进行关联 stu1.setTeacher(teacher); // 复制学生1，生成学生2 Student stu2 = stu1.clone(); stu2.setName("test2"); // 修改学生2的老师 stu2.getTeacher().setName("王老师"); // 查看修改结果 System.out.println("学生" + stu1.getName() + "的老师是:" + stu1.getTeacher().getName()); System.out.println("学生" + stu1.getName() + "的老师是:" + stu2.getTeacher().getName()); &#125;&#125; 我们想要的结果是：12学生test1的老师是：刘老师学生test2的老师是：王老师 但实际结果是：12学生test1的老师是：王老师学生test2的老师是：王老师 观察以上运行结果，我们可以发现：在我们给学生2修改老师的时候，学生1的老师也跟着被修改了。这就是浅拷贝带来的问题。 我们可以通过深拷贝的方式解决这类问题，修改 Student 类的 clone() 方法：123456789101112131415/** * 重写克隆方法，对学生和老师都进行克隆 */public Student clone() &#123; Student student = null; try &#123; student = (Student) super.clone(); // 克隆 teacher 对象 Teacher teacher = this.teacher.clone(); student.setTeacher(teacher); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return student;&#125; 此时，我们再次运行 Test 中的 main() 方法，就可以得到我们预想的结果了。 适用场景在一些重复创建对象的场景下，我们就可以使用原型模式来提高对象的创建性能。例如：循环体内创建对象时，我们就可以考虑用 clone() 的方式来实现。 除此之外，原型模式在开源框架中的应用也非常广泛。例如 Spring 中，@Service 默认都是单例的。用了私有全局变量，若不想影响下次注入或每次上下文获取 bean，就需要用到原型模式，我们可以通过以下注解来实现，@Scope(“prototype”)。有兴趣的朋友深入了解一下其中的原理。 总结原型模式，就是针对需要大量复制同一对象的场景，比如用户获取商品、循环体内创建对象等，都是不错的选择，且效率好。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://jjcoder.top/]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>原型模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty - 粘包和半包(下)]]></title>
    <url>%2F2019%2F10%2F24%2FNetty%20-%20%E7%B2%98%E5%8C%85%E5%92%8C%E5%8D%8A%E5%8C%85(%E4%B8%8B)%2F</url>
    <content type="text"><![CDATA[上一篇介绍了粘包和半包及其通用的解决方案，今天重点来看一下 Netty 是如何实现封装成帧(Framing)方案的。 解码核心流程之前介绍过三种解码器FixedLengthFrameDecoder、DelimiterBasedFrameDecoder、LengthFieldBasedFrameDecoder，它们都继承自ByteToMessageDecoder，而ByteToMessageDecoder继承自ChannelInboundHandlerAdapter，其核心方法为channelRead。因此，我们来看看ByteToMessageDecoder的channelRead方法：123456789101112131415161718192021@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof ByteBuf) &#123; CodecOutputList out = CodecOutputList.newInstance(); try &#123; // 将传入的消息转化为data ByteBuf data = (ByteBuf) msg; // 最终实现的目标是将数据全部放进cumulation中 first = cumulation == null; // 第一笔数据直接放入 if (first) &#123; cumulation = data; &#125; else &#123; // 不是第一笔数据就进行追加 cumulation = cumulator.cumulate(ctx.alloc(), cumulation, data); &#125; // 解码 callDecode(ctx, cumulation, out); &#125; // 以下代码省略，因为不属于解码过程&#125; 再来看看callDecode方法：123456789101112131415protected void callDecode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) &#123; try &#123; while (in.isReadable()) &#123; int outSize = out.size(); if (outSize &gt; 0) &#123; // 以下代码省略，因为初始状态时，outSize 只可能是0，不可能进入这里 &#125; int oldInputLength = in.readableBytes(); // 在进行 decode 时，不执行handler的remove操作。 // 只有当 decode 执行完之后，开始清理数据。 decodeRemovalReentryProtection(ctx, in, out); // 省略以下代码，因为后面的内容也不是解码的过程 再来看看decodeRemovalReentryProtection方法：12345678910111213141516171819final void decodeRemovalReentryProtection(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; // 设置当前状态为正在解码 decodeState = STATE_CALLING_CHILD_DECODE; try &#123; // 解码 decode(ctx, in, out); &#125; finally &#123; // 执行hander的remove操作 boolean removePending = decodeState == STATE_HANDLER_REMOVED_PENDING; decodeState = STATE_INIT; if (removePending) &#123; handlerRemoved(ctx); &#125; &#125;&#125;// 子类都重写了该方法，每种实现都会有自己特殊的解码方式protected abstract void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception; 从上面的过程可以总结出，在解码之前，需要先将数据写入cumulation，当解码结束后，需要通过 handler 进行移除。 具体解码过程刚刚说到decode方法在子类中都有实现，那针对我们说的三种解码方式，一一看其实现。 FixedLengthFrameDecoder其源码为：1234567891011121314151617@Overrideprotected final void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; Object decoded = decode(ctx, in); if (decoded != null) &#123; out.add(decoded); &#125;&#125;protected Object decode( @SuppressWarnings("UnusedParameters") ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; // 收集到的数据是否小于固定长度，小于就代表无法解析 if (in.readableBytes() &lt; frameLength) &#123; return null; &#125; else &#123; return in.readRetainedSlice(frameLength); &#125;&#125; 就和这个类的名字一样简单，就是固定长度进行解码，因此，在设置该解码器的时候，需要在构造方式里传入frameLength。 DelimiterBasedFrameDecoder其源码为：12345678910111213141516171819202122232425@Overrideprotected final void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; Object decoded = decode(ctx, in); if (decoded != null) &#123; out.add(decoded); &#125;&#125;protected Object decode(ChannelHandlerContext ctx, ByteBuf buffer) throws Exception &#123; // 当前的分割符是否是换行分割符(\n或者\r\n) if (lineBasedDecoder != null) &#123; return lineBasedDecoder.decode(ctx, buffer); &#125; // Try all delimiters and choose the delimiter which yields the shortest frame. int minFrameLength = Integer.MAX_VALUE; ByteBuf minDelim = null; // 其他分割符进行一次切分 for (ByteBuf delim: delimiters) &#123; int frameLength = indexOf(buffer, delim); if (frameLength &gt;= 0 &amp;&amp; frameLength &lt; minFrameLength) &#123; minFrameLength = frameLength; minDelim = delim; &#125; &#125; // 以下代码省略 根据它的名字可以知道，分隔符才是它的核心。它将分割符分成两类，只有换行分割符(\n或者\r\n)和其他。因此，需要注意的是，你可以定义多种分割符，它都是支持的。 LengthFieldBasedFrameDecoder该类比较复杂，如果直接看方法容易把自己看混乱，因此我准备结合类上的解释，先看看其私有变量。 2 bytes length field at offset 1 in the middle of 4 bytes header, strip the first header field and the length field, the length field represents the length of the whole message Let’s give another twist to the previous example. The only difference from the previous example is that the length field represents the length of the whole message instead of the message body, just like the third example. We have to count the length of HDR1 and Length into lengthAdjustment. Please note that we don’t need to take the length of HDR2 into account because the length field already includes the whole header length. 12345* BEFORE DECODE (16 bytes) AFTER DECODE (13 bytes)* +------+--------+------+----------------+ +------+----------------+* | HDR1 | Length | HDR2 | Actual Content |-----&gt;| HDR2 | Actual Content |* | 0xCA | 0x0010 | 0xFE | &quot;HELLO, WORLD&quot; | | 0xFE | &quot;HELLO, WORLD&quot; |* +------+--------+------+----------------+ +------+----------------+ lengthFieldOffset : 该字段代表 Length 字段是从第几个字节开始的。上面的例子里，Length 字段是从第1个字节开始（HDR1 是第0个字节），因此该值即为0。 lengthFieldLength : 该字段代表 Length 字段所占用的字节数。上面的例子里，Length 字段占用2个字节，因此该值为2。 lengthAdjustment : 该字段代表 Length 字段结束位置到真正的内容开始位置的距离。上面例子里，因为 Length 字段的含义是整个消息（包括 HDR1、Length、HDR2、Actual Content，一般 Length 指的只是 Actual Content），所以 Length 末尾到真正的内容开始位置（HDR1的开始处），相当于减少3个字节，所以是-3。 initialBytesToStrip : 展示时需要从 Length 字段末尾开始跳过几个字节。上面例子里，因为真正的内容是从 HDR1 开始的，最终展示的内容是从 HDR2 开始的，所以中间差了3个字节，所以该值是3。 该类的解码方法比较复杂，有兴趣的同学可以试着自己分析一下。 总结这一篇主要是结合 Netty 里的源代码讲解了 Netty 中封装成帧(Framing)的三种方式，相信你一定有了不一样的理解。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
        <tag>粘包</tag>
        <tag>半包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty - 粘包和半包(上)]]></title>
    <url>%2F2019%2F10%2F23%2FNetty%20-%20%E7%B2%98%E5%8C%85%E5%92%8C%E5%8D%8A%E5%8C%85(%E4%B8%8A)%2F</url>
    <content type="text"><![CDATA[在网络传输中，粘包和半包应该是最长出现的问题，作为 Java 中最常使用的 NIO 网络框架 Netty，它又是如何解决的呢？今天就让我们来看看。 定义TCP 传输中，客户端发送数据，实际是把数据写入到了 TCP 的缓存中，粘包和半包也就会在此时产生。 客户端给服务端发送了两条消息ABC和DEF，服务端这边的接收会有多少种情况呢？有可能是一次性收到了所有的消息ABCDEF，有可能是收到了三条消息AB、CD、EF。 上面所说的一次性收到了所有的消息ABCDEF，类似于粘包。如果客户端发送的包的大小比 TCP 的缓存容量小，并且 TCP 缓存可以存放多个包，那么客户端和服务端的一次通信就可能传递了多个包，这时候服务端从 TCP 缓存就可能一下读取了多个包，这种现象就叫粘包。 上面说的后面那种收到了三条消息AB、CD、EF，类似于半包。如果客户端发送的包的大小比 TCP 的缓存容量大，那么这个数据包就会被分成多个包，通过 Socket 多次发送到服务端，服务端第一次从接受缓存里面获取的数据，实际是整个包的一部分，这时候就产生了半包(半包不是说只收到了全包的一半，是说收到了全包的一部分)。 产生原因其实从上面的定义，我们就可以大概知道产生的原因了。 粘包的主要原因： 发送方每次写入数据 &lt; 套接字(Socket)缓冲区大小 接收方读取套接字(Socket)缓冲区数据不够及时 半包的主要原因： 发送方每次写入数据 &gt; 套接字(Socket)缓冲区大小 发送的数据大于协议的 MTU (Maximum Transmission Unit，最大传输单元)，因此必须拆包 其实我们可以换个角度看待问题： 从收发的角度看，便是一个发送可能被多次接收，多个发送可能被一次接收。 从传输的角度看，便是一个发送可能占用多个传输包，多个发送可能公用一个传输包。 根本原因，其实是 TCP 是流式协议，消息无边界。 (PS ： UDP 虽然也可以一次传输多个包或者多次传输一个包，但每个消息都是有边界的，因此不会有粘包和半包问题。) 解决方法就像上面说的，UDP 之所以不会产生粘包和半包问题，主要是因为消息有边界，因此，我们也可以采取类似的思路。 改成短连接将 TCP 连接改成短连接，一个请求一个短连接。这样的话，建立连接到释放连接之间的消息即为传输的信息，消息也就产生了边界。 这样的方法就是十分简单，不需要在我们的应用中做过多修改。但缺点也就很明显了，效率低下，TCP 连接和断开都会涉及三次握手以及四次握手，每个消息都会涉及这些过程，十分浪费性能。 因此，并不推介这种方式。 封装成帧封装成帧(Framing)，也就是原本发送消息的单位是缓冲大小，现在换成了帧，这样我们就可以自定义边界了。一般有4种方式： 固定长度这种方式下，消息边界也就是固定长度即可。 优点就是实现很简单，缺点就是空间有极大的浪费，如果传递的消息中大部分都比较短，这样就会有很多空间是浪费的。 因此，这种方式一般也是不推介的。 分隔符这种方式下，消息边界也就是分隔符本身。 优点是空间不再浪费，实现也比较简单。缺点是当内容本身出现分割符时需要转义，所以无论是发送还是接受，都需要进行整个内容的扫描。 因此，这种方式效率也不是很高，但可以尝试使用。 专门的 length 字段这种方式，就有点类似 Http 请求中的 Content-Length，有一个专门的字段存储消息的长度。作为服务端，接受消息时，先解析固定长度的字段（length字段）获取消息总长度，然后读取后续内容。 优点是精确定位用户数据，内容也不用转义。缺点是长度理论上有限制，需要提前限制可能的最大长度从而定义长度占用字节数。 因次，十分推介用这种方式。 其他方式其他方式就各不相同了，比如 JSON 可以看成是使用{}是否成对。这些优缺点就需要大家在各自的场景中进行衡量了。 Netty 中的实现Netty 支持上文所讲的封装成帧(Framing)中的前三种方式，简单介绍下： 方式 解码 编码 固定长度 FixedLengthFrameDecoder 简单 分割符 DelimiterBasedFrameDecoder 简答 专门的 length 字段 LengthFieldBasedFrameDecoder LengthFieldPrepender 总结今天主要介绍了粘包和半包问题、解决思路和 Netty 中的支持，我会在下一篇文章里重点讲述 Netty 中的具体实现，敬请期待。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
        <tag>粘包</tag>
        <tag>半包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面试 - ThreadLocal 原理]]></title>
    <url>%2F2019%2F10%2F23%2FJava%20%E9%9D%A2%E8%AF%95%20-%20ThreadLocal%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[关于 ThreadLocal，我们经常用它来解决多线程并发问题，那它究竟是如何做到的？今天就让我们来好好看一下。 从源码入手首先，让我们看看 ThreadLocal 类中的介绍： This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). Each thread holds an implicit reference to its copy of a thread-local variable as long as the thread is alive and the ThreadLocal instance is accessible; after a thread goes away, all of its copies of thread-local instances are subject to garbage collection (unless other references to these copies exist). 按照文中所述，ThreadLocal 提供的是线程本地变量，每个线程都有一份单独的副本，经常使用的方式是私有静态变量。关键在于下一段，线程存活，ThreadLocal 实例就可以被访问，线程消失，就会被垃圾回收。 get()方法看到这儿，有没有想起上一篇内容所说的引用类型，有可能是软引用或者弱引用，具体是什么呢？还是来看看代码：12345678910111213141516171819public T get() &#123; // 获取当前线程 Thread t = Thread.currentThread(); // 获取线程里的map ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 上面展示的是 ThreadLocal 中的get()方法，关键的 map 是在 Thread 类中的threadLocals变量，让我们继续看看 ThreadLocalMap 的源代码： 12345678910111213141516ThreadLocal.ThreadLocalMap threadLocals = null;static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; // 使用ThreadLocal作为key，并且是弱引用 super(k); value = v; &#125; &#125; // 省略代码&#125; 根据上一篇文章所述，如果一个对象只有弱引用，那么当下一次 GC 进行时，该对象就会被回收。那么让我们整理一下： ThreadLocalMap 的 Entry 对 ThreadLocal 的引用为弱引用。 ThreadLocal 本身并不存储值，具体的 value 依旧在各个线程中。因此你可以把 ThreadLocal 看成一个工具类。 但需要注意的是，Entry 中，只有key是弱引用，但 value 依旧是强引用。那会不会出现 key 被垃圾回收后，这个 map 的 key 为 null，但 value 依旧存在的情况呢？ set()方法确实是有可能的，但 JDK 本身也做了优化，可以看看 ThreadLocalMap 的 set()方法：1234567891011121314151617181920212223242526272829303132private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 调用 set()的时候，ThreadLocalMap 检查到 key 为 null 的 entry 时，会将 value 也设置为 null，这样 value 之前对应的实例也可以被回收。 使用场景简单使用先让我们看一个简单的例子：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class ThreadLocalSimpleDemo &#123; public static void main(String[] args) &#123; int threads = 3; InnerClass innerClass = new InnerClass(); for (int i = 1; i &lt;= threads; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 4; j++) &#123; innerClass.add(String.valueOf(j)); innerClass.print(); &#125; innerClass.set("hello world"); &#125;, "thread - " + i).start(); &#125; &#125; private static class InnerClass &#123; /** * 添加 */ public void add(String newStr) &#123; StringBuilder str = Counter.counter.get(); Counter.counter.set(str.append(newStr)); &#125; /** * 打印 */ public void print() &#123; System.out.printf( "Thread name:%s , ThreadLocal hashcode:%s, Instance hashcode:%s, Value:%s\n", Thread.currentThread().getName(), Counter.counter.hashCode(), Counter.counter.get().hashCode(), Counter.counter.get().toString() ); &#125; /** * 赋值 */ public void set(String words) &#123; Counter.counter.set(new StringBuilder(words)); System.out.printf( "Set, Thread name:%s , ThreadLocal hashcode:%s, Instance hashcode:%s, Value:%s\n", Thread.currentThread().getName(), Counter.counter.hashCode(), Counter.counter.get().hashCode(), Counter.counter.get().toString() ); &#125; &#125; private static class Counter &#123; /** * 初始化时是一个空的StringBuilder对象 */ private static ThreadLocal&lt;StringBuilder&gt; counter = ThreadLocal.withInitial(StringBuilder::new); &#125;&#125; 其打印结果为：123456789101112131415Thread name:thread - 3 , ThreadLocal hashcode:310471657, Instance hashcode:640658548, Value:0Thread name:thread - 2 , ThreadLocal hashcode:310471657, Instance hashcode:126253473, Value:0Thread name:thread - 2 , ThreadLocal hashcode:310471657, Instance hashcode:126253473, Value:01Thread name:thread - 2 , ThreadLocal hashcode:310471657, Instance hashcode:126253473, Value:012Thread name:thread - 2 , ThreadLocal hashcode:310471657, Instance hashcode:126253473, Value:0123Thread name:thread - 1 , ThreadLocal hashcode:310471657, Instance hashcode:829132711, Value:0Thread name:thread - 1 , ThreadLocal hashcode:310471657, Instance hashcode:829132711, Value:01Thread name:thread - 1 , ThreadLocal hashcode:310471657, Instance hashcode:829132711, Value:012Thread name:thread - 1 , ThreadLocal hashcode:310471657, Instance hashcode:829132711, Value:0123Set, Thread name:thread - 1 , ThreadLocal hashcode:310471657, Instance hashcode:820066274, Value:hello worldThread name:thread - 3 , ThreadLocal hashcode:310471657, Instance hashcode:640658548, Value:01Thread name:thread - 3 , ThreadLocal hashcode:310471657, Instance hashcode:640658548, Value:012Set, Thread name:thread - 2 , ThreadLocal hashcode:310471657, Instance hashcode:155293473, Value:hello worldThread name:thread - 3 , ThreadLocal hashcode:310471657, Instance hashcode:640658548, Value:0123Set, Thread name:thread - 3 , ThreadLocal hashcode:310471657, Instance hashcode:1804272849, Value:hello world 可以看出，我们在使用 ThreadLocal 时，用的是同一个对象，但各个线程对应的实例是不一样的。而在调用 set() 方法后，对应的实例会被替换。 Session对于 Java Web 应用而言，Session 保存了很多信息。很多时候需要通过 Session 获取信息，有些时候又需要修改 Session 的信息。一方面，需要保证每个线程有自己单独的 Session 实例。另一方面，由于很多地方都需要操作 Session，存在多方法共享 Session 的需求。使用 ThreadLocal 进行实现：123456789101112131415161718192021222324252627282930313233public class SessionHandler &#123; public static ThreadLocal&lt;Session&gt; session = ThreadLocal.&lt;Session&gt;withInitial(() -&gt; new Session()); @Data public static class Session &#123; private String id; private String user; private String status; &#125; public String getUser() &#123; return session.get().getUser(); &#125; public String getStatus() &#123; return session.get().getStatus(); &#125; public void setStatus(String status) &#123; session.get().setStatus(status); &#125; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; SessionHandler handler = new SessionHandler(); handler.getStatus(); handler.getUser(); handler.setStatus("close"); handler.getStatus(); &#125;).start(); &#125;&#125; 总结ThreadLocal 使用起来虽然简单，但考虑到其设计确实很精巧，值得了解一下。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>弱引用</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面试 - 四种引用类型]]></title>
    <url>%2F2019%2F10%2F22%2FJava%20%E9%9D%A2%E8%AF%95%20-%20%E5%9B%9B%E7%A7%8D%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[之前我们提到过 GC，但当 Java 中引用的对象越来越多，会导致内存空间不足，最终会产生错误 OutOfMemoryError，并让应用程序终止。那为什么 GC 在此时不能多收集一些对象呢？这就和今天说的引用类型有关了。 首先，从 JDK1.2 开始，对象的引用被划分为4种级别，从而使程序能更加灵活地控制对象的生命周期。这4种级别由高到低依次为：强引用、软引用、弱引用和虚引用。 强引用强引用(Strong Reference)是使用最普遍的引用。如果一个对象具有强引用，那么它永远不会被 GC。例如：1Object strongReference = new Object(); 当内存空间不足时，JVM 宁愿抛出OutOfMemoryError，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。 如果强引用对象不使用时，需要弱化从而可以被 GC，例如ArrayList中的clear()方法：12345678910111213/** * Removes all of the elements from this list. The list will * be empty after this call returns. */public void clear() &#123; modCount++; // clear to let GC do its work for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0;&#125; 显式地设置强引用对象为null，或让其超出对象的生命周期范围，则垃圾回收器认为该对象不存在引用，就会回收这个对象。具体什么时候收集这要取决于具体的垃圾回收器。 软引用如果一个对象只具有软引用(Soft Reference)，当内存空间充足时，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。让我们来看一个例子具体了解一下：123String str = new String("abc");SoftReference&lt;String&gt; softReference = new SoftReference&lt;&gt;(str);String result = softReference.get(); 让我们来看一下get()：12345678public T get() &#123; T o = super.get(); // timestamp代表上一次软引用上一次被使用的时间(初始化、get()) // clock代表上一次GC的时间 if (o != null &amp;&amp; this.timestamp != clock) this.timestamp = clock; return o;&#125; 因此，软引用在被垃圾回收时，也遵循LRU法则，优先回收最近最少被使用的对象进行回收。 软引用的使用场景多是内存敏感的高速缓存。具体来说，就是我们希望将数据存放到缓存中，这样可以快速进行读取。但是，当 JVM 中内存不够用时，我们又不希望缓存数据会占用到 JVM 的内存。例如配合ReferenceQueue，如果软引用所引用对象被垃圾回收，JVM 就会把这个软引用加入到与之关联的引用队列中：123456789101112ReferenceQueue&lt;String&gt; referenceQueue = new ReferenceQueue&lt;&gt;();String str = new String("abc");SoftReference&lt;String&gt; softReference = new SoftReference&lt;&gt;(str, referenceQueue);str = null;// Notify GCSystem.gc();System.out.println(softReference.get()); // abcReference&lt;? extends String&gt; reference = referenceQueue.poll();System.out.println(reference); //null 但是需要注意的时，如果使用软引用缓存，有可能导致Full GC增多。 弱引用如果一个对象只具有弱引用(Weak Reference)，其生命周期相比于软引用更加短暂。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会对它进行回收。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。其使用为：123String str = new String("abc");WeakReference&lt;String&gt; weakReference = new WeakReference&lt;&gt;(str);str = weakReference.get(); 讲到弱引用，就不得不提到WeakHashMap。和HashMap相比，当我们给 JVM 分配的内存不足的时候，HashMap 宁可抛出 OutOfMemoryError 异常，也不会回收其相应的没有被引用的对象，而 WeakHashMap 则会回收存储在其中但有被引用的对象。 WeakHashMap 通过将一些没有被引用的键的值赋值为 null ，这样的话就会告知GC去回收这些存储的值了。假如我们特地传入 key 为 null 的键，WeakHashMap 会将键设置为特殊的 Oject，源码为：123456789101112131415161718192021222324252627282930313233343536public V put(K key, V value) &#123; // key会被重新赋值 Object k = maskNull(key); int h = hash(k); Entry&lt;K,V&gt;[] tab = getTable(); int i = indexFor(h, tab.length); for (Entry&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if (h == e.hash &amp;&amp; eq(k, e.get())) &#123; V oldValue = e.value; if (value != oldValue) e.value = value; return oldValue; &#125; &#125; modCount++; Entry&lt;K,V&gt; e = tab[i]; tab[i] = new Entry&lt;&gt;(k, value, queue, h, e); if (++size &gt;= threshold) resize(tab.length * 2); return null;&#125;/** * Value representing null keys inside tables. * 特殊的key */private static final Object NULL_KEY = new Object();/** * Use NULL_KEY for key if it is null. */private static Object maskNull(Object key) &#123; return (key == null) ? NULL_KEY : key;&#125; 虚引用虚引用(PhantomReference),顾名思义，就是形同虚设。与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。 虚引用主要用来跟踪对象被垃圾回收器回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列(ReferenceQueue)联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 例如：1234String str = new String("abc");ReferenceQueue queue = new ReferenceQueue();// 创建虚引用，要求必须与一个引用队列关联PhantomReference pr = new PhantomReference(str, queue); 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要进行垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动，也可以理解为一种回调方法。 总结Java 中4种引用的级别和强度由高到低依次为：强引用 -&gt; 软引用 -&gt; 弱引用 -&gt; 虚引用 通过表格，说明其特性： 引用类型 被垃圾回收的时间 使用场景 生存时间 强引用 从来不会 对象的一般状态 JVM停止运行时 软引用 内存不足时 对象缓存 内存不足时 弱引用 正常垃圾回收时 对象缓存 垃圾回收后终止 虚引用 正常垃圾回收时 跟踪对象的垃圾回收 垃圾回收后终止 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>强引用</tag>
        <tag>软引用</tag>
        <tag>弱引用</tag>
        <tag>虚引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面试 - 垃圾回收（下）]]></title>
    <url>%2F2019%2F10%2F21%2FJava%20%E9%9D%A2%E8%AF%95%20-%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[接着上一篇，介绍完了 JVM 中识别需要回收的垃圾对象之后，这一篇我们来说说 JVM 是如何进行垃圾回收。 首先要在这里介绍一下80/20 法则: 约仅有20%的变因操纵着80%的局面。也就是说：所有变量中，最重要的仅有20%，虽然剩余的80%占了多数，控制的范围却远低于“关键的少数”。 Java 对象的生命周期也满足也这样的定律，即大部分的 Java 对象只存活一小段时间，而存活下来的小部分 Java 对象则会存活很长一段时间。 因此，这也就造就了 JVM 中分代回收的思想。简单来说，就是将堆空间划分为两代，分别叫做新生代和老年代。新生代用来存储新建的对象。当对象存活时间够长时，则将其移动到老年代。 这样也就可以让 JVM 给不同代使用不同的回收算法。 对于新生代，我们猜测大部分的 Java 对象只存活一小段时间，那么便可以频繁地采用耗时较短的垃圾回收算法，让大部分的垃圾都能够在新生代被回收掉。 对于老年代，我们猜测大部分的垃圾已经在新生代中被回收了，而在老年代中的对象有大概率会继续存活。当真正触发针对老年代的回收时，则代表这个假设出错了，或者堆的空间已经耗尽了。此时，JVM 往往需要做一次全堆扫描，耗时也将不计成本。（当然，现代的垃圾回收器都在并发收集的道路上发展，来避免这种全堆扫描的情况。） 那么，我们先来看看 JVM 中堆究竟是如何划分的。 堆划分按照上文所述，JVM 将堆划分为新生代和老年代，其中，新生代又被划分为 Eden 区，以及两个大小相同的 Survivor 区。 通常来说，当我们调用 new 指令时，它会在 Eden 区中划出一块作为存储对象的内存。由于堆空间是线程共享的，因此直接在这里边划空间是需要进行同步的。否则，将有可能出现两个对象共用一段内存的事故。 JVM 的解决方法是为每个线程预先申请一段连续的堆空间，并且只允许每个线程在自己申请过的堆空间中创建对象，如果申请的堆空间被用完了，那么再继续申请即可，这也就是 TLAB（Thread Local Allocation Buffer，对应虚拟机参数 -XX:+UseTLAB，默认开启）。 此时，如果线程操作涉及到加锁，则该线程需要维护两个指针（实际上可能更多，但重要也就两个），一个指向 TLAB 中空余内存的起始位置，一个则指向 TLAB 末尾。 接下来的 new 指令，便可以直接通过指针加法（bump the pointer）来实现，即把指向空余内存位置的指针加上所请求的字节数。 如果加法后空余内存指针的值仍小于或等于指向末尾的指针，则代表分配成功。否则，TLAB 已经没有足够的空间来满足本次新建操作。这个时候，便需要当前线程重新申请新的 TLAB。 那有没有可能出现申请不到的情况呢？有的，这个时候就会触发Minor GC了。 Minor GC所谓 Minor GC，就是指： 当 Eden 区的空间耗尽时，JVM 会进行一次 Minor GC，来收集新生代的垃圾。存活下来的对象，则会被送到 Survivor 区。 上文提到，新生代共有两个 Survivor 区，我们分别用 from 和 to 来指代。其中 to 指向的 Survivior 区是空的。 当发生 Minor GC 时，Eden 区和 from 指向的 Survivor 区中的存活对象会被复制到 to 指向的 Survivor 区中，然后交换 from 和 to 指针，以保证下一次 Minor GC 时，to 指向的 Survivor 区还是空的。 JVM 会记录 Survivor 区中每个对象一共被来回复制了几次。如果一个对象被复制的次数为 15（对应虚拟机参数 -XX:+MaxTenuringThreshold），那么该对象将被晋升（promote）至老年代。 另外，如果单个 Survivor 区已经被占用了 50%（对应虚拟机参数 -XX:TargetSurvivorRatio），那么较高复制次数的对象也会被晋升至老年代。 总而言之，当发生 Minor GC 时，我们应用了标记 - 复制算法，将 Survivor 区中的老存活对象晋升到老年代，然后将剩下的存活对象和 Eden 区的存活对象复制到另一个 Survivor 区中。理想情况下，Eden 区中的对象基本都死亡了，那么需要复制的数据将非常少，因此采用这种标记 - 复制算法的效果极好。 Minor GC 的另外一个好处是不用对整个堆进行垃圾回收。但是，它却有一个问题，那就是老年代中的对象可能引用新生代的对象。也就是说，在标记存活对象的时候，我们需要扫描老年代中的对象。如果该对象拥有对新生代对象的引用，那么这个引用也会被作为 GC Roots。这样一来，岂不是又做了一次全堆扫描呢？ 为了避免扫描全堆，JVM 引入了名为卡表的技术，大致地标出可能存在老年代到新生代引用的内存区域。有兴趣的朋友可以去详细了解一下，这里限于篇幅，就不具体介绍了。 Full GC那什么时候会发生Full GC呢？针对不同的垃圾收集器，Full GC 的触发条件可能不都一样。按 HotSpot VM 的 serial GC 的实现来看，触发条件是: 当准备要触发一次 Minor GC 时，如果发现统计数据说之前 Minor GC 的平均晋升大小比目前老年代剩余的空间大，则不会触发 Minor GC 而是转为触发 Full GC。 因为 HotSpot VM 的 GC 里，除了垃圾回收器 CMS 能单独收集老年代之外，其他的 GC 都会同时收集整个堆，所以不需要事先准备一次单独的 Minor GC。 垃圾回收基础的回收方式有三种：清除、压缩、复制，接下来让我们来一一了解一下。 清除所谓清除，就是把死亡对象所占据的内存标记为空闲内存，并记录在一个空闲列表之中。当需要新建对象时，内存管理模块便会从该空闲列表中寻找空闲内存，并划分给新建的对象。 其原理十分简单，但是有两个缺点： 会造成内存碎片。由于 JVM 的堆中对象必须是连续分布的，因此可能出现总空闲内存足够，但是无法分配的极端情况。 分配效率较低。如果是一块连续的内存空间，那么我们可以通过指针加法（pointer bumping）来做分配。而对于空闲列表，JVM 则需要逐个访问空闲列表中的项，来查找能够放入新建对象的空闲内存。 压缩所谓压缩，就是把存活的对象聚集到内存区域的起始位置，从而留下一段连续的内存空间。 这种做法能够解决内存碎片化的问题，但代价是压缩算法的性能开销，因此分配效率问题依旧没有解决。 复制所谓复制，就是把内存区域平均分为两块，分别用两个指针 from 和 to 来维护，并且只是用 from 指针指向的内存区域来分配内存。当发生垃圾回收时，便把存活的对象复制到 to 指针所指向的内存区域中，并且交换 from 指针和 to 指针的内容。 这种回收方式同样能够解决内存碎片化的问题，但是它的缺点也极其明显，即堆空间的使用效率极其低下。 具体垃圾收集器针对新生代的垃圾回收器共有三个：Serial ，Parallel Scavenge 和 Parallel New。这三个采用的都是标记 - 复制算法。 其中，Serial 是一个单线程的，Parallel New 可以看成是 Serial 的多线程版本，Parallel Scavenge 和 Parallel New 类似，但更加注重吞吐率。此外，Parallel Scavenge 不能与 CMS 一起使用。 针对老年代的垃圾回收器也有三个：Serial Old ，Parallel Old 和 CMS。 Serial Old 和 Parallel Old 都是标记 - 压缩算法。同样，前者是单线程的，而后者可以看成前者的多线程版本。 CMS 采用的是标记 - 清除算法，并且是并发的。除了少数几个操作需要 STW(Stop the world) 之外，它可以在应用程序运行过程中进行垃圾回收。在并发收集失败的情况下，JVM 会使用其他两个压缩型垃圾回收器进行一次垃圾回收。由于 G1 的出现，CMS 在 Java 9 中已被废弃。 G1（Garbage First）是一个横跨新生代和老年代的垃圾回收器。实际上，它已经打乱了前面所说的堆结构，直接将堆分成极其多个区域。每个区域都可以充当 Eden 区、Survivor 区或者老年代中的一个。它采用的是标记 - 压缩算法，而且和 CMS 一样都能够在应用程序运行过程中并发地进行垃圾回收。 G1 能够针对每个细分的区域来进行垃圾回收。在选择进行垃圾回收的区域时，它会优先回收死亡对象较多的区域。这也是 G1 名字的由来。 总结这篇文章主要讲述的是 JVM 中具体的垃圾回收方法，从对象的生存规律，引出回收方法，结合多线程的特点，逐步优化，最终产生了我们现在所能知道各种垃圾收集器。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面试 - 垃圾回收（上）]]></title>
    <url>%2F2019%2F10%2F20%2FJava%20%E9%9D%A2%E8%AF%95%20-%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Java 中的垃圾回收，常常是由 JVM 帮我们做好的。虽然这节省了大家很多的学习的成本，提高了项目的执行效率，但是当项目变得越来越复杂，用户量越来越大时，还是需要我们懂得垃圾回收机制，这样也能进行更深一步的优化。 辨别对象存亡垃圾回收( Garbage Collection，以下简称 GC )，从字面上理解，就是将已经分配出去的，但却不再使用的内存回收回来，以便能够再次分配。 在 JVM 中，垃圾就是指的死亡对象所占据的堆空间( GC 是发生在堆空间中)，那么我们如果辨别一个对象是否死亡呢？JVM 使用的是引用计数法和可达性分析。 引用计数法引用计数法( Reference Counting)，是为每个对象添加一个引用计数器，用来统计引用该对象的个数。一旦某个对象的引用计数器为0，则说明该对象已经死亡，便可以被回收了。 其具体实现为： 如果有一个引用，被赋值为某一对象，那么将该对象的引用计数器 +1。 如果一个指向某一对象的引用，被赋值为其他值，那么将该对象的引用计数器 -1。 也就是说，我们需要截获所有的引用更新操作，并且相应地增减目标对象的引用计数器。 看似很简单的实现，其实里面有不少缺陷： 需要额外的空间来存储计数器。 计数器的更新操作十分繁琐。 最重要的：无法处理循环引用对象。 针对第3点，举个例子特别说明一下： 假设对象 a 与 b 相互引用，除此之外没有其他引用指向他们。在这种情况下，a 和 b 实际上已经死了。 但由于它们的引用计数器皆不为0（因为相互引用，两者均为1），在引用计数法的计算中，这两个对象还活着。因此，这些循环引用对象所占据的空间将不可回收，从而造成了内存泄露。 可达性分析可达性分析( Reachability Analysis )，是目前 JVM 主要采取的判定对象死亡的方法。实质在于将一系列GC Roots作为初始的存活对象合集（live set），然后从该合集出发，探索所有能够被该集合引用到的对象，并将其加入到该集合中，这个过程我们也称之为标记（mark）。最终，未被探索到的对象便是死亡的，是可以回收的。 那么什么是GC Roots呢？我们可以暂时理解为由堆外指向堆内的引用，一般而言，GC Roots 包括（但不限于）如下几种： Java 方法栈桢中的局部变量 已加载类的静态变量 JNI handles 已启动且未停止的 Java 线程 之前我们说引用计数法会有循环引用的问题，可达性分析就不会了。举例来说，即便对象 a 和 b 相互引用，只要从 GC Roots 出发无法到达 a 或者 b，那么可达性分析便会认为它们已经死亡。 那可达性分析有没有什么缺点呢？有的，在多线程环境下，其他线程可能会更新已经分析过的对象中的引用，从而造成误报（将引用设置为 null）或者漏报（将引用设置为未被访问过的对象）。 误报并没有什么伤害，JVM 至多损失了部分垃圾回收的机会。漏报则比较麻烦，因为垃圾回收器可能回收事实上仍被引用的对象内存。一旦从原引用访问已经被回收了的对象，则很有可能会直接导致 JVM 崩溃。 STW既然可达性分析在多线程下有缺点，那 JVM 是如何解决的呢？答案便是 Stop-the-world(以下简称JWT)，停止了其他非垃圾回收线程的工作直到完成垃圾回收。这也就造成了垃圾回收所谓的暂停时间（GC pause）。 那 SWT 是如何实现的呢？当 JVM 收到 SWT 请求后，它会等待所有的线程都到达安全点（Safe Point），才允许请求 SWT 的线程进行独占的工作。 那什么又叫安全点呢？安全点是 JVM 能找到一个稳定的执行状态，在这个执行状态下，JVM 的堆栈不会发生变化。 这么一来，垃圾回收器便能够“安全”地执行可达性分析，所有存活的对象也都可以成功被标记，那么之后就可以将死亡的对象进行垃圾回收了。 总结以上便是发现死亡对象的过程，这也为之后的垃圾回收进行铺垫，具体的垃圾回收过程，我会在下一篇文章中讲述，敬请期待。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面试-即时编译( JIT )]]></title>
    <url>%2F2019%2F10%2F14%2FJava%20%E9%9D%A2%E8%AF%95-%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91(%20JIT%20)%20%2F</url>
    <content type="text"><![CDATA[当我们在写代码时，一个方法内部的行数自然是越少越好，这样逻辑清晰、方便阅读，其实好处远不止如此，通过即时编译，甚至可以提高执行时的性能，今天就让我们好好来了解一下其中的原理。 简介当 JVM 的初始化完成后，类在调用执行过程中，执行引擎会把字节码转为机器码，然后在操作系统中才能执行。在字节码转换为机器码的过程中，虚拟机中还存在着一道编译，那就是即时编译。 最初，JVM 中的字节码是由解释器（ Interpreter ）完成编译的，当虚拟机发现某个方法或代码块的运行特别频繁的时候，就会把这些代码认定为热点代码。 为了提高热点代码的执行效率，在运行时，即时编译器（JIT，Just In Time）会把这些代码编译成与本地平台相关的机器码，并进行各层次的优化，然后保存到内存中。 分类在 HotSpot 虚拟机中，内置了两种 JIT，分别为C1 编译器和C2 编译器，这两个编译器的编译过程是不一样的。 C1 编译器C1 编译器是一个简单快速的编译器，主要的关注点在于局部性的优化，适用于执行时间较短或对启动性能有要求的程序，也称为Client Compiler，例如，GUI 应用对界面启动速度就有一定要求。 C2 编译器C2 编译器是为长期运行的服务器端应用程序做性能调优的编译器，适用于执行时间较长或对峰值性能有要求的程序，也称为Server Compiler，例如，服务器上长期运行的 Java 应用对稳定运行就有一定的要求。 分层编译在 Java7 之前，需要根据程序的特性来选择对应的 JIT，虚拟机默认采用解释器和其中一个编译器配合工作。 Java7 引入了分层编译，这种方式综合了 C1 的启动性能优势和 C2 的峰值性能优势，我们也可以通过参数 -client或者-server 强制指定虚拟机的即时编译模式。 分层编译将 JVM 的执行状态分为了 5 个层次： 第 0 层：程序解释执行，默认开启性能监控功能（Profiling），如果不开启，可触发第二层编译； 第 1 层：可称为 C1 编译，将字节码编译为本地代码，进行简单、可靠的优化，不开启 Profiling； 第 2 层：也称为 C1 编译，开启 Profiling，仅执行带方法调用次数和循环回边执行次数 profiling 的 C1 编译； 第 3 层：也称为 C1 编译，执行所有带 Profiling 的 C1 编译； 第 4 层：可称为 C2 编译，也是将字节码编译为本地代码，但是会启用一些编译耗时较长的优化，甚至会根据性能监控信息进行一些不可靠的激进优化。 对于 C1 的三种状态，按执行效率从高至低：第 1 层、第 2层、第 3层。 通常情况下，C2 的执行效率比 C1 高出30%以上。 在 Java8 中，默认开启分层编译，-client 和 -server 的设置已经是无效的了。如果只想开启 C2，可以关闭分层编译（-XX:-TieredCompilation），如果只想用 C1，可以在打开分层编译的同时，使用参数：-XX:TieredStopAtLevel=1。 你可以通过 java -version命令行可以直接查看到当前系统使用的编译模式：1234C:\Users\Administrator&gt;java -versionjava version &quot;1.8.0_45&quot;Java(TM) SE Runtime Environment (build 1.8.0_45-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode) mixed mode代表是默认的混合编译模式，除了这种模式外，我们还可以使用-Xint参数强制虚拟机运行于只有解释器的编译模式下，这时 JIT 完全不介入工作；也可以使用参数-Xcomp强制虚拟机运行于只有 JIT 的编译模式下。例如：123456789C:\Users\Administrator&gt;java -Xint -versionjava version &quot;1.8.0_45&quot;Java(TM) SE Runtime Environment (build 1.8.0_45-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, interpreted mode)C:\Users\Administrator&gt;java -Xcomp -versionjava version &quot;1.8.0_45&quot;Java(TM) SE Runtime Environment (build 1.8.0_45-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, compiled mode) 触发标准在 HotSpot 虚拟机中，热点探测是 JIT 的触发标准。 热点探测是基于计数器的热点探测，采用这种方法的虚拟机会为每个方法建立计数器统计方法的执行次数，如果执行次数超过一定的阈值就认为它是“热点方法” 。 虚拟机为每个方法准备了两类计数器：方法调用计数器（Invocation Counter）和回边计数器（Back Edge Counter）。在确定虚拟机运行参数的前提下，这两个计数器都有一个确定的阈值，当计数器超过阈值溢出了，就会触发 JIT 编译。 方法调用计数器方法调用计数器用于统计方法被调用的次数，默认阈值在 C1 模式下是 1500 次，在 C2 模式在是 10000 次，可通过-XX: CompileThreshold来设定；而在分层编译的情况下-XX: CompileThreshold指定的阈值将失效，此时将会根据当前待编译的方法数以及编译线程数来动态调整。当方法计数器和回边计数器之和超过方法计数器阈值时，就会触发 JIT 编译器。 回边计数器回边计数器用于统计一个方法中循环体代码执行的次数，在字节码中遇到控制流向后跳转的指令称为“回边”（Back Edge），该值用于计算是否触发 C1 编译的阈值，在不开启分层编译的情况下，C1 默认为 13995，C2 默认为 10700，可通过-XX: OnStackReplacePercentage=N来设置；而在分层编译的情况下，-XX: OnStackReplacePercentage指定的阈值同样会失效，此时将根据当前待编译的方法数以及编译线程数来动态调整。 建立回边计数器的主要目的是为了触发 OSR（On StackReplacement）编译，即栈上编译。在一些循环周期比较长的代码段中，当循环达到回边计数器阈值时，JVM 会认为这段是热点代码，JIT 编译器就会将这段代码编译成机器语言并缓存，在该循环时间段内，会直接将执行代码替换，执行缓存的机器语言。 优化技术JIT 编译运用了一些经典的编译优化技术来实现代码的优化，即通过一些例行检查优化，可以智能地编译出运行时的最优性能代码。主要有两种：方法内联、逃逸分析。 方法内联调用一个方法通常要经历压栈和出栈。调用方法是将程序执行顺序转移到存储该方法的内存地址，将方法的内容执行完后，再返回到执行该方法前的位置。 这种执行操作要求在执行前保护现场并记忆执行的地址，执行后要恢复现场，并按原来保存的地址继续执行。 因此，方法调用会产生一定的时间和空间方面的开销（其实可以理解为一种上下文切换的精简版）。 那么对于那些方法体代码不是很大，又频繁调用的方法来说，这个时间和空间的消耗会很大。 方法内联的优化行为就是把目标方法的代码复制到发起调用的方法之中，避免发生真实的方法调用。 JVM 会自动识别热点方法，并对它们使用方法内联进行优化。我们可以通过-XX:CompileThreshold来设置热点方法的阈值。但要强调一点，热点方法不一定会被 JVM 做内联优化，如果这个方法体太大了，JVM 将不执行内联操作。而方法体的大小阈值，我们也可以通过参数设置来优化： 经常执行的方法，默认情况下，方法体大小小于 325 字节的都会进行内联，我们可以通过-XX:MaxFreqInlineSize=N来设置大小值； 不是经常执行的方法，默认情况下，方法大小小于 35 字节才会进行内联，我们也可以通过-XX:MaxInlineSize=N来重置大小值。 之后我们就可以通过配置 JVM 参数来查看到方法被内联的情况：123456// 在控制台打印编译过程信息-XX:+PrintCompilation// 解锁对 JVM 进行诊断的选项参数。默认是关闭的，开启后支持一些特定参数对 JVM 进行诊断-XX:+UnlockDiagnosticVMOptions// 将内联方法打印出来-XX:+PrintInlining 热点方法的优化可以有效提高系统性能，一般我们可以通过以下几种方式来提高方法内联： 通过设置 JVM 参数来减小热点阈值或增加方法体阈值，以便更多的方法可以进行内联，但这种方法意味着需要占用更多地内存； 在编程中，避免在一个方法中写大量代码，习惯使用小方法体； 尽量使用 final、private、static 关键字修饰方法，编码方法因为继承，会需要额外的类型检查。 此处就联系到了最开始提出的观点，一个方法中的内容越少，当该方法经常被执行时，则容易进行方法内联，从而优化性能。 逃逸分析逃逸分析（Escape Analysis）是判断一个对象是否被外部方法引用或外部线程访问的分析技术，编译器会根据逃逸分析的结果对代码进行优化。 可以通过JVM参数进行设置：12-XX:+DoEscapeAnalysis 开启逃逸分析（jdk1.8 默认开启）-XX:-DoEscapeAnalysis 关闭逃逸分析 其具体优化方法主要有三种：栈上分配、锁消除、标量替换。 栈上分配在 Java 中默认创建一个对象是在堆中分配内存的，而当堆内存中的对象不再使用时，则需要通过垃圾回收机制回收，这个过程相对分配在栈中的对象的创建和销毁来说，更消耗时间和性能。 这个时候，逃逸分析如果发现一个对象只在方法中使用，就会将对象分配在栈上。 但是，HotSpot 虚拟机目前的实现导致栈上分配实现比较复杂，可以说，在 HotSpot 中暂时没有实现这项优化，所以大家可能暂时无法体会到这种优化（我看的资料显示在 Java8 中还没有实现，如果大家有什么其他的发现，欢迎留言）。 锁消除如果是在单线程环境下，其实完全没有必要使用线程安全的容器，但就算使用了，因为不会有线程竞争，这个时候 JIT 编译会对这个对象的方法锁进行锁消除。例如： 123456public static String getString(String s1, String s2) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb.toString(); &#125; 可以通过JVM参数进行设置：12-XX:+EliminateLocks 开启锁消除（jdk1.8 默认开启）-XX:-EliminateLocks 关闭锁消除 标量替换逃逸分析证明一个对象不会被外部访问，如果这个对象可以被拆分的话，当程序真正执行的时候可能不创建这个对象，而直接创建它的成员变量来代替。将对象拆分后，可以分配对象的成员变量在栈或寄存器上，原本的对象就无需分配内存空间了。这种编译优化就叫做标量替换。 例如：123456public void foo() &#123; TestInfo info = new TestInfo(); info.id = 1; info.count = 99; // to do something&#125; 逃逸分析后，代码会被优化为：12345public void foo() &#123; id = 1; count = 99; // to do something&#125; 可以通过JVM参数进行设置：12-XX:+EliminateAllocations 开启标量替换（jdk1.8 默认开启）-XX:-EliminateAllocations 关闭就可以了 总结今天的内容，由最基本的常识方法内部行数和逻辑需要尽可能简单引出，了解了 JVM 通过即时编译对热点代码进行优化的过程。如果你有什么想法，欢迎在下方留言。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java面试- JVM 内存模型讲解]]></title>
    <url>%2F2019%2F10%2F11%2FJava%E9%9D%A2%E8%AF%95-JVM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[经常有人会有这么一个疑惑，难道 Java 开发就一定要懂得 JVM 的原理吗？我不懂 JVM ，但我照样可以开发。确实，但如果懂得了 JVM ，可以让你在技术的这条路上走的更远一些。 JVM 的重要性首先你应该知道，运行一个 Java 应用程序，我们必须要先安装 JDK 或者 JRE 。这是因为 Java 应用在编译后会变成字节码，然后通过字节码运行在 JVM 中，而 JVM 是 JRE 的核心组成部分。 优点JVM 不仅承担了 Java 字节码的分析（JIT compiler）和执行（Runtime），同时也内置了自动内存分配管理机制。这个机制可以大大降低手动分配回收机制可能带来的内存泄露和内存溢出风险，使 Java 开发人员不需要关注每个对象的内存分配以及回收，从而更专注于业务本身。 缺点这个机制在提升 Java 开发效率的同时，也容易使 Java 开发人员过度依赖于自动化，弱化对内存的管理能力，这样系统就很容易发生 JVM 的堆内存异常、垃圾回收（GC）的不合适以及 GC 次数过于频繁等问题，这些都将直接影响到应用服务的性能。 内存模型JVM 内存模型共分为5个区：堆(Heap)、方法区(Method Area)、程序计数器(Program Counter Register)、虚拟机栈(VM Stack)、本地方法栈(Native Method Stack)。 其中，堆(Heap)、方法区(Method Area)为线程共享，程序计数器(Program Counter Register)、虚拟机栈(VM Stack)、本地方法栈(Native Method Stack)为线程隔离。 堆(Heap)堆是 JVM 内存中最大的一块内存空间，该内存被所有线程共享，几乎所有对象和数组都被分配到了堆内存中。 堆被划分为新生代和老年代，新生代又被进一步划分为 Eden 区和 Survivor 区，最后 Survivor 由 From Survivor 和 To Survivor 组成。 随着 Java 版本的更新，其内容又有了一些新的变化： 在 Java6 版本中，永久代在非堆内存区；到了 Java7 版本，永久代的静态变量和运行时常量池被合并到了堆中；而到了 Java8，永久代被元空间(处于本地内存)取代了。 为什么要用元空间替换永久代呢？ 为了融合 HotSpot JVM 与 JRockit VM，因为 JRockit 没有永久代，所以不需要配置永久代。 永久代内存经常不够用或发生内存溢出（应该是 JVM 中占用内存最大的一块），产生异常 java.lang.OutOfMemoryError: PermGen。在 JDK1.7 版本中，指定的 PermGen 区大小为 8M，由于 PermGen 中类的元数据信息在每次 FullGC 的时候都可能被收集，回收率都偏低，成绩很难令人满意；还有，为 PermGen 分配多大的空间很难确定，PermSize 的大小依赖于很多因素，比如，JVM 加载的 class 总数、常量池的大小和方法的大小等。 看到这儿，自然就想到了 GC 回收算法，不用急，我会在之后的文章中进行讲解，现在还是以 JVM 内存模型为主。 方法区(Method Area)什么是方法区？ 方法区主要是用来存放已被虚拟机加载的类相关信息，包括类信息、常量池(字符串常量池以及所有基本类型都有其相应的常量池)、运行时常量池。这其中，类信息又包括了类的版本、字段、方法、接口和父类等信息。 类信息JVM 在执行某个类的时候，必须经过加载、连接、初始化，而连接又包括验证、准备、解析三个阶段。 在加载类的时候，JVM 会先加载 class 文件，而在 class 文件中便有类的版本、字段、方法和接口等描述信息，这就是类信息。 常量池在 class 文件中，除了类信息，还有一项信息是常量池 (Constant Pool Table)，用于存放编译期间生成的各种字面量和符号引用。 那字面量和符号引用又是什么呢？ 字面量包括字符串（String a=“b”）、基本类型的常量（final 修饰的变量），符号引用则包括类和方法的全限定名（例如 String 这个类，它的全限定名就是 Java/lang/String）、字段的名称和描述符以及方法的名称和描述符。 运行时常量池当类加载到内存后，JVM 就会将 class 文件常量池中的内容存放到运行时常量池中；在解析阶段，JVM 会把符号引用替换为直接引用（对象的索引值）。 例如： 类中的一个字符串常量在 class 文件中时，存放在 class 文件常量池中的。 在 JVM 加载完类之后，JVM 会将这个字符串常量放到运行时常量池中，并在解析阶段，指定该字符串对象的索引值。 运行时常量池是全局共享的，多个类共用一个运行时常量池，因此，class 文件中常量池多个相同的字符串在运行时常量池只会存在一份。 讲到这里，大家是不是有些头晕了，说实话，我在看到这些内容的时候，也是云里雾里的，这里举个例子帮助大家理解：123456789public static void main(String[] args) &#123; String str = "Hello"; System.out.println((str == ("Hel" + "lo"))); String loStr = "lo"; System.out.println((str == ("Hel" + loStr))); System.out.println(str == ("Hel" + loStr).intern());&#125; 其运行结果为：123truefalsetrue 第一个为 true，是因为在编译成 class 文件时，能够识别为同一字符串的, JVM 会将其自动优化成字符串常量,引用自同一 String 对象。 第二个为 false，是因为在运行时创建的字符串具有独立的内存地址,所以不引用自同一 String 对象。 最后一个为 true，是因为 String 的 intern() 方法会查找在常量池中是否存在一个相等(调用 equals() 方法结果相等)的字符串,如果有则返回该字符串的引用,如果没有则添加自己的字符串进入常量池。 涉及到的Error OutOfMemoryError出现在方法区无法满足内存分配需求的时候，比如一直往常量池中加入数据，运行时常量池就会溢出，从而报错。 程序计数器(Program Counter Register)程序计数器是一块很小的内存空间，主要用来记录各个线程执行的字节码的地址，例如，分支、循环、跳转、异常、线程恢复等都依赖于计数器。 由于 Java 是多线程语言，当执行的线程数量超过 CPU 数量时，线程之间会根据时间片轮询争夺 CPU 资源。如果一个线程的时间片用完了，或者是其它原因导致这个线程的 CPU 资源被提前抢夺，那么这个退出的线程就需要单独的一个程序计数器，来记录下一条运行的指令。 由此可见，程序计数器和上下文切换有关。 虚拟机栈(VM Stack) 虚拟机栈是线程私有的内存空间，它和 Java 线程一起创建。 当创建一个线程时，会在虚拟机栈中申请一个线程栈，用来保存方法的局部变量、操作数栈、动态链接方法和返回地址等信息，并参与方法的调用和返回。 每一个方法的调用都伴随着栈帧的入栈操作，方法的返回则是栈帧的出栈操作。 可以这么理解，虚拟机栈针对当前 Java 应用中所有线程，都有一个其相应的线程栈，每一个线程栈都互相独立、互不影响，里面存储了该线程中独有的信息。 涉及到的Error StackOverflowError出现在栈内存设置成固定值的时候，当程序执行需要的栈内存超过设定的固定值时会抛出这个错误。 OutOfMemoryError出现在栈内存设置成动态增长的时候，当JVM尝试申请的内存大小超过了其可用内存时会抛出这个错误。 本地方法栈(Native Method Stack) 本地方法栈跟虚拟机栈的功能类似，虚拟机栈用于管理 Java 方法的调用，而本地方法栈则用于管理本地方法的调用。 但本地方法并不是用 Java 实现的，而是由 C 语言实现的。 也就是说，本地方法栈中并没有我们写的代码逻辑，其由native修饰，由 C 语言实现。 总结以上就是 JVM 内存模型的基本介绍，大致了解了一下5个分区及其相应的含义和功能，由此可以继续延伸出 Java 内存模型、 GC 算法等等，我也会在之后的文章中进行讲解。如果你有什么想法，欢迎在下方留言。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok中关于@Data的使用]]></title>
    <url>%2F2019%2F10%2F10%2FLombok%E4%B8%AD%E5%85%B3%E4%BA%8E-Data%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[当你在使用 Lombok 的 @Data 注解时，其实会有一些坑需要关注，今天就让我们来见识一下。 Lombok先来简单介绍一下 Lombok ，其官方介绍如下： Project Lombok makes java a spicier language by adding ‘handlers’ that know how to build and compile simple, boilerplate-free, not-quite-java code. 大致意思是 Lombok 通过增加一些”处理程序”，可以让 Java 代码变得简洁、快速。 Lombok 提供了一系列的注解帮助我们简化代码，比如： 注解名称 功能 @Setter 自动添加类中所有属性相关的 set 方法 @Getter 自动添加类中所有属性相关的 get 方法 @Builder 使得该类可以通过 builder (建造者模式)构建对象 @RequiredArgsConstructor 生成一个该类的构造方法，禁止无参构造 @ToString 重写该类的toString()方法 @EqualsAndHashCode 重写该类的equals()和hashCode()方法 @Data 等价于上面的@Setter、@Getter、@RequiredArgsConstructor、@ToString、@EqualsAndHashCode 看起来似乎这些注解都很正常，并且对我们的代码也有一定的优化，那为什么说@Data注解存在坑呢？ @Data注解内部实现由上面的表格我们可以知道，@Data是包含了@EqualsAndHashCode的功能，那么它究竟是如何重写equals()和hashCode()方法的呢？ 我们定义一个类TestA：12345@Datapublic class TestA &#123; String oldName;&#125; 我们将其编译后的 class 文件进行反编译：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class TestA &#123; String oldName; public TestA() &#123; &#125; public String getOldName() &#123; return this.oldName; &#125; public void setOldName(String oldName) &#123; this.oldName = oldName; &#125; public boolean equals(Object o) &#123; // 判断是否是同一个对象 if (o == this) &#123; return true; &#125; // 判断是否是同一个类 else if (!(o instanceof TestA)) &#123; return false; &#125; else &#123; TestA other = (TestA) o; if (!other.canEqual(this)) &#123; return false; &#125; else &#123; // 比较类中的属性(注意这里，只比较了当前类中的属性) Object this$oldName = this.getOldName(); Object other$oldName = other.getOldName(); if (this$oldName == null) &#123; if (other$oldName != null) &#123; return false; &#125; &#125; else if (!this$oldName.equals(other$oldName)) &#123; return false; &#125; return true; &#125; &#125; &#125; protected boolean canEqual(Object other) &#123; return other instanceof TestA; &#125; public int hashCode() &#123; int PRIME = true; int result = 1; Object $oldName = this.getOldName(); int result = result * 59 + ($oldName == null ? 43 : $oldName.hashCode()); return result; &#125; public String toString() &#123; return "TestA(oldName=" + this.getOldName() + ")"; &#125;&#125; 针对其equals()方法，当它进行属性比较时，其实只比较了当前类中的属性。如果你不信的话，我们再来创建一个类TestB，它是TestA的子类：1234567@Datapublic class TestB extends TestA &#123; private String name; private int age;&#125; 我们将其编译后的 class 文件进行反编译：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class TestB extends TestA &#123; private String name; private int age; public TestB() &#123; &#125; public String getName() &#123; return this.name; &#125; public int getAge() &#123; return this.age; &#125; public void setName(String name) &#123; this.name = name; &#125; public void setAge(int age) &#123; this.age = age; &#125; public boolean equals(Object o) &#123; if (o == this) &#123; return true; &#125; else if (!(o instanceof TestB)) &#123; return false; &#125; else &#123; TestB other = (TestB)o; if (!other.canEqual(this)) &#123; return false; &#125; else &#123; // 注意这里，真的是只比较了当前类中的属性，并没有比较父类中的属性 Object this$name = this.getName(); Object other$name = other.getName(); if (this$name == null) &#123; if (other$name == null) &#123; return this.getAge() == other.getAge(); &#125; &#125; else if (this$name.equals(other$name)) &#123; return this.getAge() == other.getAge(); &#125; return false; &#125; &#125; &#125; protected boolean canEqual(Object other) &#123; return other instanceof TestB; &#125; public int hashCode() &#123; int PRIME = true; int result = 1; Object $name = this.getName(); int result = result * 59 + ($name == null ? 43 : $name.hashCode()); result = result * 59 + this.getAge(); return result; &#125; public String toString() &#123; return "TestB(name=" + this.getName() + ", age=" + this.getAge() + ")"; &#125;&#125; 按照代码的理解，如果两个子类对象，其子类中的属性相同、父类中的属性不同时，利用equals()方法时，依旧会认为这两个对象相同，测试一下：12345678910111213141516171819202122public static void main(String[] args) &#123; TestB t1 = new TestB(); TestB t2 = new TestB(); t1.setOldName("123"); t2.setOldName("12345"); String name = "1"; t1.name = name; t2.name = name; int age = 1; t1.age = age; t2.age = age; System.out.println(t1.equals(t2)); System.out.println(t2.equals(t1)); System.out.println(t1.hashCode()); System.out.println(t2.hashCode()); System.out.println(t1 == t2); System.out.println(Objects.equals(t1, t2));&#125; 结果为：123456truetrue63736373falsetrue 问题总结 对于父类是Object且使用了@EqualsAndHashCode(callSuper = true)注解的类，这个类由 Lombok 生成的equals()方法只有在两个对象是同一个对象时，才会返回 true ，否则总为 false ，无论它们的属性是否相同。 这个行为在大部分时间是不符合预期的，equals()失去了其意义。即使我们期望equals()是这样工作的，那么其余的属性比较代码便是累赘，会大幅度降低代码的分支覆盖率。 解决方法 用了@Data就不要有继承关系，类似 Kotlin 的做法。 自己重写equals()， Lombok 不会对显式重写的方法进行生成。 显式使用@EqualsAndHashCode(callSuper = true)， Lombok 会以显式指定的为准。 总结以上便是我在使用@Data时碰到的问题以及自己的一些思考，在现在的项目，我干脆不再使用该注解。如果你有什么想法，欢迎在下方留言。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中Synchronized的优化原理]]></title>
    <url>%2F2019%2F10%2F02%2FJava%E4%B8%ADSynchronized%E7%9A%84%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[我们知道，从 JDK1.6 开始，Java 对 Synchronized 同步锁做了充分的优化，甚至在某些场景下，它的性能已经超越了 Lock 同步锁。那么就让我们来看看，它究竟是如何优化的。 原本的问题Synchronized是基于底层操作系统的 Mutex Lock 实现的，每次获取锁和释放锁的操作都会带来用户态和内核态的切换，从而增加系统性能开销。 因此，在锁竞争激烈的情况下，Synchronized同步锁在性能上就表现得非常糟糕，它也常被大家称为重量级锁。 到了 JDK1.5 版本，并发包中新增了 Lock 接口来实现锁功能，它提供了与 Synchronized 关键字类似的同步功能，只是在使用时需要显示获取锁和释放锁。 在单个线程重复申请锁的情况下，JDK1.5 版本的 Lock 性能要比 Synchronized 锁的性能好很多，也就是当时的 Synchronized 并不具备可重入锁的功能。 那么当时的 Synchronized 是怎么实现的？又为什么不具备可重入的功能呢？ Synchronized原理JVM 中的同步是基于进入和退出管程（Monitor）对象实现的。每个对象实例都会有一个 Monitor，Monitor 可以和对象一起创建、销毁。 当多个线程同时访问一段同步代码时，多个线程会先被存放在EntryList集合（也可称为阻塞队列）中，处于BLOCKED状态的线程，都会被加入到该列表。 接下来当线程获取到对象的 Monitor 时，Monitor 是依靠底层操作系统的 Mutex Lock 来实现互斥的，线程申请 Mutex 成功，则持有该 Mutex，其它线程将无法获取到该 Mutex。 如果线程调用 wait() 方法，就会释放当前持有的 Mutex，并且该线程会进入WaitSet集合（也可称为等待队列）中，等待下一次被唤醒。此时线程会处于WAITING或者TIMEDWAITING状态， 如果当前线程顺利执行完方法，也将释放 Mutex。 总的来说，就是同步锁在这种实现方式中，因 Monitor 是依赖于底层的操作系统实现，存在用户态与内核态之间的切换(可以理解为上下文切换)，所以增加了性能开销。 锁升级为了提升性能，JDK1.6 引入了偏向锁、轻量级锁、重量级锁概念，来减少锁竞争带来的上下文切换，而正是新增的Java对象头实现了锁升级功能。 所谓锁升级，就是指 Synchronized 同步锁初始为偏向锁，随着线程竞争越来越激烈，偏向锁升级到轻量级锁，最终升级到重量级锁。 偏向锁偏向锁主要用来优化同一线程多次申请同一个锁的竞争，也就是现在的Synchronized锁实际已经拥有了可重入锁的功能。 为什么要有偏向锁？因为在我们的应用中，可能大部分时间是同一个线程竞争锁资源（比如单线程操作一个线程安全的容器），如果这个线程每次都要获取锁和释放锁，那么就在不断的从内核态与用户态之间切换。 那么有了偏向锁，当一个线程再次访问这个同步代码或方法时，该线程只需去对象头中去判断一下是否当前线程是否持有该偏向锁就可以了。 一旦出现其它线程竞争锁资源时，偏向锁就会被撤销。偏向锁的撤销需要等待全局安全点(JVM的stop the world)，暂停持有该锁的线程，同时检查该线程是否还在执行该方法，如果是，则升级锁，反之则被其它线程抢占。 轻量级锁当有另外一个线程竞争获取这个锁时，由于该锁已经是偏向锁，当发现对象头中的线程 ID 不是自己的线程 ID，就会进行 CAS 操作获取锁，如果获取成功，直接替换对象头中的线程 ID 为自己的 ID，该锁会保持偏向锁状态；如果获取锁失败，代表当前锁有一定的竞争，偏向锁将升级为轻量级锁。 轻量级锁适用于线程交替执行同步块的场景，绝大部分的锁在整个同步周期内都不存在长时间的竞争。 轻量级锁也支持自旋，因此其他线程再次争抢时，如果CAS失败，将不再会进入阻塞状态，而是不断自旋。 之所以自旋更好，是因为之前说了，默认线程持有锁的时间都不会太长，如果线程被挂起阻塞可能代价会更高。 如果自旋锁重试之后抢锁依然失败，那么同步锁就会升级至重量级锁。 重量级锁在这个状态下，未抢到锁的线程都会进入 Monitor，之后会被阻塞在WaitSet集合中，也就变成了优化之前的Synchronized锁。 JVM参数优化偏向锁升级为轻量级锁时，会发生stop the world，如果系统常常是多线程竞争，那么禁止偏向锁也许是更好的选择，可以通过以下JVM参数进行优化： 1234// 关闭偏向锁（默认打开）-XX:-UseBiasedLocking// 设置重量级锁-XX:+UseHeavyMonitors 轻量级锁拥有自旋锁的功能，那么如果线程持有锁的时间很长，那么竞争的线程也会常常处于自旋状态，占用系统 CPU ，增加系统开销，那么此时关闭自旋锁的优化可以更好一些： 1-XX:-UseSpinning 总结以上便是 Java 中针对 Synchronized 锁的优化，也正是因为这个优化，ConcurrentHashMap 在 JDK1.8 之后，再次采用 Synchronized 锁。如果你有什么想法，欢迎在下方留言。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Synchronized</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[升级@Scheduled-分布式定时任务]]></title>
    <url>%2F2019%2F09%2F30%2F%E5%8D%87%E7%BA%A7%40Scheduled-%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[最近我在对项目的定时任务服务升级，希望改造成分布式，原本是利用@Scheduled注解实现，然而它并不支持分布式，如果改成quartz或者Spring Cloud Task，感觉对于自己这个简单的项目也没有必要。因此，我准备手写一个简单的支持分布式定时调度任务的框架。 项目地址是https://github.com/death00/dis-schedule，欢迎大家star、提意见。 分析先分析了一下自己的项目，全都是用的cron表达式，因此执行时间点都是固定的，如果升级为分布式的话，肯定是希望在同一个时间点只有一个应用去执行定时调度。 场景就变成了： 多个应用在同一个时间都尝试去执行任务，但最终只有一个应用真正执行。 这样的话，立马就会让人联想到使用锁去解决，因为是多个应用，所以就是分布式锁。那么，场景又变了： 多个应用在同一个时间都尝试去获取分布式锁，只有一个应用能抢到这把锁，抢到锁的应用可以执行定时任务，其他应用则直接放弃，等待下一次执行时间。 抢锁的时机是每次定时任务执行之前，这又让我联想到了AOP，那么利用注解也就顺理成章了。 分布式锁既然谈到了分布式锁，那么就想一下，这把锁的名称构成是什么。因为定时任务都有自己专门的时间，如果仅仅采用时间的话，那么当有两个任务同时执行时，则就是在抢一把锁，这同样是不合理的。 所以，锁的名称由两部分组成：任务执行时间、任务名称。 实现实现方案其实已经很成熟了，可以利用Redis、数据库、Zookeeper等，Redis用的命令是setNx，数据库一般都是利用的唯一索引，Zookeeper这点我也不是很了解（如果有感兴趣的同学，欢迎在我的项目中添加）。 我的项目中实现了Redis、数据库两种方式，可以看类DisScheduleRedisServiceImpl、DisScheduleMongodbServiceImpl。 注解其次，我自定义了一个注解DisSchedule， 12345678910111213141516171819@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface DisSchedule &#123; /** * 定时调度任务的名称(默认是方法名) */ String name() default ""; /** * 任务的间隔时间 */ int duration(); /** * duration的时间单位(默认：分钟) */ DisScheduleUnit unit() default DisScheduleUnit.MINUTES;&#125; name代表此次定时调度任务的名称。 duration代表任务的间隔时间，配合unit。 unit是自定义的时间单位，有秒、分钟。 该注解需要配合@Scheduled共同使用，例如：12@DisSchedule(name = "testSchedule", duration = 1, unit = DisScheduleUnit.MINUTES)@Scheduled(cron = "0 0/1 * * * ?") 该cron表达式代表1分钟执行一次，且是在整数分钟开始的时候执行，因此@DisSchedule也需要设置为1分钟的时间。 切面接下来，我们只需要在Aspect中定义好切入点（有注解@DisSchedule的方法上），针对这些方法，需要使用Around(环绕增强)进行拦截，因为当抢不到锁的时候，就不允许执行。 具体可以参考类DisScheduleAspect。 总结以上就是我实现的简单的分布式定时任务，虽然简单，但应该可以满足你的基础需求，接下来，我会在这个之上，逐步增加功能（比如监测、失败后预警等）。如果你有什么想法，欢迎在下方留言。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>Scheduled</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx的负载均衡]]></title>
    <url>%2F2019%2F09%2F30%2FNginx%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[我们都知道，Nginx支持负载均衡，可以很方便的帮助我们进行水平扩容，然而它究竟是依据什么原则进行请求的分发，其中又有哪些负载均衡算法可供选择和配置，今天就让我们好好来了解一下。 负载均衡的定义什么叫负载均衡，我们可以参考一下图片中的这种情况： 当客户端发送请求时，会先到Nginx，然后Nginx会将请求分发到后台不同的服务器上。 如果后台的服务器群中有一个宕机了，那么Nginx会自动忽略这台服务器，不会将请求再次分发到这台服务器上。 如果有新加入的服务器，Nginx也会将请求分发到这台服务器上。 我所理解的负载均衡，就是： 能够将客户端的请求均匀地分发到后台各个应用服务器上，从而缓解服务器压力。 并且当服务器出现宕机或者扩容时，也能正常运行。 负载均衡的方法上面了解了什么是负载均衡，那么Nginx是怎么实现这个功能的呢？ upstream和server的使用Nginx中负责与上游交互的模块，统称为upstream模块。 而指定上游服务地址是通过upstream和server指令完成的，其关系为： 指定上游服务器的address时，其地址可以是域名、IP地址或者unix socket地址。 可以在域名或者IP地址后加端口，如果不加端口，那么默认使用80端口。 在address后面可以添加一些参数，比如： backup：指定当前server为备份服务，仅当非备份server不可用时，请求才会转发到该server。 down：标识某台服务已经下线，不再服务。 举个例子： 1234upstream upstream-service &#123; server 127.0.0.1:17002; server 127.0.0.1:17000;&#125; round-robin在upstream这个模块中，它还提供了一个最基本的负载均衡算法round-robin。 其功能是： 以加权轮询的方式访问server指令指定的上游服务。 这个算法是默认集成在Nginx的框架中，无法移除，所以后面讲解的所有算法都是基于此，所有算法在某些特殊情况下最终都会变成round-robin。 涉及到的指令有： weight：服务访问的权重，默认是1。 max_conns：server的最大并发连接数，仅作用于单worker进程。 max_fails：在fail_timeout时间内，最大的失败次数。当达到最大失败时，会在fail_timeout时间内不允许再次被选择。 fail_timeout：单位为秒，默认是10秒。指定一段时间内，最大的失败次数max_fails。到达max_fails后，该server不能访问的时间。 简单的hash模块有的时候，正常的轮询算法并不能满足我们的需求， 比如：带有cookie请求状态的连接，如果应用服务没有设置专门的管理cookie的服务器，那么我们就希望同一个用户能被分配到同一个服务器。 再比如：我们后端应用需要针对请求当中的参数或者URL，将相同的请求放到相同的服务器上进行处理。 针对第一种情况，就可以用upstream_ip_hash。针对第二种情况，可以使用upstream_hash。 upstream_ip_hash功能： 以客户端的IP地址作为hash算法的关键字，映射到特定的上游服务器中。 对IPV4地址使用前3个字节作为关键字，对IPV6则使用完整地址。 可以使用round-robin算法的参数。 可以基于realip模块修改用于执行算法的IP地址。 举个例子： 12345upstream upstream-service &#123; ip_hash; server 127.0.0.1:17002; server 127.0.0.1:17000;&#125; upstream_hash功能： 通过制定关键字作为hash key，基于hash算法映射到特定的上游服务器中。 关键字可以含有变量、字符串。 可以使用round-robin算法的参数。 举个例子(以请求中的参数username作为hash key)： 12345upstream upstream-service &#123; hash user_$arg_username; server 127.0.0.1:17002; server 127.0.0.1:17000;&#125; 一致性哈希算法hash算法在一定程度上已经可以满足了我们的业务需求，但如果这个时候遇到应用宕机或者应用扩容，那么hash的总数就会变化，这样很有可能带来大量请求原本请求的服务器会更换，路由会失效，这样对于我们的应用服务也会产生极大的影响，这时候就可以采用一致性hash算法。 对于一致性哈希算法的理解，可以参考这篇文章：一致性哈希算法的理解与实践 它的使用也十分简单，就是在之前说的upstream_hash模块的hash指令最后，添加参数consistent，这样Nginx就可以使用一致性哈希算法了。 举个例子(仍以请求中的参数username作为hash key)： 12345upstream upstream-service &#123; hash user_$arg_username consistent; server 127.0.0.1:17002; server 127.0.0.1:17000;&#125; 总结以上就是Nginx中比较常见的负载均衡方法了，还有一些比如最少连接算法等，都是在此之上的一些应用。如果大家有什么疑问，欢迎在下方留言。 有兴趣的话可以访问我的博客或者关注我的公众号、头条号，说不定会有意外的惊喜。 https://death00.github.io/]]></content>
      <tags>
        <tag>Nginx</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github博客Hexo引流到微信]]></title>
    <url>%2F2019%2F09%2F26%2Fgithub%E5%8D%9A%E5%AE%A2hexo%E5%BC%95%E6%B5%81%E5%88%B0%E5%BE%AE%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[相信有不少小伙伴都在github上创建了属于自己的博客，其中用Hexo的Next主题应该不少，那么，我们究竟该如何将博客的流量引流到微信呢？今天就来带你看一看。 如何引流现在网上有一种套路，当你在看别人博客时，只能看一半，想继续看的话，需要扫码关注别人的公众号才能继续，这样的话，你的公众号粉丝自然就能蹭蹭上涨。 这里需要解决两个问题： 文章看到一半就不允许继续观看 关注你的公众号后才能继续观看 这里我是借助了OpenWrite中的引流工具实现的。 导流当你注册进入OpenWrite后，会有一个博客导流公众号功能，添加完相应的信息后，即可获得一段具有隐藏功能的JS代码： 如何设置文章看到一半这就需要我们在文章模块页面增加相应的隐藏功能，并且能够展示二维码并锁住页面。 增加自定义swig文件进入你的博客文件夹，在themes\next\layout\_custom文件夹中，新建一个hide.swig文件（这个文件夹专门用来存放自定义的一些代码），复制上文提到的JS代码，注意id的值，它默认用的是container，我设置成了container-1。 修改文章模板文件进入你的博客文件夹，在themes\next\layout文件夹中，会有一个_layout.swig文件，这就是你的文章模板文件。其中有一段内容是： 123456&lt;main id="main" class="main"&gt; &lt;div class="main-inner"&gt; &lt;div class="content-wrap"&gt; &lt;div id="content" class="content"&gt; &#123;% block content %&#125;&#123;% endblock %&#125; &lt;/div&gt; id为content的地方，就是你的文章内容，这时候你可以在外面再嵌套一层div，其id就是上面我设置的container-1： 12345678&lt;main id="main" class="main"&gt; &lt;div class="main-inner"&gt; &lt;div class="content-wrap"&gt; &lt;div id="container-1"&gt; &lt;div id="content" class="content"&gt; &#123;% block content %&#125;&#123;% endblock %&#125; &lt;/div&gt; &lt;/div&gt; 此时就可以发布你的博客，现在你的文章就会产生阅读全文的按钮了： 按下这个按钮，就会弹出相应的二维码和你当初设置的关键字： 微信公众号自动回复设置在微信公众号后台页面，选择自动回复-关键词回复，点击添加回复： 填写规则名称、关键词(你当初在OpenWrite中设置的)，回复内容选择文字，填上OpenWrite中返回的那段文字。 此时，当别人关注你的公众号并输入关键字后(比如我设置的关键字就是git)，就会显示回复了 总结如果你的博客有一定的日活，那就千万不要错过这种微信涨粉、互相引流的机会。如果大家有什么疑问，欢迎在下方留言。]]></content>
      <tags>
        <tag>github</tag>
        <tag>Hexo</tag>
        <tag>Next</tag>
        <tag>微信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的interrupt]]></title>
    <url>%2F2019%2F09%2F25%2FJava%E4%B8%AD%E7%9A%84interrupt%2F</url>
    <content type="text"><![CDATA[我们都知道，Java中停止一个线程不能用stop，因为stop会瞬间强行停止一个线程，且该线程持有的锁并不能释放。大家多习惯于用interrupt，那么使用它又有什么需要注意的呢？ interrupt相关的方法Java中和interrupt相关的方法有三个 12345public boolean isInterrupted()public void interrupt()public static boolean interrupted() boolean isInterrupted()每个线程都一个状态位用于标识当前线程对象是否是中断状态。isInterrupted主要用于判断当前线程对象的中断标志位是否被标记了，如果被标记了则返回true，表示当前已经被中断，否则返回false。我们也可以看看它的实现源码： 12345public boolean isInterrupted() &#123; return isInterrupted(false);&#125;private native boolean isInterrupted(boolean ClearInterrupted); 底层调用的native方法isInterrupted，传入一个boolean类型的参数，用于指定调用该方法之后是否需要清除该线程的中断标识位。从这里我们也可以看出来，调用isInterrupted()并不会清除线程的中断标识位。 void interrupt()interrupt()用于设置当前线程对象的中断标识位，其源码为： 123456789101112131415public void interrupt() &#123; // 检查当前线程是否有权限修改目标线程，如果没有，则会抛出异常SecurityException if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; blockerLock和blocker都和阻塞IO时产生的中断相关，因此推测interrupt()需要当阻塞IO操作执行完之后，才可以执行。 interrupt()其实只是改变了一个标志位，对于线程本身的状态并没有影响。 boolean interrupted()该方法是一个静态的方法，用于返回当前线程是否被中断，其源码是： 123public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; 需要注意的是：该方法调用结束的时候会清空中断标识位。 线程的状态与中断的关系我们知道，Java中的线程一共6种状态，分别是NEW，RUNNABLE，BLOCKED，WAITING，TIMED_WAITING，TERMINATED（Thread类中有一个State枚举类型列举了线程的所有状态）。下面我们就将把线程分别置于上述的不同种状态，然后看看中断操作对它们的影响。 NEW和TERMINATEDNEW状态表示线程还未调用start()方法，TERMINATED状态表示线程已经运行终止。 这两个状态下调用中断方法来中断线程的时候，Java认为毫无意义，所以并不会设置线程的中断标识位。例如： NEW状态： 12345678public static void main(String[] args) &#123; Thread thread = new Thread(); System.out.println(thread.getState()); System.out.println(thread.isInterrupted()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 输出结果：123NEWfalsefalse TERMINATED状态： 123456789101112public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(); // 开始线程 thread.start(); // 等待线程结束 thread.join(); System.out.println(thread.getState()); System.out.println(thread.isInterrupted()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 输出结果：123TERMINATEDfalsefalse 从上述的两个例子来看，处于NEW和TERMINATED状态的线程，对于中断是屏蔽的，也就是说中断操作对这两种状态下的线程是无效的。 RUNNABLE处于RUNNABLE状态的线程，当中断线程后，会修改其中断标志位，但并不会影响线程本身。例如： 123456789101112131415161718192021222324/** * 自定义线程类 */public class MyThread extends Thread&#123; @Override public void run()&#123; while(true)&#123; // 什么都不做，就是空转 &#125; &#125; public static void main(String[] args) &#123; Thread thread = new MyThread(); thread.start(); System.out.println(thread.getState()); System.out.println(thread.isInterrupted()); thread.interrupt(); System.out.println(thread.isInterrupted()); System.out.println(thread.getState()); &#125;&#125; 结果为：1234RUNNABLEfalsetrueRUNNABLE 中断标志位确实被改变了，但线程依旧继续运行。那我们调用interrupt()方法的意义在哪儿？ 其实Java是将中断线程的权利交给了我们自己的程序，通过中断标志位，我们的程序可以通过boolean isInterrupted()方法来判断当前线程是否中断，从而决定之后的操作。 我们可以在此基础上，保证执行任务的原子性。例如修改MyThread类的方法： 12345678910111213141516171819202122232425262728/** * 自定义线程类 */public class MyThread extends Thread&#123; @Override public void run()&#123; while(true)&#123; if (this.isInterrupted())&#123; System.out.println("exit MyThread"); break; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); System.out.println(thread.getState()); System.out.println(thread.isInterrupted()); thread.interrupt(); System.out.println(thread.isInterrupted()); thread.join(); System.out.println(thread.getState()); &#125;&#125; 结果为：12345RUNNABLEfalsetrueexit MyThreadTERMINATED BLOCKED当线程处于BLOCKED状态，说明该线程由于竞争某个对象的锁失败而被挂在了该对象的阻塞队列上了。 那么此时发起中断操作不会对该线程产生任何影响，依然只是设置中断标志位。例如： 1234567891011121314151617181920212223242526272829303132/** * 自定义线程类 */public class MyThread extends Thread&#123; public synchronized static void doSomething()&#123; while(true)&#123; // 空转 &#125; &#125; @Override public void run()&#123; doSomething(); &#125; public static void main(String[] args) throws InterruptedException &#123; // 启动两个线程 Thread thread1 = new MyThread(); thread1.start(); Thread thread2 = new MyThread(); thread2.start(); Thread.sleep(1000); System.out.println(thread1.getState()); System.out.println(thread2.getState()); System.out.println(thread2.isInterrupted()); thread2.interrupt(); System.out.println(thread2.isInterrupted()); System.out.println(thread2.getState()); &#125;&#125; 结果为：12345RUNNABLEBLOCKEDfalsetrueBLOCKED thread2处于BLOCKED状态，执行中断操作之后，该线程仍然处于BLOCKED状态，但是中断标志位却已被修改。 这种状态下的线程和处于RUNNABLE状态下的线程是类似的，给了我们程序更大的灵活性去判断和处理中断。 WAITING/TIMED_WAITING这两种状态本质上是同一种状态，只不过TIMED_WAITING在等待一段时间后会自动释放自己，而WAITING则是无限期等待，需要其他线程调用类似notify方法释放自己。但是他们都是线程在运行的过程中由于缺少某些条件而被挂起在某个对象的等待队列上。 当这些线程遇到中断操作的时候，会抛出一个InterruptedException异常，并清空中断标志位。例如： 123456789101112131415161718192021222324252627282930/** * 自定义线程类 */public class MyThread extends Thread&#123; @Override public void run()&#123; synchronized (this)&#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; System.out.println("catch InterruptedException"); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); Thread.sleep(1000); System.out.println(thread.getState()); System.out.println(thread.isInterrupted()); thread.interrupt(); Thread.sleep(1000); System.out.println(thread.isInterrupted()); &#125;&#125; 结果为：1234WAITINGfalsecatch InterruptedExceptionfalse 从运行结果看，当线程启动之后就被挂起到该线程对象的等待队列上，然后我们调用interrupt()方法对该线程进行中断，输出了我们在catch中的输出语句，显然是捕获了InterruptedException异常，接着就看到该线程的中断标志位被清空。 因此我们要么就在catch语句中结束线程，否则就在catch语句中加上this.interrupt();，再次设置标志位，这样也方便在之后的逻辑或者其他地方继续判断。 总结我们介绍了线程在不同状态下对于中断请求的反应： NEW和TERMINATED对于中断操作几乎是屏蔽的。 RUNNABLE和BLOCKED类似，对于中断操作只是设置中断标志位并没有强制终止线程，对于线程的终止权利依然在程序手中。 WAITING和TIMED_WAITING状态下的线程对于中断操作是敏感的，他们会抛出异常并清空中断标志位。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>interrupt</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的管程]]></title>
    <url>%2F2019%2F06%2F29%2FJava%E4%B8%AD%E7%9A%84%E7%AE%A1%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Java是利用管程解决并发编程问题的，那么究竟什么是管程？而它又是如何解决并发问题的呢？ 什么是管程管程，英文名是 Monitor ，因此有的时候会被翻译为监视器。其实你也许很早就接触到这个概念了，比如 synchronized关键字，很多文章就介绍过其原理是使用了监视器，只是你那个时候还并不知道监视器和管程，其实是一回事。 我们来看看维基百科上的概念： 管程 (英语：Monitors，也称为监视器) 是一种程序结构，结构内的多个子程序（对象或模块）形成的多个工作线程互斥访问共享资源。 感觉这句话听得有点迷糊，但下面这句话应该就很好理解了： 管程提供了一种机制，线程可以临时放弃互斥访问，等待某些条件得到满足后，重新获得执行权恢复它的互斥访问。 我的理解是：我们通过管程管理 Java 中的类，使得类是线程安全的。 这应该是管程最终要达到的效果，那么，它是怎么做到的呢？ 管程模型管程这个概念最早来源于操作系统，操作系统发展了那么多年，管程的实现也有多种方式，主流的有三种：Hasen模型、Hoare模型和MESA模型， Java 中借鉴的是MESA模型，让我们来重点看一下。 谈到MESA模型，就不得不提到并发主要解决2个核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即多个线程之间如何通信、协作。 如何解决互斥呢？我们可以在操作共享变量之前，增加一个等待队列，每一个线程想要操作共享变量的话，都需要在等待队列中等待，直到管程选出一个线程操作共享变量。 那又是如何解决同步的呢？线程在操作共享变量时候，它不一定是直接执行，可能有一些自己的执行条件限制（比如取钱操作要求账户里一定要有钱，出队操作要求队列一定不能是空的），我们将这些限制称之为条件变量，每一个条件变量也有自己对应的等待队列，当线程发现自己的条件变量不满足时，就进入相应的等待队列中排队，直至条件变量满足，那么其等待队列中的线程也不会是立马执行，而是到最开始共享变量对应的等待队列中再次排队，重复之前的过程。 可以参考下面这幅图： 理论说了那么多，还是来看看用代码是如何实现的吧 实现首先可以自定一个支持并发的队列12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class MyQueen &#123; // 共享变量（任何操作之前，都需要获得该锁才可以执行） private final Lock lock = new ReentrantLock(); // 条件变量：队列不满 private final Condition notFull = lock.newCondition(); // 条件变量：队列不空 private final Condition notEmpty = lock.newCondition(); /** * 存储队列的容器 */ private final LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); /** * 最大容量 */ private int capacity; /** * 当前容器中存储的数量 */ private int size; public MyQueen(int capacity) &#123; this.capacity = capacity; this.size = 0; &#125; /** * 入队 */ public void enter(int value) &#123; lock.lock(); try &#123; // 如果队列已满，则需要等到队列不满 while (size &gt;= capacity) &#123; notFull.await(1, TimeUnit.MILLISECONDS); &#125; // 入队 list.add(value); size++; System.out.println(value + &quot; has bean entered&quot;); // 通知可以出队 notEmpty.signal(); &#125; catch (InterruptedException e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125; /** * 出队 */ public int dequeue() &#123; Integer result = null; lock.lock(); try &#123; // 如果队列已空，则需要等到队列不空 while (size &lt;= 0) &#123; notEmpty.await(1, TimeUnit.MILLISECONDS); &#125; // 出队 result = list.removeFirst(); size--; System.out.println(result + &quot; has bean dequeued&quot;); // 通知可以入队 notFull.signal(); return result; &#125; catch (InterruptedException e) &#123; &#125; finally &#123; lock.unlock(); &#125; return result; &#125; public static void main(String[] args) &#123; MyQueen myQueen = new MyQueen(3); new Thread(new Pruducer(&quot;producer1&quot;, myQueen, 0, 2)).start(); new Thread(new Pruducer(&quot;producer2&quot;, myQueen, 2, 5)).start(); new Thread(new Consumer(&quot;consumer2&quot;, myQueen, 5)).start(); new Thread(new Consumer(&quot;consumer1&quot;, myQueen, 3)).start(); &#125;&#125; 定义生产者和消费者：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Pruducer implements Runnable &#123; private final MyQueen queen; /** * 该线程的名字 */ private final String name; /** * 开始的大小 */ private final int start; /** * 需要生产的资料个数 */ private final int size; public Pruducer(String name, MyQueen queen, int start, int size) &#123; this.name = name; this.queen = queen; this.start = start; this.size = size; &#125; @Override public void run() &#123; for (int i = 1; i &lt;= size; i++) &#123; int now = start + i;// System.out.println(name + &quot; produce : &quot; + now + &quot; start&quot;); queen.enter(now);// System.out.println(name + &quot; produce : &quot; + now + &quot; end&quot;); &#125; &#125;&#125;class Consumer implements Runnable &#123; private final MyQueen queen; /** * 该线程的名字 */ private final String name; /** * 需要消费的资料个数 */ private final int size; public Consumer(String name, MyQueen queen, int size) &#123; this.name = name; this.queen = queen; this.size = size; &#125; @Override public void run() &#123; for (int i = 1; i &lt;= size; i++) &#123;// System.out.println(name + &quot; consume start&quot;); int result = queen.dequeue();// System.out.println(name + &quot; consume : &quot; + result + &quot; end&quot;); &#125; &#125;&#125; 做一个测试的main方法：1234567public static void main(String[] args) &#123; MyQueen myQueen = new MyQueen(3); new Thread(new Pruducer(&quot;producer1&quot;, myQueen, 0, 2)).start(); new Thread(new Pruducer(&quot;producer2&quot;, myQueen, 2, 5)).start(); new Thread(new Consumer(&quot;consumer1&quot;, myQueen, 3)).start(); new Thread(new Consumer(&quot;consumer2&quot;, myQueen, 5)).start();&#125; 结果为：1234567891011121314151617181 has bean entered2 has bean entered3 has bean entered1 has bean dequeued2 has bean dequeued3 has bean dequeued4 has bean entered5 has bean entered6 has bean entered4 has bean dequeued5 has bean dequeued6 has bean dequeued7 has bean entered8 has bean entered9 has bean entered7 has bean dequeued8 has bean dequeued9 has bean dequeued 虽然满足我想要的结果，但显示的内容有些奇怪，总是容器先被填满之后，然后容器被清空，感觉原因应该和可重入锁有关。 总结以上就是我对于管程的一些理解，如果大家有什么疑问，欢迎在下方留言。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>管程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx报错111: Connection refused]]></title>
    <url>%2F2019%2F06%2F06%2FNginx%E6%8A%A5%E9%94%99111-Connection-refused%2F</url>
    <content type="text"><![CDATA[最近遇到了Nginx疯狂抛错，access.log一天一共5W多条，但error.log中有大概9K多条，基本都是111: Connection refused，这到底是为什么呢？ 从日志看起我们还是先来看日志。我提取了一条error.log当中抛错的日志(稍微分一下行，否则实在太长，敏感信息稍微处理了一下):12342019/06/06 10:09:45 [error] 28652#0: *883239 connect() failed (111: Connection refused) while connecting to upstream, client: 124.104.90.145, server: xxx.xxxxx.com, request: &quot;POST /test-service/upload?mcachenum=155978698 HTTP/1.1&quot;, upstream: &quot;http://[::1]:17000/test-service/upload?mcachenum=155978698&quot;, host: &quot;xxx.xxxxx.com&quot;, referrer: &quot;https://servicewechat.com/x98b46f69/2/page-frame.html&quot; 看了一下前面的报错和后面的描述，第一眼看上去感觉都是正常。但再看之后发现，upstream中的host有些不一样。[::1]，这实际是一个IPv6的地址。 这时候你可以查看一下你的机器是否开启了IPv6的地址，linux的命令是：ip address，看看返回结果中是否出现了inet6，如果有，那么恭喜你，原因找到了。 解决办法解决方法有两种，一个是禁用你机器的IPv6配置，另一个则是修改nginx.conf中的配置。 个人觉得后一个方法更加保险一些，因为这不涉及到你的机器配置，应该相对而言最少。 nginx.conf的修改，则是针对server模块中的location，修改proxy_pass中的host，我们在网上经常看到别人用的是： proxy_pass http://localhost:18000/test-service/; 但为了强制指定IPv4的地址，需要变成： proxy_pass http://127.0.0.1:18000/test-service/; 这样操作之后，再观察nginx的error.log，应该就不会再报upstream里含有IPv6地址的错误了。 总结以上就是我这次错误的整个过程，虽然整个过程不长，但确实让我知道了，作为一个后端开发，我的知识面还是太窄了。而且Bing也是真的好用，最近无法翻墙了，暂时用Bing代替，感觉还是不错的。]]></content>
      <tags>
        <tag>Nginx</tag>
        <tag>[object Object]</tag>
        <tag>upstream</tag>
        <tag>IPv6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[力扣-65不同路径]]></title>
    <url>%2F2019%2F05%2F10%2F%E5%8A%9B%E6%89%A3-65%E4%B8%8D%E5%90%8C%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[最近在刷力扣上的题目，刷到了65不同路径，当初上大学的时候，曾在hihocoder上刷到过这道题目，但是现在已经几乎全忘光了，大概的知识点是动态规划，如今就让我们一起来回顾一下。 从题目说起题目原文是： 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 问总共有多少条不同的路径？ 例如，上图是一个7 x 3 的网格。有多少可能的路径？ 说明：m 和 n 的值均不超过 100。 示例 1: 输入: m = 3, n = 2 输出: 3 解释: 从左上角开始，总共有 3 条路径可以到达右下角。 向右 -&gt; 向右 -&gt; 向下 向右 -&gt; 向下 -&gt; 向右 向下 -&gt; 向右 -&gt; 向右 示例 2: 输入: m = 7, n = 3 输出: 28 正向思路我们先按照正常思路来想一下，当你处于起点时，你有两个选择，向右或者向下，除非你处于最下面一排或者最右边一列，那你只有一种选择（比如处于最下面一排，你只能往右），其他位置，你都有两种选择。 因此，我们就根据这个思路，可以写出代码：123456789101112131415161718192021222324252627class Solution &#123; public int uniquePaths(int m, int n) &#123; // 特殊情况：起点即终点 if (m == 1 &amp;&amp; n == 1) &#123; return 1; &#125; // 当前处于(1,1)，终点为(m,n) return walk(1, 1, m, n); &#125; public int walk(int x, int y, int m, int n)&#123; // 已经处于终点 if (x &gt;= m &amp;&amp; y &gt;= n) &#123; return 0; &#125; // 处于最下面一排或者最右边一列 if (x &gt;= m || y &gt;= n) &#123; return 1; &#125; // 往下走，有多少种走法 int down = walk(x, y + 1, m, n); // 往右走，有多少种走法 int right = walk(x + 1, y, m, n); // 从当前(x,y)出发，走到(m,n)，共有多少种走法 return down + right; &#125;&#125; 优化我们考虑一下，这种写法，有没有可以优化的地方。 你们应该一眼就发现，walk方法的第一个判断if (x &gt;= m &amp;&amp; y &gt;= n)，永远都不可能为true，因为下一个判断if (x &gt;= m || y &gt;= n)就已经是临界点情况，直接就已经有返回值，根本不可能达到x &gt;= m &amp;&amp; y &gt;= n的情况。因此，该判断可以删除。 假设我们从(1,1)的位置出发，终点是(3,3)，那么到达(2,2)这个中间点的话有几种走法呢？两种，先到(1,2)再到(2,2)，或者，先到(2,1)再到(2,2)。 因此，如果根据我们上面的写法，从(2,2)到终点(3,3)，我们会算两次，虽然这样的思路本身是正确，但这样的情况应该是可以优化的。因为从(1,1)到(3,3)，一共只有6种路径，但已经有2条是重复的路径了，那么随着m与n越来越大，中间点会越来越多，那么重复的路径也会越来越多。 这就是前面的选择对于后面的选择会有影响，即使后面的选择相同，但由于前面的选择不同，从而也被认为是不同的选择。 很明显，后面的选择更加唯一，如果我们先在后面做出选择，那么就可以减少重复计算的次数。因此，我们可以试试反向思路。 反向思路如果我们不是从起点出发，而是从终点倒退到起点开始算的话。假设终点是(3,3)，它只能由(2,3)和(3,2)直接到达，(2,3)也只能由(2,2)和(1,3)直接到达，(1,3)只能由(1,2)直接到达，(1,2)只能由(1,1)直接到达，因此(1,3)只能由(1,1)直达。 我们可以得出规律：除了最左边一列和最上面一排的点，只能由起点(1,1)直达以外，其他的点(x,y)都是由(x-1,y)和(x,y-1)两个点直接到达的。 因此，根据这个思路，我们可以写出代码：12345678910111213141516171819class Solution &#123; public int uniquePaths(int m, int n) &#123; int[][] result = new int[m][n]; int j; for (int i = 0; i &lt; m; i++) &#123; for (j = 0; j &lt; n; j++) &#123; if (i == 0 || j == 0) &#123; // 最上面一排的点和最左边一列的点，只能由(1,1)到达 result[i][j] = 1; &#125; else &#123; // 其他的点都可以由左边的点和上面的点到达 result[i][j] = result[i - 1][j] + result[i][j - 1]; &#125; &#125; &#125; return result[m - 1][n - 1]; &#125;&#125; 其实这样的想法就已经是动态规划的范畴了，我们看看维基上的定义 动态规划（英语：Dynamic programming，简称DP）是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 一开始我感觉很像分治法，因为都需要将一个大问题分解为子问题，但分治法最终会将子问题合并，但动态规划却不用。 优化我们考虑一下，这种写法，有没有可以优化的地方。 首先是空间上的优化，我们一定要用二维数组吗？可以用一维数组代替吗？ 答案是肯定的，因为每个点的计算只和左边与上边相邻的点有关，因此，不需要更加久远的点。 一维数组假如只用一维数组，那么只需要存储上一排的结果，如果计算到下一排的时候，则依次替换，代码为： 12345678910111213141516171819class Solution &#123; public int uniquePaths(int m, int n) &#123; int[] dp = new int[m]; int j; for(int i = 0; i &lt; n; i++) &#123; for(j = 0; j &lt; m; j++) &#123; if(j == 0) &#123; dp[j] = 1; &#125; else &#123; // 其他的点都可以由左边的点和上面的点到达 dp[j] += dp[j-1]; &#125; &#125; &#125; return dp[m-1]; &#125;&#125; 这样的优化，差不多就结束了。那我们是否可以从思路上进行优化呢？ 组合数因为我们只有向右或向下两种选择，而我们一共要走的路径其实是(m-n-2)，其中有(m-1)的路径是向右，(n-1)的路径是向下，其实可以转变为： 从(m-n-2)中挑出(m-1)，即组合数C((m-n-2), (m-1))的值 那么我们可以写出代码：12345678910111213141516class Solution &#123; public int uniquePaths(int m, int n) &#123; // 用double，因为计算出的数值会很大 double num = 1, denom = 1; // 找出更小的数，这样可以减少计算次数和计算出的数值 int small = m &gt; n ? n : m; for (int i = 1; i &lt;= small - 1; ++i) &#123; num *= m + n - 1 - i; denom *= i; &#125; return (int)(num / denom); &#125;&#125; ##总结 以上就是我做这道题的一些思路和想法了，虽然题目本身不难，但可以讨论的点还是很多的，如果大家有什么疑问，欢迎在下方留言。 有兴趣的话可以关注我的公众号，说不定会有意外的惊喜。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>动态规划</tag>
        <tag>力扣</tag>
        <tag>组合数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWT与Session的比较]]></title>
    <url>%2F2019%2F04%2F24%2FJWT%E7%9A%84%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[如今，越来越多的项目开始采用JWT作为认证授权机制，那么它和之前的Session究竟有什么区别呢？今天就让我们来了解一下。 JWT是什么定义 JSON Web Token（JWT）是一个开放标准（RFC 7519），它定义了一种紧凑和自包含的方式，用于在各方之间作为JSON对象安全地传输信息。作为标准，它没有提供技术实现，但是大部分的语言平台都有按照它规定的内容提供了自己的技术实现，所以实际在用的时候，只要根据自己当前项目的技术平台，到官网上选用合适的实现库即可。 特点使用JWT来传输数据，实际上传输的是一个字符串，这个字符串就是所谓的json web token字符串。所以广义上，JWT是一个标准的名称；狭义上，JWT指的就是用来传递的那个token字符串。这个串有两个特点： 紧凑：指的是这个串很小，能通过url 参数，http 请求提交的数据以及http header的方式来传递； 自包含：这个串可以包含很多信息，比如用户的id、角色等，别人拿到这个串，就能拿到这些关键的业务信息，从而避免再通过数据库查询等方式才能得到它们。 结构 它由三部分组成：header（头部）、payload（载荷）、signature（签名），以.进行分割。（这个字符串本来是只有一行的，此处分成3行，只是为了区分其结构） header用来声明类型（typ）和算法（alg）。 payload一般存放一些不敏感的信息，比如用户名、权限、角色等。 signature则是将header和payload对应的json结构进行base64url编码之后得到的两个串用英文句点号拼接起来，然后根据header里面alg指定的签名算法生成出来的。 和Session的区别为什么我们要把JWT和Session做对比呢？因为我们主要在每一次请求的认证时会用JWT，在此之前我们都是用Session的。那这两者的区别在哪儿呢？ 本身的含义看了前面的介绍，我们发现JWT这个字符串其实本身就包含了关于用户的信息，比如用户名、权限、角色等。 Session传递的sessionId虽然是一个更简单的字符串，但它本身并没有任何含义。 所以一般说来JWT的字符串要比sessionId长，如果你在JWT中存储的信息越长，那么JWT本身也会越长。 而Cookie的存储容量是有限制的（通常为4KB），所以大家在使用的时候需要注意。 解析方法JWT的header和payload其实是有json转变过来的，而signature其实就是一个加密后的字符串，因此解析起来较为简单，不需要其他辅助的内容。 sessionId是服务器存储的用户对象的标识，理论上需要一个额外的map才能找出当前用户的信息。 管理方法JWT理论上用于无状态的请求，因此其用户管理也只是依赖本身而已。我们一般是在它的payload中加入过期时间，在不增加额外管理的情况下，它只有自动过期的方式。 Session因为它本就是存储在服务器端的，因此管理方案就有很多，而且大多都很成熟。 跨平台JWT本身就是基于json的，因此它是比较容易跨平台的，可以从官网下载不同平台的包，解析即可。 session的跨平台可能就不那么好做了，需要考虑的地方在于用户信息存储的格式，ProtoBuf、json、xml等，管理的话可能就需要专门的统一登录平台，这个就不展开了。 时效性无状态JWT一旦被生成，就不会再和服务端有任何瓜葛。一旦服务端中的相关数据更新，无状态JWT中存储的数据由于得不到更新，就变成了过期的数据。 session就不一样了，sessionId本身就没有太多含义，只需修改服务端中存储的数据即可。 适用场景JWTJWT的最佳用途是一次性授权Token，这种场景下的Token的特性如下： 有效期短 只希望被使用一次 真实场景的例子——文件托管服务，由两部分组成： Web 应用：这是一个可以被用户登录并维持状态的应用，用户在应用中挑选想要下载的文件。 文件下载服务：无状态下载服务，只允许通过密钥下载。 如何把JWT用在这个场景中呢？ 用户登录到 Web 应用中，挑选好想要下载的文件，点击下载。 认证服务颁发包含下载信息的、具有较短过期时间的JWT。JWT中包含的信息可以是这样的： 1234&#123; &quot;file&quot;: &quot;/books/我这一辈子.pdf&quot;, &quot;exp&quot;: 1500719759621&#125; 使用 JWT 从文件下载服务下载文件。 SessionSession比较适用于Web应用的会话管理，其特点一般是： 权限多，如果用JWT则其长度会很长，很有可能突破Cookie的存储限制。 基本信息容易变动。如果是一般的后台管理系统，肯定会涉及到人员的变化，那么其权限也会相应变化，如果使用JWT，那就需要服务器端进行主动失效，这样就将原本无状态的JWT变成有状态，改变了其本意。 总结我们使用JWT，并不是说看到它新就用，而应该考虑其适用场景，如果需要进行管理，可以考虑使用Session，毕竟其方案更加成熟。如果大家有什么新发现想和作者探讨的，欢迎在下方留言。]]></content>
      <tags>
        <tag>JWT</tag>
        <tag>Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP连接及其优化]]></title>
    <url>%2F2019%2F04%2F18%2FTCP%E8%BF%9E%E6%8E%A5%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[作为一个后端程序员，网络连接这块是一个绕不过的砍，当你在做服务器优化的时候，网络优化也是其中一环，那么作为网络连接中最基础的部分-TCP连接你了解吗？今天我们来仔细看看这个部分。 TCP建立连接-三次握手详解 客户端和服务器还未建立连接，但服务器一般处于listen状态 客户端主动建立连接，向服务器发送SYN报文，客户端变为SYN_SENT状态 服务器收到客户端发送的报文，也回了一个SYN报文，包含了一个ack。此时，服务器变为SYN_RCVD状态 客户端收到了服务器发送的SYN报文，确认了ack，它将向服务器发送一个ACK报文。此时，客户端变为ESTABLISHED 服务器收到客户端的ACK报文，确认了ack。此时，服务器也变为ESTABLISHED 服务器和客户端可以正常通信了 其中步骤2~4就是三次握手，那么为什么需要三次握手呢？为什么不是一次或者两次握手呢？ 首先，我们需要知道，只有当服务器和客户端都能确保自己能够发消息和接收消息，这次网络通信才算成功的。 步骤2的作用是让服务器知道了自己是可以接收消息的。 步骤3的作用是让客户端知道自己发送消息和接收消息的功能是OK的，发送消息的能力是通过服务器返回的ack=x+1确认的，因为这个值基于当初客户端发送的消息seq=x。接收消息的能力是因为收到了服务器的返回。 步骤4的作用是让服务器端知道自己发送消息的能力是OK的（和步骤3类似）。 linux查看linux服务器可以利用netstat -anp | grep tcp命令，查看服务器上各个端口和应用的连接状态。 你还可以通过修改linux的配置文件/etc/sysctl.conf，调整各个状态的数量 SYN_SENT状态相关 主动建立连接时，发SYN（步骤2）的重试次数 1nct.ipv4.tcp_syn_rctries = 6 建立连接时的本地端口可用范围 1net.ipv4.ip_local_port_range = 32768 60999 SYN_RCVD状态相关 SYN_RCVD状态连接的最大个数 1net.ipv4.tcp_max_syn_backlog 被动建立连接时，发SYN/ACK（步骤3）重试次数 1net.ipv4.tcp_synack_retries 说完了TCP建立连接，接下来，我们再来看看TCP正常断开连接的过程 TCP断开连接-四次挥手详解 客户端与服务器端正常传输数据 客户端主动断开连接，向服务器端发送FIN报文，客户端变为FIN_WAIT1状态 服务器收到客户端的FIN后，向客户端发送ACK报文，服务器变为CLOSE_WAIT状态 客户端收到服务器的ACK报文后，客户端变为FIN_WAIT2状态 服务器向客户端发送FIN报文，服务器变为LAST_ACK状态 客户端收到服务器发送的FIN报文后，向服务器发送ACK报文，客户端变为TIME_WAIT状态 服务器收到客户端的ACK报文后，服务器变为CLOSED状态 客户端经过2MSL(max segment lifetime，报文最大生存时间)时间后，也变为CLOSED状态 其中，步骤2、3、5、6即为4次挥手。 TIME_WAIT状态及其优化看完之后，大家想必会有一个疑问，为什么TIME_WAIT状态需要保持2MSL？因为这可以保证至少一次报文的往返时间内，端口是不可复用的。 假设TIME_WAIT状态的持续时间很短，我们来模拟下面这种场景： 客户端向服务器端发送了三条报文，其中第3条报文卡在网络中，服务器只收到了前两条，向客户单发送ACK=2，客户端重新发送第三条报文。 服务器主动发送FIN报文，客户端收到后发送FIN、ACK，服务器端收到后发送ACK并进入TIME_WAIT状态（假设这个状态很短）。 现在服务器又再次和客户端建立连接，三次握手之后开始发送正常数据，结果之前卡住的第三条报文，现在终于发送到服务器，但服务器也不知道该如何处理这条报文。 因此这也是TIME_WAIT状态需要保持2MSL的原因，如果这么长时间也没有收到报文，即使有正确的报文从客户端发出，也已经过期了，因此不会影响到之后的通信。 但这同样也会带来一个问题，TIME_WAIT状态保持的时间较长，假设服务器端有大量TIME_WAIT状态的TCP连接，就相当于白白浪费掉大量的服务器资源(端口)。此时，我们可以通过修改以下配置进行服务器调优：1net.ipv4.tcp_tw_reuse = 1 开启后，作为客户端时新连接可以使用仍然处于TIME_WAIT状态的端口 由于timestamp的存在，操作系统可以拒绝迟到的报文（例如上面说的第三条报文），可以利用以下配置： 1net.ipv4.tcp_timestamps = 1 其他状态的优化CLOSE_WAIT状态如果服务器端有大量CLOSE_WAIT状态的连接，很有可能是应用进程出现bug，没有及时关闭连接。 FIN_WAIT1状态调整发送FIN报文的重试次数，0相当于81net.ipv4.tcp_orphan_retries = 0 FIN_WAIT2状态调整保持在FIN_WAIT2状态的时间1net.ipv4.tcp_fin_timeout = 60 总结看到这里，想必你应该对TCP连接有了一个大致的了解。现在服务器大多都用了nginx做了负载均衡，因此，我们可能需要在此基础上了解一些nginx相关的配置原理，这样应该会对我们的服务器性能调优会有更大的帮助。有兴趣的同学不妨可以去了解一下，如果有什么新发现想和作者探讨的，欢迎在下方留言。]]></content>
      <tags>
        <tag>优化</tag>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中容器的遍历]]></title>
    <url>%2F2019%2F04%2F17%2FJava%E4%B8%AD%E5%AE%B9%E5%99%A8%E7%9A%84%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[当我们用增强for循环遍历非并发容器（HashMap、ArrayList等），如果修改其结构，会抛出异常ConcurrentModificationException，因此在阿里巴巴的Java规范中有说到：不要在foreach循环里进行元素的remove/add操作，remove元素请使用Iterator方式。，但是不是真的就不可以在增强for循环中修改结构吗？其原理又是什么呢？ ConcurrentModificationException的含义ConcurrentModificationException可以将其通俗的翻译为并发修改异常，那么关注点就在并发和修改了。也许有些人会说，我只是在单线程中修改了，并没有并发操作，但系统也抛了这样的这样的错误，这是为什么呢？别急，我们看看它的源码解释： This exception may be thrown by methods that have detected concurrent modification of an object when such modification is not permissible. 这个异常就是应用程序在做一些系统不允许的操作时抛出的。记住，只要是系统不允许的操作，就一定会抛错的。 后面有一个值得注意的地方 Note that fail-fast behavior cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. Fail-fast operations throw ConcurrentModificationException on a best-effort basis. Therefore, it would be wrong to write a program that depended on this exception for its correctness: ConcurrentModificationException should be used only to detect bugs. fail-fast（快速失败）并不能一定被保证，所以fail-fast操作会尽最大努力抛出该异常。既然是尽最大努力，因此无论是不是并发操作，只要是修改了，就一定会报错。 既然如此，我们来看看for循环中遍历修改容器结构，系统是如何知道的。 增加for循环的原理我们来看看增强for循环遍历修改HashMap的代码：12345678910Map&lt;String, String&gt; hashMap = new HashMap&lt;&gt;(10);// 添加for (int i = 0; i &lt; 10; i++) &#123; hashMap.put(&quot;key&quot; + i, &quot;value&quot; + i);&#125;// 遍历修改for (Entry&lt;String, String&gt; entry : hashMap.entrySet()) &#123; String key = entry.getKey(); hashMap.remove(key);&#125; 这个时候，你如果运行的话，就会抛出ConcurrentModificationException，这个时候我们需要具体调试一下，发现遍历第一次并删除时没有报错，但第二次遍历，在for循环的括号执行完后，就抛出了异常，这又是为什么呢？ 让我们反编译一下class文件，看看究竟增强for循环做了什么：123456789101112Map&lt;String, String&gt; hashMap = new HashMap(10);for(int i = 0; i &lt; 10; ++i) &#123; hashMap.put(&quot;key&quot; + i, &quot;value&quot; + i);&#125;Iterator var5 = hashMap.entrySet().iterator();while(var5.hasNext()) &#123; Entry&lt;String, String&gt; entry = (Entry)var5.next(); String key = (String)entry.getKey(); hashMap.remove(key);&#125; 我们发现，虽然写法上是增强for循环，但实际还是使用的while结合iterator进行遍历，现在我们贴上这个代码进行调试。 发现在第二次var5.next()处抛异常，接下来我们看看next方法究竟做了什么？ 在HashMap的源码中显示：1234567891011121314151617final class EntryIterator extends HashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125;final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e;&#125; 我们注意到，nextNode()方法的第一个判断就决定了是否抛出ConcurrentModificationException，那么modCount和expectedModCount究竟是什么呢？ modCount和expectedModCount我们来看看modCount和expectedModCount的关系，当我们调用Iterator var5 = hashMap.entrySet().iterator();时，源代码做了什么：123456789HashIterator() &#123; expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125;&#125; 在一开始，就让expectedModCount等于modCount，而当我们调用hashMap.remove(key);时，实际上修改了modCount的值：12345678910111213141516171819202122232425262728293031323334353637383940final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; modCount增大1，那么，当我们下一次调用var5.next()时，自然就发现modCount和expectedModCount不等了。 修改结构的正确姿势使用增强for循环，本质还是在使用iterator，那为什么大家都在推介使用iterator.remove()呢？让我们看看源代码：1234567891011public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount;&#125; 我们发现，这个remove方法虽然也调用了removeNode，但它在最后一步再次将modCount的值赋给expectedModCount，因此保证了下一次调用next()方法是不抛错。 所以，我们要么就直接显示地使用iterator，用它的remove方法移除对象。如果你实在想用增强for循环遍历删除，那么也只能在删除一个后，立刻退出循环。但无论用哪种方法，当多个线程同时修改时，都会有出错的可能性，因为你即时保证单个线程内的modCount和expectedModCount，但这个操作并不能保证原子性。 因此，如果在多线程环境下，我更推介使用ConcurrentHashMap，因为它没有modCount和expectedModCount的概念，因此，即时你是使用增强for循环遍历删除，也不会出现问题。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDL-事务的一种实现]]></title>
    <url>%2F2019%2F03%2F09%2FDDL-%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%B8%80%E7%A7%8D%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[上次曾和大家说过DDL-脏数据层的实现，当时的我以为使用它的最大好处就是减少IO操作。但最近在项目中使用时，发现它也可以作为事务的一种临时实现。 大家知道，在Mongodb4.0之前是不支持事务的。因此，如果你的MongoDB使用的是低于4.0的版本，那么你一般都是在业务层去弥补，而这个DDL也可以做这样的事。 DDL中，业务场景一般是一个请求过来，直接修改缓存中的数据，我们默认这一步是成功的。后台会有一个专门的线程去定期扫描所有的脏数据，如果脏数据有脏字段，那么就将脏数据入库，入库成功自然便是皆大欢喜了，但如果不成功呢？ 记得当时用MySql数据库，项目用的Spring boot + MyBatis，利用@Transactional注解，将几个数据库操作放在一个方法，这样即便出错，也可以自动回退。 但现在我们用的是MongoDB，而且我现在的版本是3.2（坑爹的阿里云服务）。数据库本身就不支持事务，这样我的DDL在将脏数据刷入数据库时，如果抛错，那么我的缓存里依旧是正确的数据，我只需要继续记录这些没有成功刷入数据库的脏数据字段，那么缓存里的数据，就依旧是刷入数据库之前的状态。而当后续请求进来后，其访问到的，也还是缓存里的正确数据。你只需要在抛错处设置报警，这样就可以即时知道问题，此时只需要专心修改这些问题，这样脏数据就会在线程下一次运行中，将数据刷入数据库中。]]></content>
      <categories>
        <category>DDL</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Disruptor原理探讨]]></title>
    <url>%2F2019%2F02%2F14%2FDisruptor%E5%8E%9F%E7%90%86%E6%8E%A2%E8%AE%A8%2F</url>
    <content type="text"><![CDATA[之前谈到了在我的项目里用到了Disruptor，因为对它了解不足的原因，才会引发之前的问题，因此，今天特意来探讨其原理。 为什么采用Disruptor先介绍一下我的这个服务。这个服务主要是作为游戏服务器的游戏逻辑部分，包括帧同步逻辑及其他在游戏过程中玩家产生的一些业务逻辑。 从用户量来说，现在最高峰大概有300人同时在线，游戏服务器设置1秒有30帧的数据量，因此，1秒内服务器接收到的请求量为30 * 300 = 9000。 虽然QPS并不是很高，但对于多人对抗竞技类游戏而言，低延迟十分重要，每一次客户端向服务器端的响应时间需要低于1/30秒（因为1秒需要发送30次）。针对这种情况，我需要的存储消息的容器应该具备快速生产、快速消费的特性。 那为什么当初要选择使用Disruptor作为存储客户端发来消息的容器，为什么不直接使用Java本身自带的那些队列结构呢？ 让我们看看Java里常用的线程安全的内置队列： 类 是否有界 是否加锁 底层数据结构 ArrayBlockingQueue 有界 加锁 数组 LinkedBlockingQueue 有界（2^31-1） 加锁 链表 ConcurrentLinkedQueue 无界 无锁 链表 LinkedTransferQueue 无界 无锁 链表 PriorityBlockingQueue 无界 加锁 堆 DelayQueue 无界 加锁 堆 一般来说我们并不会考虑堆，因为堆在实现带有优先级的队列更好。 从性能上来说，无锁时的QPS一般来说优于加锁，而ConcurrentLinkedQueue的无锁其实是通过原子变量进行compare and swap（以下简称为CAS，由CPU保证原子性）这种不加锁的方式来实现的。 但无锁的结构都是无界的，为了系统的稳定，我们需要防止生产者速度过快导致内存溢出，我们需要使队列有界；同时，为了减少Java的垃圾回收对系统性能的影响，会尽量选择array/heap（因为使用这两种结构，数据在内存中存储的地址连续）。 这样筛选下来，ArrayBlockingQueue可能相对而言更加合适，但它依旧存在性能问题——加锁、伪共享。 加锁上面也提到了，更好的方式是使用CAS，那伪共享又是什么呢？ 伪共享什么是共享下图是计算的基本结构。L1、L2、L3分别表示一级缓存、二级缓存、三级缓存，越靠近CPU的缓存，速度越快，容量也越小。所以L1缓存很小但很快，并且紧靠着在使用它的CPU内核；L2大一些，也慢一些，并且仍然只能被一个单独的CPU核使用；L3更大、更慢，并且被单个插槽上的所有CPU核共享；最后是主存，由全部插槽上的所有CPU核共享。如图： 当CPU执行运算的时候，它先去L1查找所需的数据、再去L2、然后是L3，如果最后这些缓存中都没有，所需的数据就要去主内存拿。走得越远，运算耗费的时间就越长。所以如果你在做一些很频繁的事，你要尽量确保数据在L1缓存中。 另外，线程之间共享一份数据的时候，需要一个线程把数据写回主存，而另一个线程访问主存中相应的数据。 缓存行Cache是由很多个cache line组成的。每个cache line通常是64字节，并且它有效地引用主内存中的一块儿地址。一个Java的long类型变量是8字节，因此在一个缓存行中可以存8个long类型的变量。 CPU每次从主存中拉取数据时，会把相邻的数据也存入同一个cache line。 在访问一个long数组的时候，如果数组中的一个值被加载到缓存中，它会自动加载另外7个。因此你能非常快的遍历这个数组。事实上，你可以非常快速的遍历在连续内存块中分配的任意数据结构。 下面的例子是测试利用cache line的特性和不利用cache line的特性的效果对比。 12345678910111213141516171819202122232425262728293031323334353637383940/** * 缓存行 * Cache是由很多个cache line组成的。每个cache line通常是64字节，并且它有效地引用主内存中的一块儿地址。一个Java的long类型变量是8字节，因此在一个缓存行中可以存8个long类型的变量。 * * CPU每次从主存中拉取数据时，会把相邻的数据也存入同一个cache line。 * * 在访问一个long数组的时候，如果数组中的一个值被加载到缓存中，它会自动加载另外7个。因此你能非常快的遍历这个数组。事实上，你可以非常快速的遍历在连续内存块中分配的任意数据结构。 */public class CacheLineEffect &#123; //考虑一般缓存行大小是64字节，一个 long 类型占8字节 static long[][] arr; public static void main(String[] args) &#123; arr = new long[1024 * 1024][]; for (int i = 0; i &lt; 1024 * 1024; i++) &#123; arr[i] = new long[8]; for (int j = 0; j &lt; 8; j++) &#123; arr[i][j] = 0L; &#125; &#125; long sum = 0L; long marked = System.currentTimeMillis(); for (int i = 0; i &lt; 1024 * 1024; i+=1) &#123; // 此时的8个数据其实已经直接在拿第1次的时候就全部拿下来了 for(int j =0; j&lt; 8;j++)&#123; sum = arr[i][j]; &#125; &#125; System.out.println(&quot;Loop times:&quot; + (System.currentTimeMillis() - marked) + &quot;ms&quot;); marked = System.currentTimeMillis(); for (int i = 0; i &lt; 8; i+=1) &#123; // 此时拿的数据其实同一列上的数据，从内存地址上来说并不连续 for(int j =0; j&lt; 1024 * 1024;j++)&#123; sum = arr[j][i]; &#125; &#125; System.out.println(&quot;Loop times:&quot; + (System.currentTimeMillis() - marked) + &quot;ms&quot;); &#125;&#125; 结果为：12Loop times:16msLoop times:72ms 速度差异还是比较明显的。 什么是伪共享ArrayBlockingQueue有三个成员变量： - takeIndex：需要被取走的元素下标 - putIndex：可被元素插入的位置的下标 - count：队列中元素的数量 这三个变量很容易放到一个缓存行中，但是之间修改没有太多的关联。所以每次修改，都会使之前缓存的数据失效，从而不能完全达到共享的效果。 如上图所示，当生产者线程put一个元素到ArrayBlockingQueue时，putIndex会修改，从而导致消费者线程的缓存中的缓存行无效，需要从主存中重新读取。 这种无法充分使用缓存行特性的现象，称为伪共享。 对于伪共享，一般的解决方案是，增大数组元素的间隔使得由不同线程存取的元素位于不同的缓存行上，以空间换时间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/** * 伪共享 * * 针对处在同一个缓存行内的数据，假设线程1修改了其中的一个数据a后，线程2想要读取数据a， * 因为a已经被修改了，因此缓存行失效，需要从主内存中重新读取。 * 这种无法充分使用缓存行特性的现象，称为伪共享。 * 当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。 */public class FalseSharing implements Runnable&#123; public final static long ITERATIONS = 500L * 1000L * 100L; private int arrayIndex = 0; private static ValueNoPadding[] longsNoPadding; private static ValuePadding[] longsPadding; private boolean padding; public FalseSharing(final int arrayIndex, boolean padding) &#123; this.arrayIndex = arrayIndex; this.padding = padding; &#125; public static void main(final String[] args) throws Exception &#123; for(int i=1;i&lt;10;i++)&#123; System.gc(); final long start = System.currentTimeMillis(); runTestNoPadding(i); System.out.println("NoPadding Thread num "+i+" duration = " + (System.currentTimeMillis() - start)); &#125; for(int i=1;i&lt;10;i++)&#123; System.gc(); final long start = System.currentTimeMillis(); runTestPadding(i); System.out.println("Padding Thread num "+i+" duration = " + (System.currentTimeMillis() - start)); &#125; &#125; private static void runTestPadding(int NUM_THREADS) throws InterruptedException &#123; Thread[] threads = new Thread[NUM_THREADS]; longsPadding = new ValuePadding[NUM_THREADS]; for (int i = 0; i &lt; longsPadding.length; i++) &#123; longsPadding[i] = new ValuePadding(); &#125; for (int i = 0; i &lt; threads.length; i++) &#123; threads[i] = new Thread(new FalseSharing(i, true)); &#125; for (Thread t : threads) &#123; t.start(); &#125; for (Thread t : threads) &#123; t.join(); &#125; &#125; private static void runTestNoPadding(int NUM_THREADS) throws InterruptedException &#123; Thread[] threads = new Thread[NUM_THREADS]; longsNoPadding = new ValueNoPadding[NUM_THREADS]; for (int i = 0; i &lt; longsNoPadding.length; i++) &#123; longsNoPadding[i] = new ValueNoPadding(); &#125; for (int i = 0; i &lt; threads.length; i++) &#123; threads[i] = new Thread(new FalseSharing(i, false)); &#125; for (Thread t : threads) &#123; t.start(); &#125; for (Thread t : threads) &#123; t.join(); &#125; &#125; public void run() &#123; long i = ITERATIONS + 1; while (0 != --i) &#123; if (padding) &#123; longsPadding[arrayIndex].value = 0L; &#125; else &#123; longsNoPadding[arrayIndex].value = 0L; &#125; &#125; &#125; public final static class ValuePadding &#123; protected long p1, p2, p3, p4, p5, p6, p7; protected volatile long value = 0L; protected long p9, p10, p11, p12, p13, p14; protected long p15; &#125; public final static class ValueNoPadding &#123; // protected long p1, p2, p3, p4, p5, p6, p7; protected volatile long value = 0L; // protected long p9, p10, p11, p12, p13, p14, p15; &#125;&#125; 结果：123456789101112131415161718NoPadding Thread num 1 duration = 394NoPadding Thread num 2 duration = 1594NoPadding Thread num 3 duration = 1702NoPadding Thread num 4 duration = 1580NoPadding Thread num 5 duration = 3217NoPadding Thread num 6 duration = 3539NoPadding Thread num 7 duration = 3269NoPadding Thread num 8 duration = 3317NoPadding Thread num 9 duration = 2800Padding Thread num 1 duration = 373Padding Thread num 2 duration = 432Padding Thread num 3 duration = 453Padding Thread num 4 duration = 490Padding Thread num 5 duration = 533Padding Thread num 6 duration = 565Padding Thread num 7 duration = 622Padding Thread num 8 duration = 685Padding Thread num 9 duration = 810 从这儿可以看出，使用了共享机制比没有使用共享机制，速度快了4倍左右。（在jdk1.8中，有专门的注解@Contended来避免伪共享，更优雅地解决问题，有兴趣地朋友可以取了解一下。） 因此，虽然ArrayBlockingQueue相对于其他队列结构而言更适合我的服务，但依旧有着性能上的缺陷，因此我选择了Disruptor。 生产者和消费者Disruptor通过环形数组结构来解决队列速度慢的问题，那具体针对生产者和消费者，它是如何保证数据读写一致性的呢？ 一个生产者写数据生产者单线程写数据的流程比较简单： 1. 申请写入m个元素； 2. 若是有m个元素可以写入，则返回最大的序列号。这儿主要判断是否会覆盖未读的元素； 3. 若是返回的正确，则生产者开始写入元素。 多个生产者多个生产者的情况下，会遇到“如何防止多个线程重复写同一个元素”的问题。Disruptor的解决方法是，每个线程获取不同的一段数组空间进行操作。这个通过CAS很容易达到。只需要在分配元素的时候，通过CAS判断一下这段空间是否已经分配出去即可。 但是会遇到一个新问题：如何防止读取的时候，读到还未写的元素。Disruptor在多个生产者的情况下，引入了一个与Ring Buffer大小相同的buffer：available Buffer。当某个位置写入成功的时候，便把availble Buffer相应的位置置位，标记为写入成功。读取的时候，会遍历available Buffer，来判断元素是否已经就绪。 下面分读数据和写数据两种情况介绍。 读数据生产者多线程写入的情况会复杂很多： 1. 申请读取到序号n； 2. 若writer cursor &gt;= n，这时仍然无法确定连续可读的最大下标。从reader cursor开始读取available Buffer，一直查到第一个不可用的元素，然后返回最大连续可读元素的位置； 3. 消费者读取元素。 如下图所示，读线程读到下标为2的元素，三个线程Writer1/Writer2/Writer3正在向RingBuffer相应位置写数据，写线程被分配到的最大元素下标是11。 读线程申请读取到下标从3到11的元素，判断writer cursor&gt;=11。然后开始读取availableBuffer，从3开始，往后读取，发现下标为7的元素没有生产成功，于是WaitFor(11)返回6。 然后，消费者读取下标从3到6共计4个元素。 写数据多个生产者写入的时候： 1. 申请写入m个元素； 2. 若是有m个元素可以写入，则返回最大的序列号。每个生产者会被分配一段独享的空间； 3. 生产者写入元素，写入元素的同时设置available Buffer里面相应的位置，以标记自己哪些位置是已经写入成功的。 如下图所示，Writer1和Writer2两个线程写入数组，都申请可写的数组空间。Writer1被分配了下标3到下表5的空间，Writer2被分配了下标6到下标9的空间。 Writer1写入下标3位置的元素，同时把available Buffer相应位置置位，标记已经写入成功，往后移一位，开始写下标4位置的元素。Writer2同样的方式。最终都写入完成。 消费者的等待策略 名称 措施 适用场景 BlockingWaitStrategy 加锁 CPU资源紧缺，吞吐量和延迟并不重要的场景 BusySpinWaitStrategy 自旋 通过不断重试，减少切换线程导致的系统调用，而降低延迟。推荐在线程绑定到固定的CPU的场景下使用 PhasedBackoffWaitStrategy 自旋 + yield + 自定义策略 CPU资源紧缺，吞吐量和延迟并不重要的场景 SleepingWaitStrategy 自旋 + yield + sleep 性能和CPU资源之间有很好的折中。延迟不均匀 TimeoutBlockingWaitStrategy 加锁，有超时限制 CPU资源紧缺，吞吐量和延迟并不重要的场景 YieldingWaitStrategy 自旋 + yield + 自旋 性能和CPU资源之间有很好的折中。延迟比较均匀 从这儿可以看出，我需要的就是低延迟，因此就采用了BusySpinWaitStrategy，它虽然占用的资源多，但延迟低，非常符合我这个服务的要求。后来测试了一下其他的策略，发现都会有一些卡顿，毕竟不是一直在运行，接受到的客户端的消息就会有延迟产生。 总结Disruptor的高性能一方面是在于它没有用很重的锁，仅仅通过CPU的CAS就保证了操作的原子性；另一方面是在于它的数据结构RingBuffer（也包括Available）和Cursor的设计巧妙；当然还有它的等待策略、线程池等等。 如果你有什么意见或者建议，欢迎在下方评论。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Disruptor</tag>
        <tag>缓存行</tag>
        <tag>RingBuffer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线上Java服务使用Disruptor导致CPU占用超过100%的问题排查]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B9%8BDisruptor%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[最近看了一下部署游戏后台的服务器状况，发现我的一个Java程序其占用的CPU时长超过100%，排查后发现竟是Disruptor引起的，让我们来看看究竟为什么Disruptor会有这样的表现。 发现占用CPU时间超过100%的进程首先是在服务器上用top命令查看服务器状态，发现有一个应用程序占用的CPU时长超过100%，如图： 我根据进程号查了一下，发现是我的一个Java游戏后台服务，有一个CPU几乎被占满，因此继续排查究竟是什么代码导致了这种情况。 用top -Hp 27538将这个进程的所有线程显示出来，按照CPU占用时间排序，看到了这个结果： 27658线程占用了近乎所有的CPU时间，而且一直都是，因此查看这个进程的详细信息。 用jstack pid &gt; pid.log命令将该进程的进程快照输出到一个文件中，下载下来。 将27658转换为16进制0x6c0a后在线程快照中查询(因为线程快照中线程ID都是16进制存放，所以需要转换)：1234567&quot;disruptor-0&quot; #27 prio=5 os_prio=0 tid=0x00007fa100c58000 nid=0x6c0a runnable [0x00007fa0ae080000] java.lang.Thread.State: RUNNABLE at com.lmax.disruptor.BusySpinWaitStrategy.waitFor(BusySpinWaitStrategy.java:39) at com.lmax.disruptor.ProcessingSequenceBarrier.waitFor(ProcessingSequenceBarrier.java:56) at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:159) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) at java.lang.Thread.run(Thread.java:748) 这是Disruptor的一个堆栈，为了更直观地查看线程的状态信息，可以将快照上传到专门的分析平台上。 （博主本人对于进程快照分析也是处于新手阶段，如果大家有什么建议或者意见，欢迎在下方留言。） 分析Disruptor为何会占用整个CPU根据上面快照的分析，实际是Disruptor的等待策略相关的线程所导致的，查看BusySpinWaitStrategy类，发现有相关说明：12* This strategy will use CPU resource to avoid syscalls which can introduce latency jitter. It is best* used when threads can be bound to specific CPU cores. 现在终于知道了，原来是因为这个策略就是让线程绑定了一个CPU核心，自然其CPU占用时间就超过100%了。 总结通过这一次问题的排查，不仅了解了linux系统中进程、线程的关系，也开始着手java服务的线上排查，顺便也回顾了一下之前有过接触的Disruptor。如果大家有什么建议或者意见，欢迎在下方留言]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Disruptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java服务器获取客户端的真实IP]]></title>
    <url>%2F2018%2F11%2F12%2FJava%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E7%9C%9F%E5%AE%9EIP%2F</url>
    <content type="text"><![CDATA[在进行一些小游戏开发时，我们经常比较关注的一个功能便是分享。针对分享，我们希望能根据各个城市或者地区，能有不同的分享文案，辨识地区的功能如果由服务器来完成的话，我们就需要知道客户端的真实IP。今天我们就来看看服务器是如何获取到客户端的真实IP的。 nginx配置首先，一个请求肯定是可以分为请求头和请求体的，而我们客户端的IP地址信息一般都是存储在请求头里的。如果你的服务器有用Nginx做负载均衡的话，你需要在你的location里面配置X-Real-IP和X-Forwarded-For请求头： 12345location ^~ /your-service/ &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:60000/your-service/;&#125; X-Real-IP在《实战nginx》中，有这么一句话：1经过反向代理后，由于在客户端和web服务器之间增加了中间层，因此web服务器无法直接拿到客户端的ip，通过$remote_addr变量拿到的将是反向代理服务器的ip地址。 这句话的意思是说，当你使用了nginx反向服务器后，在web端使用request.getRemoteAddr()（本质上就是获取$remote_addr），取得的是nginx的地址，即$remote_addr变量中封装的是nginx的地址，当然是没法获得用户的真实ip的。但是，nginx是可以获得用户的真实ip的，也就是说nginx使用$remote_addr变量时获得的是用户的真实ip，如果我们想要在web端获得用户的真实ip，就必须在nginx里作一个赋值操作，即我在上面的配置：1proxy_set_header X-Real-IP $remote_addr; X-Forwarded-ForX-Forwarded-For变量，这是一个squid开发的，用于识别通过HTTP代理或负载平衡器原始IP一个连接到Web服务器的客户机地址的非rfc标准，如果有做X-Forwarded-For设置的话,每次经过proxy转发都会有记录,格式就是client1,proxy1,proxy2以逗号隔开各个地址，由于它是非rfc标准，所以默认是没有的，需要强制添加。在默认情况下经过proxy转发的请求，在后端看来远程地址都是proxy端的ip 。也就是说在默认情况下我们使用request.getAttribute(&quot;X-Forwarded-For&quot;)获取不到用户的ip，如果我们想要通过这个变量获得用户的ip，我们需要自己在nginx添加配置：1proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 意思是增加一个$proxy_add_x_forwarded_for到X-Forwarded-For里去，注意是增加，而不是覆盖，当然由于默认的X-Forwarded-For值是空的，所以我们总感觉X-Forwarded-For的值就等于$proxy_add_x_forwarded_for的值，实际上当你搭建两台nginx在不同的ip上，并且都使用了这段配置，那你会发现在web服务器端通过request.getAttribute(&quot;X-Forwarded-For&quot;)获得的将会是客户端ip和第一台nginx的ip。 那么$proxy_add_x_forwarded_for又是什么？ $proxy_add_x_forwarded_for变量包含客户端请求头中的X-Forwarded-For与$remote_addr两部分，他们之间用逗号分开。 举个例子，有一个web应用，在它之前通过了两个nginx转发，www.linuxidc.com即用户访问该web通过两台nginx。 在第一台nginx中,使用： 1proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 现在的$proxy_add_x_forwarded_for变量的X-Forwarded-For部分是空的，所以只有$remote_addr，而$remote_addr的值是用户的ip，于是赋值以后，X-Forwarded-For变量的值就是用户的真实的ip地址了。 到了第二台nginx，使用：1proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 现在的$proxy_add_x_forwarded_for变量，X-Forwarded-For部分包含的是用户的真实ip，$remote_addr部分的值是上一台nginx的ip地址，于是通过这个赋值以后现在的X-Forwarded-For的值就变成了“用户的真实ip，第一台nginx的ip”，这样就清楚了吧。 服务器获取真实IP代码为： 12345678910111213141516171819202122232425262728293031323334public static String getIpAddress(HttpServletRequest request) &#123; String Xip = request.getHeader("X-Real-IP"); String XFor = request.getHeader("X-Forwarded-For"); if (!Strings.isNullOrEmpty(XFor) &amp;&amp; !"unKnown".equalsIgnoreCase(XFor)) &#123; //多次反向代理后会有多个ip值，第一个ip才是真实ip int index = XFor.indexOf(","); if (index != -1) &#123; return XFor.substring(0, index); &#125; else &#123; return XFor; &#125; &#125; XFor = Xip; if (!Strings.isNullOrEmpty(XFor) &amp;&amp; !"unKnown".equalsIgnoreCase(XFor)) &#123; return XFor; &#125; if (Strings.nullToEmpty(XFor).trim().isEmpty() || "unknown".equalsIgnoreCase(XFor)) &#123; XFor = request.getHeader("Proxy-Client-IP"); &#125; if (Strings.nullToEmpty(XFor).trim().isEmpty() || "unknown".equalsIgnoreCase(XFor)) &#123; XFor = request.getHeader("WL-Proxy-Client-IP"); &#125; if (Strings.nullToEmpty(XFor).trim().isEmpty() || "unknown".equalsIgnoreCase(XFor)) &#123; XFor = request.getHeader("HTTP_CLIENT_IP"); &#125; if (Strings.nullToEmpty(XFor).trim().isEmpty() || "unknown".equalsIgnoreCase(XFor)) &#123; XFor = request.getHeader("HTTP_X_FORWARDED_FOR"); &#125; if (Strings.nullToEmpty(XFor).trim().isEmpty() || "unknown".equalsIgnoreCase(XFor)) &#123; XFor = request.getRemoteAddr(); &#125; return XFor;&#125; 我们来看看各个请求头的含义 X-Real-IPnginx代理一般会加上此请求头。 X-FORWARDED-FOR这是一个Squid开发的字段，只有在通过了HTTP代理或者负载均衡服务器时才会添加该项。 Proxy-Client-IP 和 WL-Proxy-Client-IP这个一般是经过apache http服务器的请求才会有，用apache http做代理时一般会加上Proxy-Client-IP请求头，而WL-Proxy-Client-IP是它的weblogic插件加上的头。 HTTP_CLIENT_IP有些代理服务器会加上此请求头。在网上搜了一下，有一个说法是：123456789101112这是普通的 http header，伪造起来很容易，不要轻易信任用户输入。 curl -H &apos;client-ip: 8.8.8.8&apos; lidian.club/phpinfo.php | grep _SERVER 你就能看到 _SERVER[&quot;HTTP_CLIENT_IP&quot;] 了。 client-ip 和 client-host 是在 NAPT 还没普及的年代，企业内网假设的 http 透明代理，传给服务器的 header，只有极少数厂家用过，从来不是标准，也从来没成为过事实标准。 （大家最熟悉的事实标准就是 x-forwarded-for） 后来出现的 web proxy 也没见用过这个 header。 TCP/IP Illustrated Vol 3 没有讲过这个 header，网上的传言不可信。 可考的最早痕迹出现在2005年，日本一部 Perl/CGI 秘籍（9784798010779，270页）通过 client-ip 与 via 两个 header 屏蔽代理用户访问。 HTTP_X_FORWARDED_FOR简称XFF头，它代表客户端，也就是HTTP的请求端真实的IP，只有在通过了HTTP 代理(比如APACHE代理)或者负载均衡服务器时才会添加该项。它不是RFC中定义的标准请求头信息，在squid缓存代理服务器开发文档中可以找到该项的详细介绍。如果有该条信息, 说明您使用了代理服务器，地址就是后面的数值。可以伪造。标准格式如下：X-Forwarded-For: client1, proxy1, proxy2 总结以上就是我在处理客户端真实IP的方法，如果你有什么意见或者建议，可以在下方留言。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>nginx</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中List的sort源码解读]]></title>
    <url>%2F2018%2F11%2F02%2Fjava%E4%B8%ADList%E7%9A%84sort%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[最近看了一些排序相关的文章，因此比较好奇，Java中的排序是如何做的。本片文章介绍的是JDK1.8，List中的sort方法。 先来看看List中的sort是怎么写的：12345678910@SuppressWarnings(&#123;"unchecked", "rawtypes"&#125;)default void sort(Comparator&lt;? super E&gt; c) &#123; Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator&lt;E&gt; i = this.listIterator(); for (Object e : a) &#123; i.next(); i.set((E) e); &#125;&#125; 首先，你需要传入一个比较器作为参数，这个好理解，毕竟你肯定要定一个比较标准。然后就是将list转换成一个数组，再对这个数组进行排序，排序完之后，再利用iterator重新改变list。 接着，我们再来看看Arrays.sort：123456789101112131415161718192021222324public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) &#123; if (c == null) &#123; sort(a); &#125; else &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a, c); else TimSort.sort(a, 0, a.length, c, null, 0, 0); &#125;&#125;public static void sort(Object[] a) &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a); else ComparableTimSort.sort(a, 0, a.length, null, 0, 0);&#125;static final class LegacyMergeSort &#123; private static final boolean userRequested = java.security.AccessController.doPrivileged( new sun.security.action.GetBooleanAction( "java.util.Arrays.useLegacyMergeSort")).booleanValue();&#125; 这样可以看出，其实排序的核心就是TimSort，LegacyMergeSort大致意思是表明如果版本很旧的话，就用这个，新版本是不会采用这种排序方式的。 我们再来看看TimSort的实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private static final int MIN_MERGE = 32;static &lt;T&gt; void sort(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c, T[] work, int workBase, int workLen) &#123; assert c != null &amp;&amp; a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // Arrays of size 0 and 1 are always sorted // If array is small, do a "mini-TimSort" with no merges if (nRemaining &lt; MIN_MERGE) &#123; // 获得最长的递增序列 int initRunLen = countRunAndMakeAscending(a, lo, hi, c); binarySort(a, lo, hi, lo + initRunLen, c); return; &#125; /** * March over the array once, left to right, finding natural runs, * extending short natural runs to minRun elements, and merging runs * to maintain stack invariant. */ TimSort&lt;T&gt; ts = new TimSort&lt;&gt;(a, c, work, workBase, workLen); int minRun = minRunLength(nRemaining); do &#123; // Identify next run int runLen = countRunAndMakeAscending(a, lo, hi, c); // If run is short, extend to min(minRun, nRemaining) if (runLen &lt; minRun) &#123; int force = nRemaining &lt;= minRun ? nRemaining : minRun; binarySort(a, lo, lo + force, lo + runLen, c); runLen = force; &#125; // Push run onto pending-run stack, and maybe merge ts.pushRun(lo, runLen); ts.mergeCollapse(); // Advance to find next run lo += runLen; nRemaining -= runLen; &#125; while (nRemaining != 0); // Merge all remaining runs to complete sort assert lo == hi; ts.mergeForceCollapse(); assert ts.stackSize == 1;&#125; 如果小于2个，代表不再不需要排序；如果小于32个，则采用优化的二分排序。怎么优化的呢？首先获得最长的递增序列： 123456789101112131415161718192021private static &lt;T&gt; int countRunAndMakeAscending(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c) &#123; assert lo &lt; hi; int runHi = lo + 1; if (runHi == hi) return 1; // Find end of run, and reverse range if descending if (c.compare(a[runHi++], a[lo]) &lt; 0) &#123; // Descending // 一开始是递减序列，就找出最长递减序列的最后一个下标 while (runHi &lt; hi &amp;&amp; c.compare(a[runHi], a[runHi - 1]) &lt; 0) runHi++; // 逆转前面的递减序列 reverseRange(a, lo, runHi); &#125; else &#123; // Ascending while (runHi &lt; hi &amp;&amp; c.compare(a[runHi], a[runHi - 1]) &gt;= 0) runHi++; &#125; return runHi - lo;&#125; 接着进行二分排序：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private static &lt;T&gt; void binarySort(T[] a, int lo, int hi, int start, Comparator&lt;? super T&gt; c) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) start++; for ( ; start &lt; hi; start++) &#123; T pivot = a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ // start位置是递增序列后的第一个数的位置 // 从前面的递增序列中找出start位置的数应该处于的位置 while (left &lt; right) &#123; // &gt;&gt;&gt; 无符号右移 int mid = (left + right) &gt;&gt;&gt; 1; if (c.compare(pivot, a[mid]) &lt; 0) right = mid; else left = mid + 1; &#125; assert left == right; /* * The invariants still hold: pivot &gt;= all in [lo, left) and * pivot &lt; all in [left, start), so pivot belongs at left. Note * that if there are elements equal to pivot, left points to the * first slot after them -- that's why this sort is stable. * Slide elements over to make room for pivot. */ int n = start - left; // The number of elements to move // Switch is just an optimization for arraycopy in default case // 比pivot大的数往后移动一位 switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125;&#125; 好了，待排序数量小于32个的讲完了，现在来说说大于等于32个情况。首先，获得一个叫minRun的东西，这是个啥含义呢：1234567891011int minRun = minRunLength(nRemaining);private static int minRunLength(int n) &#123; assert n &gt;= 0; int r = 0; // Becomes 1 if any 1 bits are shifted off while (n &gt;= MIN_MERGE) &#123; // 这里我没搞懂的是为什么不直接将(n &amp; 1)赋值给r，而要做一次逻辑或。 r |= (n &amp; 1); n &gt;&gt;= 1; &#125; return n + r;&#125; 各种位运算符，MIN_MERGE默认为32，如果n小于此值，那么返回n本身。否则会将n不断地右移，直到小于MIN_MERGE，同时记录一个r值，r代表最后一次移位n时，n最低位是0还是1。其实看注释比较容易理解： 1234Returns the minimum acceptable run length for an array of the specified length. Natural runs shorter than this will be extended with binarySort.Roughly speaking, the computation is: If n &lt; MIN_MERGE, return n (it&apos;s too small to bother with fancy stuff).Else if n is an exact power of 2, return MIN_MERGE/2.Else return an int k, MIN_MERGE/2 &lt;= k &lt;= MIN_MERGE, such that n/k is close to, but strictly less than, an exact power of 2. For the rationale, see listsort.txt. 返回结果其实就是用于接下来的合并排序中。 接下来就是一个while循环1234567891011121314151617181920212223do &#123; // Identify next run // 获得一个最长递增序列 int runLen = countRunAndMakeAscending(a, lo, hi, c); // If run is short, extend to min(minRun, nRemaining) // 如果最长递增序列 if (runLen &lt; minRun) &#123; int force = nRemaining &lt;= minRun ? nRemaining : minRun; binarySort(a, lo, lo + force, lo + runLen, c); runLen = force; &#125; // Push run onto pending-run stack, and maybe merge // lo——runLen为将要被归并的范围 ts.pushRun(lo, runLen); // 归并 ts.mergeCollapse(); // Advance to find next run lo += runLen; nRemaining -= runLen;&#125; while (nRemaining != 0); 这样，假设你的每次归并排序的两个序列为r1和r2，r1肯定是有序的，r2也已经被排成递增序列了，因此这样的归并排序就比较特殊了。 为什么要用归并排序呢，因为归并排序的时间复杂度永远为O(nlogn)，空间复杂度为O(n)，以空间换取时间。 好了，以上就是针对Java中的排序做的一次总结，但具体的归并代码还没有分析，其实我自己也没有完全研究透，为什么minRun的取值是这样的，这也和TimSort中的stackLen有关，有兴趣的小伙伴可以在下方留言，我们可以一起探讨。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDL-脏数据层的实现]]></title>
    <url>%2F2018%2F10%2F31%2FDDL-%E8%84%8F%E6%95%B0%E6%8D%AE%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[在我们的项目中，经常会有一些数据会涉及到频繁更改。如果每次都从数据库中读取再修改，这样不仅浪费时间，而且还更加危险。那此时我们究竟该如何解决这个问题呢？此时，DDL(脏数据层)就出现了。 首先说一下为什么操作不能保证原子性就会危险，因为这时就很有可能出现同时修改的情况，最终的结果极有可能并不是你所希望的（除非这些操作都是幂等性，但这种情况应该比较少）。如果是利用数据库中的锁，一来我在项目中用的比较少，二来也增加了维护难度。当然，有人说可以利用CAS，那针对一些复杂的情况（比如类里面属性的修改会有一些相关性，你的一次更改需要涉及几个属性等），可能你还是需要单独设计一套系统，而且还会有经典的ABA问题。如果你是利用CAS解决的，希望能够在下方评论区告知，就当互相学习。 那现在来说说DDL层具体是什么。DDL全称是Dirty Data Layer，即脏数据层。针对那些在系统运行经常会更改的domain类，我们将其再做一次封装，组成一个类似map的形式。单独由一组线程来管理这些map，每当有数据改动时，我们就往这个map中添加内容，而我们的线程则定期向数据库中写入内容。这样做的好处，首先是让你的每一次操作都没有IO的参与，提高了相应速度，而且定时提交意味着你可以把原本的几次提交变成为1次，减少了和数据库的交互。当然，缺点也是存在的，如果你的系统是分布式，那么你的这个DDL层的实现可能就没有那么方便，因为这些数据你可能需要存储在类似Redis这种共享缓存中，因此每次的拿和取就需要封装一下（这个应该算是小问题，因为原本就算你用的是本地缓存，所有操作依旧是需要封装的，只不过你的IO消耗由原本的数据库变成了共享缓存）。接下来，我就针对本地缓存的情况来具体实现一个DDL。 定义操作这是我定义出的一些操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public interface IDirtyEntity &#123; //region manage content /** * 获取entity的内容。 */ Object getContent(); /** * 获取entity的内容。 获取的内容是复制的对象，属性值是调用该方法时的属性值。 */ Object copyContent(); //endregion //region persisting flag /** * 是否正在进行持久化 */ boolean isPersisting(); /** * 设置正在持久化标志 */ void setPersistingFlag(); /** * 清除正在持久化标志 */ void clearPersistingFlag(); //endregion //region persist state /** * 设置为脏数据状态 */ void setDirtyState(); /** * 清除脏数据状态 */ void clearDirtyState(); /** * 当前持久化状态。 * * @see PersistState */ PersistState currentPersistState(); //endregion //region get/set field /** * 获取属性值。 */ Object getField(String fieldName); /** * 设置属性值。 */ void setField(String fieldName, Object value); /** * 设置多个属性的值。 */ void setFields(List&lt;EntityField&gt; fields); /** * 增加int类型属性的值。 */ void addInt(String fieldName, int delta); /** * 增加long类型属性的值。 */ void addLong(String fieldName, long delta); //endregion //region manage dirty field /** * 标记脏数据字段 */ void addDirtyField(String fieldName); /** * 获取修改过的属性。 */ List&lt;EntityField&gt; getAndClearDirtyFields(); //endregion //region wrapper implement /** * 返回id的属性名。 */ String getIdFieldName(); /** * 返回id */ String getId(); /** * 返回DATA的class */ Class getDataClass(); //endregion&#125; 分类DDL解决的是数据频繁更改的问题，其实这里的更改说的并不准确，并不仅仅只是update，还有insert。用过mongodb的应该清楚有一种叫upsert的操作，就是找到就修改，找不到就添加。我们这里就需要将我们的数据分成两类：Detachable(可拆分的)、Nondetachable(不可拆分的)。 可拆分的，就意味着你针对这个数据的修改最小可以精确到其中的一个属性，项目中大多数都属于这种情况。 不可拆分的，即每次都是以一个整体添加，比如一次交易，每次添加都是一个整体，不可能说你先提交买方，再提交卖方，后面还会修改买方。这种类型大多都是一条记录，整体存入数据库。 因此，我们来定义一下这两种结构：可拆分的类型：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183import com.google.common.base.Preconditions;import com.google.common.base.Strings;import java.util.List;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.cglib.beans.BeanCopier;import org.springframework.cglib.beans.BeanMap;public abstract class DetachableDirtyEntityAdapter implements IDirtyEntity &#123; private static final Logger log = LoggerFactory.getLogger(DetachableDirtyEntityAdapter.class); /** * 数据属性的map引用 */ private BeanMap beanMap; private final BeanCopier beanCopier; public DetachableDirtyEntityAdapter(Object content, BeanCopier beanCopier) &#123; Preconditions.checkNotNull(content); Preconditions.checkNotNull(beanCopier); this.content = newEmptyContentInstance(); this.beanCopier = beanCopier; this.beanCopier.copy(content, this.content, null); this.beanMap = BeanMap.create(this.content); &#125; //region manage content /** * 数据的内容。 */ private Object content; @Override public Object getContent() &#123; return content; &#125; private Object newEmptyContentInstance() &#123; Class cls = getDataClass(); try &#123; return cls.newInstance(); &#125; catch (Exception e) &#123; log.error("initiate &#123;&#125; failed: &#123;&#125;", cls.getSimpleName(), e.getMessage()); return null; &#125; &#125; @Override public synchronized Object copyContent() &#123; Object copy = newEmptyContentInstance(); beanCopier.copy(this.content, copy, null); return copy; &#125; //endregion //region persisting flag private volatile boolean persisting = false; @Override public boolean isPersisting() &#123; return persisting; &#125; @Override public void setPersistingFlag() &#123; this.persisting = true; &#125; @Override public void clearPersistingFlag() &#123; this.persisting = false; &#125; //endregion //region persist state @Override public void setDirtyState() &#123; throw new UnsupportedOperationException(); &#125; @Override public void clearDirtyState() &#123; throw new UnsupportedOperationException(); &#125; @Override public synchronized PersistState currentPersistState() &#123; int dirtySize = dirtyFieldNames.size(); if (dirtySize == 0) &#123; return PersistState.PERSISTED; &#125; else &#123; return PersistState.DIRTY; &#125; &#125; //endregion //region get/set field @Override public synchronized Object getField(String fieldName) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); return beanMap.get(fieldName); &#125; @Override public synchronized void setField(String fieldName, Object value) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); beanMap.put(fieldName, value); dirtyFieldNames.add(fieldName); &#125; @Override public synchronized void setFields(List&lt;EntityField&gt; fields) &#123; Preconditions.checkNotNull(fields); for (EntityField f : fields) &#123; beanMap.put(f.getName(), f.getValue()); dirtyFieldNames.add(f.getName()); &#125; &#125; @Override public synchronized void addInt(String fieldName, int delta) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); int origin = (int) beanMap.get(fieldName); beanMap.put(fieldName, origin + delta); dirtyFieldNames.add(fieldName); &#125; @Override public synchronized void addLong(String fieldName, long delta) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); long origin = (long) beanMap.get(fieldName); beanMap.put(fieldName, origin + delta); dirtyFieldNames.add(fieldName); &#125; //endregion //region manage dirty fields /** * 当前entity的包含脏数据的属性名列表。 */ private final HashSet&lt;String&gt; dirtyFieldNames = new HashSet&lt;&gt;(16); @Override public void addDirtyField(String fieldName) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); dirtyFieldNames.add(fieldName); &#125; @Override public synchronized List&lt;EntityField&gt; getAndClearDirtyFields() &#123; ArrayList&lt;EntityField&gt; list = new ArrayList&lt;&gt;(); for (String f : dirtyFieldNames) &#123; list.add(new EntityField(f, beanMap.get(f))); &#125; // 清空dirtyFieldNames, 记录上一次持久化的事件 dirtyFieldNames.clear(); return list; &#125; //endregion&#125; 不可拆分的类型：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161import com.google.common.base.Preconditions;import com.google.common.base.Strings;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.cglib.beans.BeanCopier;import org.springframework.cglib.beans.BeanMap;public abstract class NonDetachableDirtyEntityAdapter implements IDirtyEntity &#123; private static final Logger log = LoggerFactory.getLogger(NonDetachableDirtyEntityAdapter.class); /** * 数据属性的map引用 */ private BeanMap beanMap; private final BeanCopier beanCopier; public NonDetachableDirtyEntityAdapter(Object content, BeanCopier beanCopier) &#123; Preconditions.checkNotNull(content); Preconditions.checkNotNull(beanCopier); this.content = newEmptyContentInstance(); this.beanCopier = beanCopier; this.beanCopier.copy(content, this.content, null); this.beanMap = BeanMap.create(this.content); &#125; //region manage content /** * 数据的内容。 */ private Object content; @Override public Object getContent() &#123; return content; &#125; private Object newEmptyContentInstance() &#123; Class cls = getDataClass(); try &#123; return cls.newInstance(); &#125; catch (Exception e) &#123; log.error("initiate &#123;&#125; failed: &#123;&#125;", cls.getSimpleName(), e.getMessage()); return null; &#125; &#125; @Override public synchronized Object copyContent() &#123; Object copy = newEmptyContentInstance(); beanCopier.copy(this.content, copy, null); return copy; &#125; //endregion //region persisting flag private volatile boolean persisting = false; @Override public boolean isPersisting() &#123; return persisting; &#125; @Override public void setPersistingFlag() &#123; this.persisting = true; &#125; @Override public void clearPersistingFlag() &#123; this.persisting = false; &#125; //endregion //region persist state private volatile PersistState persistState = PersistState.DIRTY; @Override public void setDirtyState() &#123; persistState = PersistState.DIRTY; &#125; @Override public void clearDirtyState() &#123; persistState = PersistState.PERSISTED; &#125; @Override public PersistState currentPersistState() &#123; return persistState; &#125; //endregion //region get/set field @Override public synchronized Object getField(String fieldName) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); return beanMap.get(fieldName); &#125; @Override public synchronized void setField(String fieldName, Object value) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); beanMap.put(fieldName, value); &#125; @Override public synchronized void setFields(List&lt;EntityField&gt; fields) &#123; Preconditions.checkNotNull(fields); for (EntityField f : fields) &#123; beanMap.put(f.getName(), f.getValue()); &#125; &#125; @Override public synchronized void addInt(String fieldName, int delta) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); int origin = (int) beanMap.get(fieldName); beanMap.put(fieldName, origin + delta); &#125; @Override public synchronized void addLong(String fieldName, long delta) &#123; Preconditions.checkArgument(!Strings.isNullOrEmpty(fieldName)); long origin = (long) beanMap.get(fieldName); beanMap.put(fieldName, origin + delta); &#125; //endregion //region manage dirty fields @Override public void addDirtyField(String fieldName) &#123; throw new UnsupportedOperationException(); &#125; @Override public synchronized List&lt;EntityField&gt; getAndClearDirtyFields() &#123; throw new UnsupportedOperationException(); &#125; //endregion&#125; 两种类型最大的不同在于真正往数据库中存储时，前者是可以单独字段存储，后者是整体存储，因此最后和DirtyField相关的操作便需要注意，NondetachableDirtyEntityAdapter不需要记录DirtyFields。 针对原本类中属性的复制和存储，我这儿用的是spring提供的BeanCopier，如果你有什么更高效的工具，欢迎在下方留言。（我一直在找一种深度克隆高效的组件，试过kryo，但只有在实现序列化接口的前提下，其效率才能和正常的set/get大概相差10倍，如果有好的组件，希望一并告知）。 以上就是DDL的准备工作，其实后面的工作就是将具体的类做一个封装，再封装针对该类的所有操作，然后另写一个线程组执行往数据库的写入操作。这个工作其实针对各个项目都有其特殊的地方，LZ在这儿就不具体展示了，有兴趣的话大家可以在下方留言。]]></content>
      <categories>
        <category>DDL</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j日志不输出的问题]]></title>
    <url>%2F2018%2F10%2F24%2Flog4j%E6%97%A5%E5%BF%97%E4%B8%8D%E8%BE%93%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[今天服务器上报错，想先去看一下日志进行排查，结果发现日志很久都没有输出过了。从上午排查到下午，刚刚解决，因此记录一下，但现在也只是知其然，并不知其所以然，所以如果大家有什么想法请在下方评论。 先说一下环境，服务器是linux，项目是运行在tomcat下的Spring项目，日志用的是log4j。 首先，从10月13号开始便没有新的日志文件了。假设日志名为log.txt（如果你设置了DailyRollingFileAppender，那么你当天的日志文件就是log.txt），先备份该文件到其他目录下，然后删除该文件，重新启动tomcat。这是为了确认你的log4j配置是否有问题，因为这是最容易出错的地方。很遗憾，我不是这里出的问题，因为项目重启后，日志文件又重新生成了，但很奇怪的是，日志文件是空的，其大小为0. 感觉自己碰上了很神奇的问题，因此我在自己的本地进行调试，启动项目后发现，正常的项目启动日志是有的：115:13:48:0253 INFO [RMI TCP Connection(3)-127.0.0.1] -Root WebApplicationContext: initialization completed in 18479 ms 但我自己的一些日志输出是不显示的，比如：12private static final Logger log = LoggerFactory.getLogger(MyDomain.class);log.info("show info log"); show info log这句话就不打印，现在证明，我的日志配置没有问题，服务器也找到了我的日志文件，但应该是我自己的Logger是不对应正确的日志输出的，因为我的console(控制台)有显示。 接下来，我就是开始看源码了。先是LoggerFactory.getLogger(Class&lt;?&gt; clazz)方法：123456789101112public static Logger getLogger(Class&lt;?&gt; clazz) &#123; Logger logger = getLogger(clazz.getName()); if (DETECT_LOGGER_NAME_MISMATCH) &#123; Class&lt;?&gt; autoComputedCallingClass = Util.getCallingClass(); if (autoComputedCallingClass != null &amp;&amp; nonMatchingClasses(clazz, autoComputedCallingClass)) &#123; Util.report(String.format("Detected logger name mismatch. Given name: \"%s\"; computed name: \"%s\".", logger.getName(), autoComputedCallingClass.getName())); Util.report("See " + LOGGER_NAME_MISMATCH_URL + " for an explanation"); &#125; &#125; return logger;&#125; 好吧，没什么用，看不出我的logger变成了，继续看getLogger(String name)方法：1234public static Logger getLogger(String name) &#123; ILoggerFactory iLoggerFactory = getILoggerFactory(); return iLoggerFactory.getLogger(name);&#125; 这时我在return iLoggerFactory.getLogger(name);这行打了断点，我看到了这样的东西： 为什么我的iLoggerFactory是用的logback中的实现？其实也是怪我自己大意，我其实依赖了一个基于Spring Boot的项目(虽然我只是用了里面的一些domain类，但因为设计不当，还没有把这些domain类单独提成一个_项目)，而Spring Boot中一般默认就依赖的logback。通过gradle查看项目的依赖树，也证实了我的这一猜想(./gradlew 子项目名称:dependencies):1234567891011| +--- org.springframework.boot:spring-boot-starter-web:2.0.2.RELEASE| | +--- org.springframework.boot:spring-boot-starter:2.0.2.RELEASE| | | +--- org.springframework.boot:spring-boot:2.0.2.RELEASE| | | | +--- org.springframework:spring-core:5.0.6.RELEASE (*)| | | | \--- org.springframework:spring-context:5.0.6.RELEASE (*)| | | +--- org.springframework.boot:spring-boot-autoconfigure:2.0.2.RELEASE| | | | \--- org.springframework.boot:spring-boot:2.0.2.RELEASE (*)| | | +--- org.springframework.boot:spring-boot-starter-logging:2.0.2.RELEASE| | | | +--- ch.qos.logback:logback-classic:1.2.3| | | | | +--- ch.qos.logback:logback-core:1.2.3| | | | | \--- org.slf4j:slf4j-api:1.7.25 接下来就好办了，你排除掉ch.qos.logback的依赖就可以了，在你的build.gradle中增加：123configurations &#123; compile.exclude group: &apos;ch.qos.logback&apos;&#125; 这个时候你再重新调试一下看看： 完美，现在是log4j中的实现，得到了我想要的操作。当然了，既然我知道之前项目中的slf4j是logback实现了，那么我自然也可以换成logback的配置，但这就需要我将项目换成用Spring Boot启动，这个改动有点大，如果以后有必要的话，我再将这个exclude删除，换成Spring Boot的形式。 这次Spring Boot帮我们默认启用的是logback，那么有没有什么简单方法可以知道呢？如果你的项目出现了以下的日志输出，说明你的项目当前有不止一个SLF4J的实现组件：12345SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/project.war/WEB-INF/lib/logback-classic-1.2.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/project.war/WEB-INF/lib/slf4j-log4j12-1.7.21.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder] 因为在org.slf4j.LoggerFactory的bind方法中有关于这方面的输出：123456789101112131415161718192021222324252627282930313233343536373839404142434445private final static void bind() &#123; try &#123; Set&lt;URL&gt; staticLoggerBinderPathSet = null; // skip check under android, see also // http://jira.qos.ch/browse/SLF4J-328 if (!isAndroid()) &#123; // 查找你的当前项目有几个slf4j的实现 staticLoggerBinderPathSet = findPossibleStaticLoggerBinderPathSet(); // 如果多余一个就打印 reportMultipleBindingAmbiguity(staticLoggerBinderPathSet); &#125; // the next line does the binding // 这个是具体选了哪一个实现（重点关注） StaticLoggerBinder.getSingleton(); INITIALIZATION_STATE = SUCCESSFUL_INITIALIZATION; reportActualBinding(staticLoggerBinderPathSet); fixSubstituteLoggers(); replayEvents(); // release all resources in SUBST_FACTORY SUBST_FACTORY.clear(); &#125; catch (NoClassDefFoundError ncde) &#123; String msg = ncde.getMessage(); if (messageContainsOrgSlf4jImplStaticLoggerBinder(msg)) &#123; INITIALIZATION_STATE = NOP_FALLBACK_INITIALIZATION; Util.report("Failed to load class \"org.slf4j.impl.StaticLoggerBinder\"."); Util.report("Defaulting to no-operation (NOP) logger implementation"); Util.report("See " + NO_STATICLOGGERBINDER_URL + " for further details."); &#125; else &#123; failedBinding(ncde); throw ncde; &#125; &#125; catch (java.lang.NoSuchMethodError nsme) &#123; String msg = nsme.getMessage(); if (msg != null &amp;&amp; msg.contains("org.slf4j.impl.StaticLoggerBinder.getSingleton()")) &#123; INITIALIZATION_STATE = FAILED_INITIALIZATION; Util.report("slf4j-api 1.6.x (or later) is incompatible with this binding."); Util.report("Your binding is version 1.5.5 or earlier."); Util.report("Upgrade your binding to version 1.6.x."); &#125; throw nsme; &#125; catch (Exception e) &#123; failedBinding(e); throw new IllegalStateException("Unexpected initialization failure", e); &#125;&#125; 特别要注意的是StaticLoggerBinder.getSingleton();这行代码，StaticLoggerBinder在logback-classic和slf4j-log4j12这两个jar包各有一个，因此，Spring boot是自动选择logback-classic（虽然我在本地运行的时候还是默认进入的slf4j-log4j12，但是会提醒我Source code does not match the bytecode，因此我判断依旧进的是logback-classic），所以只要把logback给exclude掉，就解决了这个问题。 现在看问题，更加关注源代码，因为这可以让我们更加快速定位问题，并且也能据此大致猜出其解决方案。希望大家能一起看看源代码，如果你有什么发现，可以在下方留言，我将和你一起讨论。]]></content>
      <tags>
        <tag>log4j</tag>
        <tag>Spring Boot</tag>
        <tag>logback</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty 心跳相关(1)]]></title>
    <url>%2F2018%2F10%2F23%2FNetty-%E5%BF%83%E8%B7%B3%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[无论是B/S还是C/S架构，如果你用的是长连接，那么心跳是必不可少的。Netty提供了对心跳机制的天然支持，今天结合例子特地学习了一下。 首先，我们来设想一下何时需要发送心跳。假设你做的是一款棋牌类小游戏，那么当玩家登陆游戏后肯定是先进入大厅，再选择一张合适的桌子正式开始游戏。此时玩家的客户端与服务器建立的这一次session（会话）应该是长久保持着，如果服务器端保存着大量的session，那么整个服务器就会越来越卡，最终整个服务都会挂掉。 为了预防这种情况，我们需要清理掉一些已经不用的或者理论上不会再用的session，比如：在手机上，如果我们在游戏中，突然接到一个电话或者退回桌面，这个时候我们的游戏客户端理论上就不会再主动向我们发送任何消息。这时候，心跳就派上用场了。 心跳，是为了证明自己还活着。因此，这里的心跳，说白了就是客户端向服务器端发送一次请求，服务器端相应，这样客户端就知道了服务器端是alive(活着的)；服务器端向客户端发送一次心跳，客户端相应，这样服务器端就知道了客户端是alive。 知道了心跳的大致概念，那现在我们就需要知道Netty中是如何实现心跳，这就引出了两个类：IdleStateHandler、ChannelInboundHandlerAdapter IdleStateHandler大致作用 当连接的空闲时间（无论是读或者是写）太长时，都会触发IdleStateEvent事件。你可以写一个类继承ChannelInboundHandlerAdapter，重写userEventTriggered方法，来处理这类空闲事件。 知道了其大致作用，那么接下来就看看我们到底该如何使用了。 IdleStateHandler有3个构造方法，主要针对这4个属性，分别是：1234private final boolean observeOutput;// 是否考虑出站时较慢的情况。默认值是false（一般不考虑）。private final long readerIdleTimeNanos; // 读事件空闲时间，0 代表禁用事件private final long writerIdleTimeNanos;// 写事件空闲时间，0 代表禁用事件private final long allIdleTimeNanos; //读或写空闲时间，0 代表禁用事件 上面的三个时间，默认是秒，你也可以在构造的时候指定。 当你在pipeline中加入了该handler之后： pipeline.addLast(new IdleStateHandler(30, 90, 0)); // 这个代表只考虑读空闲30秒或写空闲90秒的情况 则会先调用handlerAdded方法：1234567891011@Overridepublic void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; if (ctx.channel().isActive() &amp;&amp; ctx.channel().isRegistered()) &#123; // channelActive() event has been fired already, which means this.channelActive() will // not be invoked. We have to initialize here instead. initialize(ctx); &#125; else &#123; // channelActive() event has not been fired yet. this.channelActive() will be invoked // and initialization will occur there. &#125;&#125; 如果channel正常，则调用initialize方法：1234567891011121314151617181920212223242526272829private byte state; // 0 - none, 1 - initialized, 2 - destroyedprivate void initialize(ChannelHandlerContext ctx) &#123; // Avoid the case where destroy() is called before scheduling timeouts. // See: https://github.com/netty/netty/issues/143 switch (state) &#123; case 1: // 避免重复添加 case 2: // 如果处于destoryed状态，则不需要添加 return; &#125; state = 1; initOutputChanged(ctx); lastReadTime = lastWriteTime = ticksInNanos(); // 当前时间 // 添加相应的定时调度任务 if (readerIdleTimeNanos &gt; 0) &#123; // readerIdleTimeNanos时间后，执行ReaderIdleTimeoutTask里面的方法 readerIdleTimeout = schedule(ctx, new ReaderIdleTimeoutTask(ctx), readerIdleTimeNanos, TimeUnit.NANOSECONDS); &#125; if (writerIdleTimeNanos &gt; 0) &#123; writerIdleTimeout = schedule(ctx, new WriterIdleTimeoutTask(ctx), writerIdleTimeNanos, TimeUnit.NANOSECONDS); &#125; if (allIdleTimeNanos &gt; 0) &#123; allIdleTimeout = schedule(ctx, new AllIdleTimeoutTask(ctx), allIdleTimeNanos, TimeUnit.NANOSECONDS); &#125;&#125; ReaderIdleTimeoutTask、WriterIdleTimeoutTask、AllIdleTimeoutTask均继承自类AbstractIdleTask12345678910111213141516171819private abstract static class AbstractIdleTask implements Runnable &#123; private final ChannelHandlerContext ctx; AbstractIdleTask(ChannelHandlerContext ctx) &#123; this.ctx = ctx; &#125; @Override public void run() &#123; if (!ctx.channel().isOpen()) &#123; return; &#125; run(ctx); &#125; // 子类需要实现的方法 protected abstract void run(ChannelHandlerContext ctx);&#125; 以ReaderIdleTimeoutTask为例：12345678910111213141516171819202122232425262728293031323334private final class ReaderIdleTimeoutTask extends AbstractIdleTask &#123; ReaderIdleTimeoutTask(ChannelHandlerContext ctx) &#123; super(ctx); &#125; @Override protected void run(ChannelHandlerContext ctx) &#123; long nextDelay = readerIdleTimeNanos; if (!reading) &#123; // 如果不在读(channelRead时会被置为true，cahnnelReadComplete时会被置为false) nextDelay -= ticksInNanos() - lastReadTime; &#125; if (nextDelay &lt;= 0) &#123; // 说明读空闲时间达到或超过预设时间 // Reader is idle - set a new timeout and notify the callback. readerIdleTimeout = schedule(ctx, this, readerIdleTimeNanos, TimeUnit.NANOSECONDS); // firstReaderIdleEvent，是否是第一次读空闲事件(该标志位会在下一次channelRead触发时改成true，所以应该理解在一次读取完成后，这个读空闲事件是不是第一次) boolean first = firstReaderIdleEvent; firstReaderIdleEvent = false; try &#123; // 生成一个IdleStateEvent对象 IdleStateEvent event = newIdleStateEvent(IdleState.READER_IDLE, first); // 找到下一个ChannelInboundHandler类（或其子类）的handler，触发其userEventTrigger(可以参考AbstractChannelHandlerContext的fireUserEventTriggered方法) channelIdle(ctx, event); &#125; catch (Throwable t) &#123; ctx.fireExceptionCaught(t); &#125; &#125; else &#123; // 要么正在读，要么读空闲时间小于预设时间 // Read occurred before the timeout - set a new timeout with shorter delay. readerIdleTimeout = schedule(ctx, this, nextDelay, TimeUnit.NANOSECONDS); &#125; &#125;&#125; schedule方法可以理解为将定时调度事件放进一个队列当中（我是在AbstractScheduledEventExecutor里找到的scheduledTaskQueue().add(task);，但这里面的代码我还没看明白，有兴趣的你可以自己研究，研究完后如果有空可在下方评论）。channelIdle(ctx, event)方法时找到下一个ChannelInboundHandler类（或其子类）的handler，因此你写的继承自ChannelInboundHandler的handler，一定要添加在IdleStateHandler的后面，比如：12pipeline.addLast(new IdleStateHandler(30, 90, 0));pipeline.addLast(heartbeatHandler); ChannelInboundHandler它就很简单了，因为上面说了，channelIdle会调用ChannelInboundHandler的userEventTrigger，所以你只要自己写一个类继承ChannelInboundHandler，并重写它的userEventTrigger方法。比如：1234567891011121314151617181920212223242526272829303132// 用Sharable是因为我的每一个pipeline中用的都是同样的handler@Sharablepublic class NettyHeartbeatHandler extends ChannelInboundHandlerAdapter &#123; private final IHeartbeatFactory factory; public NettyHeartbeatHandler(IHeartbeatFactory factory) &#123; Preconditions.checkNotNull(factory); this.factory = factory; &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; if (!(evt instanceof IdleStateEvent)) &#123; super.userEventTriggered(ctx, evt); return; &#125; IdleStateEvent event = (IdleStateEvent) evt; if (event.state() == IdleState.READER_IDLE) &#123; // 如果是读空闲，则关闭当前会话 ctx.close(); // 此时会触发下一个ChannelOutboundHandler的close方法，你可以在自己写的handler中进行断线操作 &#125; else if (event.state() == IdleState.WRITER_IDLE) &#123; // 如果是写空闲，则向客户端发送心跳请求包，如果客户端不返回心跳相应包，则说明客户端断线，下一次就将触发读空闲事件。这也是为了向客户端证明服务器端alive ctx.writeAndFlush( new BinaryWebSocketFrame( Unpooled.copiedBuffer(factory.getHeartbeatRequest().toByteArray()) ) ); &#125; &#125;&#125; 因此，以上就是关于用Netty实现心跳的简单介绍。其中带大家重点看了服务器端应该在什么情况下发起一次心跳请求，应该是长久没有收到消息时（可能是有业务含义的消息或者是一个心跳包）。如果大家有什么想法可以在下方评论。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitalk Error: Validation Failed]]></title>
    <url>%2F2018%2F09%2F30%2Fgitalk%20Error%20Validation%20Failed%2F</url>
    <content type="text"><![CDATA[我现在博客的评论系统用的是gitalk，网上教程有很多，我参考的是这份教程。 当我按照网上的说法搭好后，确实是可以利用issue进行评论了，但我在新发表文章时，竟然报错了： 当时我心里一凉，难道和gitment一样，gitalk也凉了？后来上网查了一下，发现是github现在要求issue的label name不能超过50。（奥，现在我才知道，原来gitalk应该是利用label进行筛选，取得当前评论所属的issus。但后来看了一下gitalk的源代码和github关于issue的api，issue的查找应该和你的number有关，而gitalk是当你没有number时就用id代替，看的有点晕。PS：本人在前端方面纯属小白） 好了，那就想想有什么办法可以保证名字的长度可以不超过50吧。没错，就是md5,加密过后都是32位长度，且唯一。当然了，这个也是在gitalk的issue里查到的，接下来就来看看具体应该怎么做吧。 找到js版的md5算法首推的自然是别人已经造好的成熟的轮子，JavaScript-MD5这个应该是可以的，亲测有效。 你只要在你的主题(比如我的就是next)下的source\js\src目录中创放入md5.js.min文件即可。 修改gitalk.swig文件原本你的文件内容应该是：12345678910111213141516&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; var gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;, clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;, repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;, owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;, admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;], id: location.pathname, distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos; &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&#123;% endif %&#125; 现在改成即可(修改了第4行和第12行)1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;/js/src/md5.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; var gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;, clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;, repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;, owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;, admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;], id: md5(location.pathname), distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos; &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&#123;% endif %&#125; 然后重新发布就ok了，gitalk又可以正确创建issue了，你又可以继续评论了。 需要注意的问题因为gitalk关联issue是通过number，你没有number的时候，它会直接利用你的id，而id的这个生成条件又被你修改了，因此你之前评论是无法和你的文章关联上了。如果对js稍微感兴趣的话，应该可以顺着这个思路往下能解决。博主有空也会试试，就当研究研究js，大家要是有成功的案例，可以在通过评论告知，毕竟这也是造福大家。附上gitalk源码地址。]]></content>
      <categories>
        <category>解决问题</category>
      </categories>
      <tags>
        <tag>gitalk</tag>
        <tag>Error</tag>
        <tag>Validation Failed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos6安装shadowsocks及配置]]></title>
    <url>%2F2018%2F09%2F30%2FCentos6%E5%AE%89%E8%A3%85shadowsocks%E5%8F%8A%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[买下搬瓦工的服务器，很多人首先会希望配置多个端口方便使用，这时候就需要安装shadowsocks来解决。现在我来介绍一下Centos6系统下shadowsocks的安装及配置过程。 这篇文章原是我在CSDN上发表的一篇博客，但在去年年底时莫名其妙被删除了，因此在这里，我把文章搬运过来，也是方便自己查看里面的一些命令。 这是一个搬瓦工的购买教程网站 安装python、pip、shadowsocks安装python： yum install python-setuptools 安装wget yum install wget 安装pip：（先下载再安装）1234wget https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz --no-check-certificatetar -xzvf pip-1.3.1.tar.gzcd pip-1.3.1python setup.py install 安装shadowsocks pip install shadowsocks 配置shadowsocks首先创建配置文件/etc/shadowsocks.json touch /etc/shadowsocks.json 创建并编辑shadowsocks.json vi /etc/shadowsocks.json 现在要决定你是否需要开多端口，因为开出多个端口，每个端口的速度影响不大，但如果用同一个端口，速度会有所影响。 shadowsocks.json内容为：123456789101112&#123; &quot;server&quot;:&quot;你的IP地址&quot;, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:&#123; &quot;端口号1&quot;:&quot;密码1&quot;, &quot;端口号2&quot;:&quot;密码2&quot; &#125;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;rc4-md5&quot;, &quot;fast_open&quot;: false&#125; 对应你本地的shadowsocks配置为： 这时可以在Centos上运行shadowsocks服务： ssserver -c /etc/shadowsocks.json -d start 此时理论上你就可以科学上网了。 停止shadowsocks服务命令： ssserver -c /etc/shadowsocks.json -d stop 可能遇到的问题当你设置好配置文件并且启动之后，发现本地并不能上外网，其实可以通过shadowsocks的更新PAC功能查看是否可以连接外网： 如果更新失败，则代表无法连接外网，这时候请看一下你的服务器上设置的端口是否开启： netstat -ntlp 我开启了443、7788、7789、7790四个端口，如果你发现此处没有你的端口号，代表端口未打开 此时你需要先关闭shadowsocks，使用关闭命令，然后打开你所需要的端口。 利用iptables打开端口命令: /sbin/iptables -I INPUT -p tcp --dport 端口号 -j ACCEPT 保存你刚刚添加的规则: /etc/rc.d/init.d/iptables save 查看打开的端口： /etc/init.d/iptables status 现在这里会显示出你刚刚打开的端口。 现在开启你服务器上的shadowsocks，再用查看端口命令，应该就显示出你的端口已经打开了。 资源下载shadowsocks相关下载包安卓版本官方下载电脑版Windows 7及之前的版本电脑版Windows 8及之后的版本iOS教程]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初章]]></title>
    <url>%2F2018%2F09%2F29%2F%E5%88%9D%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[第一次使用hexo写博客，希望能有一个不错的开始。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
